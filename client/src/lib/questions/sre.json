[
  {
    "id": "sre-1",
    "question": "What are SLIs, SLOs, and SLAs? How do they relate?",
    "answer": "Metrics, Goals, and Consequences.",
    "explanation": "1. **SLI (Indicator)**: The actual number. \"My latency is 200ms\".\n2. **SLO (Objective)**: The internal goal. \"Latency should be < 300ms for 99% of requests\".\n3. **SLA (Agreement)**: The legal contract with users. \"If latency > 500ms, we refund you 10%\".\n\n*SREs focus on SLOs. SLAs are for lawyers.*",
    "tags": [
      "metrics",
      "policy",
      "definitions",
      "observability"
    ],
    "difficulty": "beginner",
    "channel": "sre",
    "subChannel": "observability",
    "diagram": "graph TD\n    SLI[\"Indicator<br/>Reality\"] -->|Measured Against| SLO[\"Objective<br/>Goal\"]\n    SLO -->|Buffer| SLA[\"Agreement<br/>Contract\"]",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "sre-2",
    "question": "What is an Error Budget and how do you use it?",
    "answer": "100% - SLO = Error Budget. It's the allowed amount of unreliability.",
    "explanation": "If SLO is 99.9% uptime, you have 0.1% error budget (approx 43 mins/month).\n\n**Usage**:\n- **Budget Remaining?**: Ship features fast, take risks, do chaos engineering.\n- **Budget Exhausted?**: FREEZE deployments. Focus 100% on reliability until budget recovers.\n\n*It aligns Dev (speed) and Ops (stability) incentives.*",
    "tags": [
      "management",
      "concept",
      "risk"
    ],
    "difficulty": "beginner",
    "channel": "sre",
    "subChannel": "reliability",
    "diagram": "\ngraph LR\n    SLO[99.9% SLO] --> EB[0.1% Error Budget]\n    EB -->|Remaining| Ship[Ship Features]\n    EB -->|Exhausted| Freeze[Freeze Deploys]\n",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "gh-59",
    "question": "What is Site Reliability Engineering?",
    "answer": "Site Reliability Engineering (SRE) is a discipline that incorporates aspects of software engineering and applies them to infrastructure and operations...",
    "explanation": "Site Reliability Engineering (SRE) is a discipline that incorporates aspects of software engineering and applies them to infrastructure and operations problems to create scalable and highly reliable software systems.\n\nKey principles:\n1. **Embrace Risk:**\n- Define acceptable risk levels\n- Use error budgets\n- Balance reliability and innovation\n\n2. **Eliminate Toil:**\n- Automate manual tasks\n- Reduce operational overhead\n- Focus on engineering work",
    "tags": [
      "sre",
      "reliability"
    ],
    "difficulty": "beginner",
    "channel": "sre",
    "subChannel": "reliability",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "gh-60",
    "question": "What are Service Level Objectives (SLOs)?",
    "answer": "Service Level Objectives (SLOs) are specific, measurable targets for service performance that you set and agree to meet.",
    "explanation": "Service Level Objectives (SLOs) are specific, measurable targets for service performance that you set and agree to meet.\n\nExample SLO definition:\n```yaml\nService: User Authentication\nSLO:\nMetric: Availability\nTarget: 99.9%\nWindow: 30 days\nMeasurement:\n- Success rate of authentication requests\n- Latency under 300ms for 99% of requests\n```",
    "tags": [
      "sre",
      "reliability"
    ],
    "difficulty": "intermediate",
    "channel": "sre",
    "subChannel": "reliability",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "gh-61",
    "question": "What are Service Level Indicators (SLIs)?",
    "answer": "Service Level Indicators (SLIs) are quantitative measures of service level aspects such as latency, throughput, availability, and error rate.",
    "explanation": "Service Level Indicators (SLIs) are quantitative measures of service level aspects such as latency, throughput, availability, and error rate.\n\nCommon SLIs:\n1. **Request Latency:**\n- Time to handle a request\n- Distribution of response times\n\n2. **Error Rate:**\n- Failed requests/total requests\n- Error budget consumption\n\n3. **System Throughput:**\n- Requests per second\n- Transactions per second",
    "tags": [
      "sre",
      "reliability"
    ],
    "difficulty": "intermediate",
    "channel": "sre",
    "subChannel": "reliability",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "gh-62",
    "question": "What is Error Budget?",
    "answer": "An Error Budget is the maximum amount of time that a technical system can fail without contractual consequences. It's the difference between the SLO t...",
    "explanation": "An Error Budget is the maximum amount of time that a technical system can fail without contractual consequences. It's the difference between the SLO target and 100% reliability.\n\nExample calculation:\n```\nSLO Target: 99.9% uptime\nError Budget: 100% - 99.9% = 0.1%\nMonthly Error Budget: 43.2 minutes (0.1% of 30 days)\n```\n\nKey concepts:\n1. **Budget Calculation:**\n- Based on SLO targets\n- Measured over time windows\n- Reset periodically\n\n2. **Budget Usage:**\n- Track incidents\n- Monitor consumption\n- Alert on budget burn",
    "tags": [
      "sre",
      "reliability"
    ],
    "difficulty": "beginner",
    "channel": "sre",
    "subChannel": "reliability",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "gh-63",
    "question": "What is Toil in SRE?",
    "answer": "Toil is the kind of work tied to running a production service that tends to be manual, repetitive, automatable, tactical, devoid of enduring value, an...",
    "explanation": "Toil is the kind of work tied to running a production service that tends to be manual, repetitive, automatable, tactical, devoid of enduring value, and that scales linearly as a service grows.\n\nCharacteristics of toil:\n1. **Manual work:**\n- No automation\n- Human intervention required\n- Repetitive tasks\n\n2. **Impact:**\n- Reduces time for project work\n- Increases operational overhead\n- Affects team morale\n\n3. **Solutions:**\n\nAutomation:\n- Script repetitive tasks\n- Implement self-service tools\n- Create automated workflows\n\nProcess Improvement:\n- Identify toil sources\n- Set toil budgets\n- Track toil metrics\n\nEngineering Solutions:\n- Design for automation\n- Build self-healing systems\n- Implement proper monitoring",
    "tags": [
      "sre",
      "reliability"
    ],
    "difficulty": "beginner",
    "channel": "sre",
    "subChannel": "reliability",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "gh-73",
    "question": "What is Incident Management?",
    "answer": "Incident Management is the process of responding to and resolving IT service disruptions.",
    "explanation": "Incident Management is the process of responding to and resolving IT service disruptions.\n\nKey components:\n1. **Detection:**\n- Monitoring alerts\n- User reports\n- Automated detection\n\n2. **Response:**\n```yaml\nInitial Response:\n- Acknowledge incident\n- Assess severity\n- Notify stakeholders\n\nResolution:\n- Investigate root cause\n- Apply fix\n- Verify solution\n```",
    "tags": [
      "incident",
      "on-call"
    ],
    "difficulty": "beginner",
    "channel": "sre",
    "subChannel": "incident-management",
    "diagram": "\ngraph LR\n    Detect[Detect] --> Respond[Respond]\n    Respond --> Resolve[Resolve]\n    Resolve --> Review[Post-Mortem]\n",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "gh-76",
    "question": "What is Infrastructure Monitoring?",
    "answer": "Infrastructure Monitoring is the process of collecting and analyzing data from IT infrastructure components to ensure optimal performance and availabi...",
    "explanation": "Infrastructure Monitoring is the process of collecting and analyzing data from IT infrastructure components to ensure optimal performance and availability.\n\nKey components:\n1. **Metrics Collection:**\n- System metrics\n- Network metrics\n- Application metrics\n\n2. **Analysis:**\n```yaml\nMonitoring Areas:\n- Resource utilization\n- Performance metrics\n- Availability\n- Error rates\n- Response times\n```",
    "tags": [
      "monitoring",
      "infra"
    ],
    "difficulty": "beginner",
    "channel": "sre",
    "subChannel": "observability",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "gh-77",
    "question": "What are Monitoring Tools?",
    "answer": "Common monitoring tools used in DevOps:",
    "explanation": "Common monitoring tools used in DevOps:\n\n1. **Infrastructure Monitoring:**\n- Prometheus\n- Nagios\n- Zabbix\n- Datadog\n\n2. **Application Monitoring:**\n```yaml\nTools:\n- New Relic\n- AppDynamics\n- Dynatrace\nFeatures:\n- Transaction tracing\n- Error tracking\n- Performance analytics\n```",
    "tags": [
      "monitoring",
      "infra"
    ],
    "difficulty": "intermediate",
    "channel": "sre",
    "subChannel": "observability",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "gh-78",
    "question": "What are Monitoring Best Practices?",
    "answer": "Monitoring Best Practices are proven methods that enhance the effectiveness of monitoring tools and processes.",
    "explanation": "Monitoring Best Practices are proven methods that enhance the effectiveness of monitoring tools and processes.\n\nKey practices:\n```yaml\nTechnical Practices:\n- Infrastructure as Code\n- Continuous Integration\n- Automated Testing\n- Continuous Deployment\n- Monitoring and Logging\n\nCultural Practices:\n- Shared Responsibility\n- Blameless Post-mortems\n- Knowledge Sharing\n- Continuous Learning\n- Cross-functional Teams\n\nProcess Practices:\n- Agile Methodology\n- Version Control\n- Configuration Management\n- Release Management\n- Incident Management\n```",
    "tags": [
      "monitoring",
      "infra"
    ],
    "difficulty": "intermediate",
    "channel": "sre",
    "subChannel": "observability",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "gh-79",
    "question": "What is Application Performance Monitoring?",
    "answer": "Application Performance Monitoring (APM) is the practice of collecting and analyzing data about the performance and stability of applications to impro...",
    "explanation": "Application Performance Monitoring (APM) is the practice of collecting and analyzing data about the performance and stability of applications to improve their reliability and responsiveness.\n\nKey components:\n1. **Metrics Collection:**\n- Application metrics\n- Transaction tracing\n- Error tracking\n- Performance analytics\n\n2. **Analysis:**\n```yaml\nMonitoring Areas:\n- Application response times\n- Error rates\n- Resource utilization\n- Scalability\n- Reliability\n```",
    "tags": [
      "monitoring",
      "infra"
    ],
    "difficulty": "beginner",
    "channel": "sre",
    "subChannel": "observability",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "gh-80",
    "question": "What is Log Management?",
    "answer": "Log Management is the practice of collecting, analyzing, and managing log data to help diagnose and troubleshoot issues.",
    "explanation": "Log Management is the practice of collecting, analyzing, and managing log data to help diagnose and troubleshoot issues.\n\nKey components:\n1. **Log Collection:**\n- Collecting log data from various sources\n- Centralized logging infrastructure\n\n2. **Log Analysis:**\n- Log aggregation\n- Security analytics\n- Application performance monitoring\n- Website search\n- Business analytics\n\n3. **Log Visualization:**\n- Dashboard creation\n- Alerting\n- Visualization",
    "tags": [
      "monitoring",
      "infra"
    ],
    "difficulty": "beginner",
    "channel": "sre",
    "subChannel": "observability",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "sr-124",
    "question": "What are the four golden signals of monitoring?",
    "answer": "Latency, Traffic, Errors, and Saturation - key metrics for service health.",
    "explanation": "**The Four Golden Signals**:\n\n1. **Latency**: Time to serve a request\n2. **Traffic**: Demand on your system (requests/sec)\n3. **Errors**: Rate of failed requests\n4. **Saturation**: How full your service is (CPU, memory)\n\nThese signals help identify issues before they become outages.",
    "tags": [
      "metrics",
      "monitoring"
    ],
    "difficulty": "beginner",
    "channel": "sre",
    "subChannel": "observability",
    "diagram": "graph TD\n    M[Monitoring] --> L[Latency]\n    M --> T[Traffic]\n    M --> E[Errors]\n    M --> S[Saturation]",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "sr-126",
    "question": "What makes a good blameless postmortem?",
    "answer": "Focus on systems and processes, not individuals; identify root causes and actionable improvements.",
    "explanation": "**Blameless Postmortem Elements**:\n- Timeline of events\n- Root cause analysis (5 Whys)\n- What went well\n- What could be improved\n- Action items with owners\n\n**Key Principles**:\n- No finger-pointing\n- Assume good intentions\n- Focus on learning\n- Share widely",
    "tags": [
      "incident",
      "postmortem"
    ],
    "difficulty": "advanced",
    "channel": "sre",
    "subChannel": "incident-management",
    "diagram": "graph TD\n    Incident[Incident] --> Timeline[Timeline]\n    Timeline --> RCA[Root Cause]\n    RCA --> Actions[Action Items]\n    Actions --> Prevention[Prevention]",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "sr-130",
    "question": "Your web service has an SLO of 99.9% availability over 30 days. You've had 3 outages: 45 minutes, 20 minutes, and 15 minutes. What's your current availability SLI and are you meeting your SLO?",
    "answer": "SLI: 99.81% availability. Not meeting 99.9% SLO - exceeded error budget by 0.09%.",
    "explanation": "## SLI Calculation\n\n**Total time in 30 days:** 30 × 24 × 60 = 43,200 minutes\n\n**Total downtime:** 45 + 20 + 15 = 80 minutes\n\n**Uptime:** 43,200 - 80 = 43,120 minutes\n\n**SLI (Service Level Indicator):** (43,120 ÷ 43,200) × 100 = **99.81%**\n\n## SLO Analysis\n\n**Target SLO:** 99.9% availability\n**Current SLI:** 99.81%\n**Status:** ❌ **Not meeting SLO**\n\n## Error Budget\n\n**Allowed downtime for 99.9% SLO:** 43,200 × 0.001 = 43.2 minutes\n**Actual downtime:** 80 minutes\n**Error budget exceeded by:** 80 - 43.2 = 36.8 minutes (0.09%)\n\n## Key Concepts\n\n- **SLI (Service Level Indicator):** Actual measured performance metric\n- **SLO (Service Level Objective):** Target reliability goal\n- **Error Budget:** Allowed unreliability (100% - SLO%)\n\nWhen SLI < SLO, you've exceeded your error budget and should focus on reliability improvements over new features.",
    "tags": [
      "slo",
      "sli",
      "error-budget"
    ],
    "difficulty": "intermediate",
    "channel": "sre",
    "subChannel": "slo-sli",
    "diagram": "graph TD\n    A[30 Days Total Time<br/>43,200 minutes] --> B[Calculate Downtime]\n    B --> C[Outage 1: 45 min<br/>Outage 2: 20 min<br/>Outage 3: 15 min]\n    C --> D[Total Downtime<br/>80 minutes]\n    A --> E[Calculate Uptime<br/>43,200 - 80 = 43,120 min]\n    E --> F[SLI Calculation<br/>43,120 ÷ 43,200 × 100]\n    F --> G[Current SLI<br/>99.81%]\n    H[SLO Target<br/>99.9%] --> I{SLI ≥ SLO?}\n    G --> I\n    I -->|No| J[❌ SLO Breach<br/>Error Budget Exceeded]\n    I -->|Yes| K[✅ SLO Met<br/>Within Error Budget]\n    L[Error Budget<br/>43.2 minutes allowed] --> M[Budget Exceeded<br/>36.8 minutes over]",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "sr-131",
    "question": "You're managing a microservices platform with 50 services. Service A has a 95th percentile latency of 200ms and handles 10,000 RPS. It calls Service B (50ms, 5,000 RPS) and Service C (100ms, 3,000 RPS). During Black Friday, you expect 5x traffic. Service A's CPU utilization is currently 60%, memory at 70%. How do you plan capacity to maintain <500ms 95th percentile end-to-end latency?",
    "answer": "Scale Service A to 15 instances, Service B to 8 instances, Service C to 5 instances. Add circuit breakers, implement caching, and use load shedding.",
    "explanation": "## Capacity Planning Analysis\n\n### Current State Assessment\n- **Service A**: 200ms p95, 10K RPS, 60% CPU, 70% memory\n- **Service B**: 50ms p95, 5K RPS (called by A)\n- **Service C**: 100ms p95, 3K RPS (called by A)\n- **Current end-to-end latency**: ~350ms (200+50+100)\n\n### Black Friday Projections (5x traffic)\n- **Service A**: 50K RPS target\n- **Service B**: 25K RPS target  \n- **Service C**: 15K RPS target\n\n### Capacity Planning Strategy\n\n#### 1. **Horizontal Scaling Calculations**\n```\nService A: 50K RPS ÷ (10K RPS × 0.4 headroom) = ~12.5 → 15 instances\nService B: 25K RPS ÷ 5K RPS = 5 → 8 instances (buffer)\nService C: 15K RPS ÷ 3K RPS = 5 instances\n```\n\n#### 2. **Latency Optimization**\n- **Circuit breakers**: Prevent cascade failures\n- **Caching**: Reduce Service B/C calls by 30-40%\n- **Connection pooling**: Reduce connection overhead\n- **Load shedding**: Drop non-critical requests at 80% capacity\n\n#### 3. **Resource Allocation**\n- **CPU**: Target 40-50% utilization under peak load\n- **Memory**: Target 60% utilization with garbage collection headroom\n- **Network**: Ensure bandwidth can handle 5x throughput\n\n#### 4. **Monitoring & Alerting**\n- Set alerts at 70% capacity utilization\n- Monitor queue depths and connection pool exhaustion\n- Track error rates and implement auto-scaling triggers\n\n#### 5. **Fallback Strategies**\n- **Graceful degradation**: Disable non-essential features\n- **CDN offloading**: Cache static content\n- **Database read replicas**: Distribute read load\n\nThis approach ensures <500ms p95 latency while maintaining system reliability during traffic spikes.",
    "tags": [
      "capacity",
      "scaling"
    ],
    "difficulty": "advanced",
    "channel": "sre",
    "subChannel": "capacity-planning",
    "diagram": "graph TD\n    A[Load Balancer] --> B[Service A - 15 instances]\n    B --> C[Service B - 8 instances]\n    B --> D[Service C - 5 instances]\n    B --> E[Cache Layer]\n    F[Circuit Breaker] --> B\n    G[Auto Scaler] --> B\n    G --> C\n    G --> D\n    H[Monitoring] --> I[Alerts]\n    I --> G\n    J[Load Shedder] --> A\n    K[CDN] --> A\n    \n    style B fill:#ff9999\n    style C fill:#99ccff\n    style D fill:#99ff99\n    style E fill:#ffcc99\n    style F fill:#ff6666\n    style J fill:#cc99ff",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "sr-133",
    "question": "What's the difference between logs, metrics, and traces in observability?",
    "answer": "Logs: events, Metrics: numbers over time, Traces: request journeys.",
    "explanation": "**The Three Pillars of Observability**:\n\n1. **Logs**: Timestamped events showing what happened\n   - Structured logs with key-value pairs\n   - Error messages, debug info\n   - \"User login failed at 2:15 PM\"\n\n2. **Metrics**: Numerical data aggregated over time\n   - CPU usage, request count, error rate\n   - Charts and dashboards\n   - \"95th percentile latency: 200ms\"\n\n3. **Traces**: End-to-end request flow across services\n   - Shows path through distributed systems\n   - Identifies bottlenecks and failures\n   - \"API call took 500ms across 3 services\"\n\n**Why all three?** Logs tell you *what* happened, metrics show *how much*, traces reveal *where*.",
    "tags": [
      "metrics",
      "monitoring"
    ],
    "difficulty": "beginner",
    "channel": "sre",
    "subChannel": "observability",
    "diagram": "graph TD\n    A[Request] --> B[Frontend]\n    B --> C[API Gateway]\n    C --> D[Service A]\n    C --> E[Service B]\n    D --> F[Database]\n    \n    G[Logs] --> H[\"Error: DB timeout\"]\n    I[Metrics] --> J[\"CPU: 80%\"]\n    K[Traces] --> L[\"500ms total\"]\n    \n    style G fill:#ff9999\n    style I fill:#99ccff\n    style K fill:#99ff99",
    "lastUpdated": "2025-12-12T09:14:39.919Z"
  }
]