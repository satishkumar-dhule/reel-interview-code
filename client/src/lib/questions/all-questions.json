{
  "questions": {
    "al-1": {
      "id": "al-1",
      "question": "When would you use a Linked List over an Array?",
      "answer": "Linked Lists excel at insertions/deletions, Arrays excel at random access.",
      "explanation": "**Array**:\n- Access: O(1) (Direct memory address)\n- Insert/Delete: O(n) (Shift elements)\n- Memory: Contiguous\n\n**Linked List**:\n- Access: O(n) (Traversal)\n- Insert/Delete: O(1) (Change pointer - if you have the node)\n- Memory: Scattered (Heap)\n\n**Use List when**: Implementing Queues, Stacks, or when memory is fragmented.",
      "diagram": "\ngraph LR\n    subgraph Array\n    A1[0] --- A2[1] --- A3[2] --- A4[3]\n    end\n    subgraph LinkedList\n    L1[Node] --> L2[Node] --> L3[Node]\n    end\n",
      "difficulty": "beginner",
      "tags": [
        "struct",
        "comparison",
        "basics"
      ],
      "lastUpdated": "2025-12-12T09:07:04.185Z"
    },
    "al-2": {
      "id": "al-2",
      "question": "Explain QuickSort vs MergeSort. Which is better?",
      "answer": "QuickSort is generally faster in practice (cache locality) but unstable. MergeSort is stable but uses O(n) space.",
      "explanation": "**QuickSort**:\n- Avg: O(n log n), Worst: O(n^2) (bad pivot).\n- Space: O(log n) stack.\n- **In-place**.\n\n**MergeSort**:\n- Always O(n log n).\n- Space: O(n) (aux array).\n- **Stable** (preserves order of equals).\n\n**Verdict**: Arrays -> QuickSort. Linked Lists -> MergeSort.",
      "diagram": "\ngraph TD\n    A[Array] --> P{Pick Pivot}\n    P --> L[Left < Pivot]\n    P --> R[Right > Pivot]\n    L --> Sort1[Recurse]\n    R --> Sort2[Recurse]\n",
      "difficulty": "intermediate",
      "tags": [
        "sort",
        "recursion",
        "complexity"
      ],
      "lastUpdated": "2025-12-12T09:07:04.185Z"
    },
    "al-3": {
      "id": "al-3",
      "question": "What is Dynamic Programming? How is it different from Recursion?",
      "answer": "DP is Recursion + Optimization (Memoization/Tabulation).",
      "explanation": "Recursion solves subproblems blindly (potentially repeating work).\n\n**DP** stores the result of subproblems so you never solve the same problem twice.\n\n**Example**: Fibonacci.\n- Recursion: `fib(n-1) + fib(n-2)` -> O(2^n)\n- DP: Store `fib` array -> O(n)\n\n**Two Approaches**:\n1. Top-Down (Memoization)\n2. Bottom-Up (Tabulation)",
      "diagram": "\ngraph TD\n    P[Problem] --> S1[Subproblem 1]\n    P --> S2[Subproblem 2]\n    S1 --> C[(Cache)]\n    S2 --> C\n    C --> R[Result]\n",
      "difficulty": "advanced",
      "tags": [
        "dp",
        "optimization",
        "theory"
      ],
      "lastUpdated": "2025-12-12T09:07:04.185Z"
    },
    "al-152": {
      "id": "al-152",
      "question": "You have a staircase with n steps. You can climb 1, 2, or 3 steps at a time. How many distinct ways can you reach the top? Implement a solution with O(n) time and O(1) space complexity.",
      "answer": "Use DP with 3 variables tracking last 3 positions. dp[i] = dp[i-1] + dp[i-2] + dp[i-3]. Base: dp[0]=1, dp[1]=1, dp[2]=2",
      "explanation": "## Approach\n\nThis is a classic dynamic programming problem similar to climbing stairs, but with three possible steps instead of two.\n\n## Solution\n\n### Recurrence Relation\n\nFor each step `i`, the number of ways to reach it is the sum of ways to reach the previous three steps:\n\n```\ndp[i] = dp[i-1] + dp[i-2] + dp[i-3]\n```\n\n### Base Cases\n\n- `dp[0] = 1` (one way to stay at ground)\n- `dp[1] = 1` (one step)\n- `dp[2] = 2` (1+1 or 2)\n\n### Space Optimization\n\nInstead of maintaining an array of size n, we only need three variables to track the last three positions:\n\n```python\ndef climbStairs(n):\n    if n <= 2:\n        return n if n > 0 else 1\n    \n    a, b, c = 1, 1, 2  # dp[0], dp[1], dp[2]\n    \n    for i in range(3, n + 1):\n        current = a + b + c\n        a, b, c = b, c, current\n    \n    return c\n```\n\n## Complexity\n\n- **Time**: O(n) - single pass through n steps\n- **Space**: O(1) - only three variables used\n\n## Example\n\nFor n=4:\n- dp[3] = dp[2] + dp[1] + dp[0] = 2 + 1 + 1 = 4\n- dp[4] = dp[3] + dp[2] + dp[1] = 4 + 2 + 1 = 7\n\nThere are 7 distinct ways to climb 4 steps.",
      "diagram": "graph TD\n    A[\"n=4 (target)\"] --> B[\"n=3 (4 ways)\"]\n    A --> C[\"n=2 (2 ways)\"]\n    A --> D[\"n=1 (1 way)\"]\n    B --> E[\"n=2 (2 ways)\"]\n    B --> F[\"n=1 (1 way)\"]\n    B --> G[\"n=0 (1 way)\"]\n    C --> H[\"n=1 (1 way)\"]\n    C --> I[\"n=0 (1 way)\"]\n    D --> J[\"n=0 (1 way)\"]\n    \n    style A fill:#ff6b6b\n    style B fill:#4ecdc4\n    style C fill:#4ecdc4\n    style D fill:#4ecdc4\n    style E fill:#95e1d3\n    style F fill:#95e1d3\n    style G fill:#95e1d3\n    style H fill:#95e1d3\n    style I fill:#95e1d3\n    style J fill:#95e1d3",
      "difficulty": "intermediate",
      "tags": [
        "dp",
        "optimization"
      ],
      "lastUpdated": "2025-12-13T01:07:52.426Z"
    },
    "al-163": {
      "id": "al-163",
      "question": "You have an array where each element appears twice except one element that appears once. Sort the array in O(n) time without using extra space for sorting. How would you approach this?",
      "answer": "Use XOR to find the unique element first, then partition array around it using modified counting sort with bit manipulation.",
      "explanation": "## Approach\n\nThis problem combines bit manipulation with in-place partitioning:\n\n### Step 1: Find the Unique Element\nUse XOR operation on all elements. Since `a ^ a = 0` and `a ^ 0 = a`, all paired elements cancel out, leaving only the unique element.\n\n```javascript\nlet unique = 0;\nfor (let num of arr) {\n  unique ^= num;\n}\n```\n\n### Step 2: Partition Around Unique Element\nUse three-way partitioning (similar to Dutch National Flag):\n- Elements less than unique go left\n- Unique element in middle\n- Elements greater than unique go right\n\n```javascript\nlet low = 0, mid = 0, high = arr.length - 1;\nwhile (mid <= high) {\n  if (arr[mid] < unique) {\n    [arr[low], arr[mid]] = [arr[mid], arr[low]];\n    low++; mid++;\n  } else if (arr[mid] > unique) {\n    [arr[mid], arr[high]] = [arr[high], arr[mid]];\n    high--;\n  } else {\n    mid++;\n  }\n}\n```\n\n### Step 3: Sort Pairs\nWithin each partition, pairs are already together. Use counting sort or simply swap pairs into position.\n\n**Time Complexity:** O(n)\n**Space Complexity:** O(1)",
      "diagram": "graph TD\n    A[Start: Unsorted Array] --> B[XOR all elements]\n    B --> C[Find unique element]\n    C --> D[Three-way partition]\n    D --> E[Elements < unique]\n    D --> F[Unique element]\n    D --> G[Elements > unique]\n    E --> H[Sort pairs in left partition]\n    G --> I[Sort pairs in right partition]\n    H --> J[Combine: Left + Unique + Right]\n    I --> J\n    J --> K[Sorted Array]\n    \n    style C fill:#90EE90\n    style D fill:#FFB6C1\n    style K fill:#87CEEB",
      "difficulty": "intermediate",
      "tags": [
        "sort",
        "complexity"
      ],
      "lastUpdated": "2025-12-13T16:57:00.393Z"
    },
    "al-164": {
      "id": "al-164",
      "question": "Count Ways to Reach Target Sum with Array Elements",
      "answer": "Use DP with subset sum approach",
      "explanation": "This is a variation of subset sum where we count ways to reach target using array elements",
      "diagram": "graph TD\n    A[Start] --> B[DP Array]\n    B --> C[Iterate Elements]\n    C --> D[Update DP]\n    D --> E[Return Count]",
      "difficulty": "intermediate",
      "tags": [
        "dp",
        "optimization"
      ],
      "lastUpdated": "2025-12-13T16:59:18.824Z"
    },
    "al-165": {
      "id": "al-165",
      "question": "Implement a Trie data structure for efficient prefix search. What are its advantages over a hash map?",
      "answer": "Trie provides O(k) prefix search, space-efficient for common prefixes.",
      "explanation": "## Trie Structure\n\nA Trie (prefix tree) stores strings character by character, sharing common prefixes.\n\n```python\nclass TrieNode:\n    def __init__(self):\n        self.children = {}\n        self.is_end = False\n\nclass Trie:\n    def __init__(self):\n        self.root = TrieNode()\n    \n    def insert(self,word):\n        node = self.root\n        for char in word:\n            if char not in node.children:\n                node.children[char] = TrieNode()\n            node = node.children[char]\n        node.is_end = True\n```\n\n## Advantages over Hash Map\n\n**Prefix Search**: Find all words with prefix in O(k) where k = prefix length\n**Space Efficiency**: Common prefixes shared (e.g., 'app', 'apple', 'application')\n**Sorted Output**: Lexicographic order traversal\n**No Collisions**: Unlike hash maps\n\n## Use Cases\n- Autocomplete systems\n- Spell checkers\n- IP routing tables\n- Dictionary implementations",
      "diagram": "graph TD\n    A[Root] --> A1[a] --> P1[pp] --> P2[p] --> P3[l] --> E1[apple]\n    A --> A2[a] --> P4[pp] --> P5[l] --> E2[apply]\n    A --> A3[a] --> P6[p] --> P7[l] --> E3[application]\n    A --> A4[a] --> P8[pp] --> P9[l] --> E4[approach]\n    \n    style A fill:#FF6B6B\n    style E1 fill:#4ECDC4\n    style E2 fill:#4ECDC4\n    style E3 fill:#4ECDC4\n    style E4 fill:#95E1D3",
      "difficulty": "intermediate",
      "tags": [
        "struct",
        "basics"
      ],
      "lastUpdated": "2025-12-13T16:59:48.670Z"
    },
    "al-166": {
      "id": "al-166",
      "question": "Given a string, find the minimum number of insertions and deletions to make it a palindrome. Each insertion costs 2, each deletion costs 1.",
      "answer": "Use DP with states (i,j) representing substring s[i..j] and compute min operations recursively.",
      "explanation": "This is a variation of the classic edit distance problem. We use DP where dp[i][j] = min operations to make substring s[i..j] a palindrome. If s[i] == s[j], dp[i][j] = dp[i+1][j-1] = 1 + min(dp[i+1][j-1], dp[i][j-1] + 1, dp[i+1][j] + 2). Base cases: i >= j return 0. The answer is dp[0][n-1].",
      "diagram": "graph TD\n    A[Start dp[i][j]] --> B{s[i] == s[j]}\n    B -->|Yes| C[dp[i+1][j-1]]\n    B -->|No| D[1 + min(dp[i+1][j-1], dp[i+1][j], dp[i][j-1])]\n    C --> E[Return dp[i][j]]\n    D --> E",
      "difficulty": "intermediate",
      "tags": [
        "dp",
        "optimization"
      ],
      "lastUpdated": "2025-12-13T16:59:58.522Z"
    },
    "al-167": {
      "id": "al-167",
      "question": "Count ways to reach target sum using dice rolls 1-6",
      "answer": "DP where dp[i] = sum(dp[i-1] to i-6)",
      "explanation": "Use DP where dp[i] = sum(dp[i-1] to i-6)",
      "diagram": "graph TD; A[0]=1; for i=1 to target; dp[i]=sum(dp[i-1] to i-6)",
      "difficulty": "intermediate",
      "tags": [
        "dp",
        "optimization"
      ],
      "lastUpdated": "2025-12-13T17:00:15.427Z"
    },
    "al-170": {
      "id": "al-170",
      "question": "Given an array of integers where each element represents the maximum number of steps you can jump forward from that position, find the minimum number of jumps needed to reach the last index. If it's impossible, return -1. You can only move forward.",
      "answer": "Use greedy approach tracking current range and furthest reachable position in O(n) time.",
      "explanation": "This is a variation of the Jump Game II problem. The optimal solution uses a greedy approach:\n\n1. Keep track of the current range (end) where we can reach with the current number of jumps\n2. Track the furthest position we can reach within the next range (furthest)\n3. When we reach the end of the current range, we must make another jump\n4. Update the range to the furthest position we found\n\nTime complexity: O(n), Space complexity: O(1)\n\nThe key insight is that we don't need to explore all possible paths - we just need to know the furthest we can reach within each jump range.",
      "diagram": "graph TD\n    A[Start at index 0] --> B[Initialize jumps=0, end=0, furthest=0]\n    B --> C[Iterate through array]\n    C --> D{Reached end of current range?}\n    D -->|Yes| E[Increment jumps, set end=furthest]\n    D -->|No| F[Update furthest = max(furthest, i + nums[i])]\n    E --> F\n    F --> G{Can reach last index?}\n    G -->|Yes| H[Return jumps]\n    G -->|No| I[Continue to next position]\n    I --> C\n    H --> J[End]\n    C --> K{i >= n-1?}\n    K -->|Yes| H\n    K -->|No| D",
      "difficulty": "advanced",
      "tags": [
        "dp",
        "optimization"
      ],
      "lastUpdated": "2025-12-14T01:17:23.338Z"
    },
    "db-1": {
      "id": "db-1",
      "question": "What is the difference between Clustered and Non-Clustered Indexes?",
      "answer": "Clustered Index defines the physical order of data. Non-Clustered is a separate lookup structure.",
      "explanation": "**Clustered Index**:\n- Only 1 per table (usually Primary Key).\n- Leaf nodes contain the ACTUAL data rows.\n- Faster for range queries.\n\n**Non-Clustered Index**:\n- Multiple allowed.\n- Leaf nodes contain pointers to the data (row ID or clustered key).\n- Requires 'Bookmark Lookup' (extra hop) to get full data.",
      "diagram": "graph TD\n    subgraph Clustered\n    Root1 --> Data[\"Data Pages<br/>Sorted\"]\n    end\n    subgraph NonClustered\n    Root2 --> Ptr[Pointers]\n    Ptr -.-> Data\n    end",
      "difficulty": "beginner",
      "tags": [
        "sql",
        "indexing",
        "perf"
      ],
      "lastUpdated": "2025-12-12T09:07:04.186Z"
    },
    "db-2": {
      "id": "db-2",
      "question": "How do ACID properties ensure data integrity in a banking transaction where $100 is transferred from Account A to Account B?",
      "answer": "Atomicity ensures all-or-nothing execution, Consistency maintains valid states, Isolation prevents interference, Durability guarantees persistence.",
      "explanation": "**Banking Transfer Scenario**: Account A transfers $100 to Account B\n\n• **Atomicity**: Either both debit from A AND credit to B succeed, or both fail and rollback completely\n• **Consistency**: Database maintains valid state - total money remains constant, account balances never go negative\n• **Isolation**: Concurrent transactions see consistent snapshots - if Account C checks A's balance during transfer, they see either before or after state, never partial\n• **Durability**: Once transaction commits, changes persist even through system crashes via Write-Ahead Logging",
      "diagram": "graph TD\n    Start[Transfer $100: A → B] --> Atomic{Atomicity Check}\n    Atomic -->|Success| Consistent[Consistency Validation]\n    Atomic -->|Failure| Rollback[Complete Rollback]\n    Consistent --> Isolated[Isolation Control]\n    Isolated --> Durable[Durability Commit]\n    Durable --> Complete[Transaction Complete]\n    Rollback --> Failed[Transaction Failed]",
      "difficulty": "intermediate",
      "tags": [
        "acid",
        "transactions",
        "theory"
      ],
      "lastUpdated": "2025-12-12T09:08:19.686Z"
    },
    "gh-67": {
      "id": "gh-67",
      "question": "How does Database DevOps integrate database schema changes into CI/CD pipelines while ensuring data integrity and minimizing downtime?",
      "answer": "Database DevOps integrates database changes into automated pipelines using version-controlled migrations, automated testing, and staged deployment strategies.",
      "explanation": "Database DevOps applies DevOps principles to database development, enabling safe, automated database change management.\n\n## Core Practices:\n• **Version Control**: Store migration scripts in Git for complete change history\n• **Automated Testing**: Validate schemas, test data migrations, verify performance\n• **Staged Deployments**: Use blue-green or canary deployments for database changes\n• **Rollback Procedures**: Automated rollback scripts for failed changes\n• **Monitoring**: Track database performance and change impact\n\n## CI/CD Integration:\n- **Continuous Integration**: Automated schema validation and testing\n- **Continuous Delivery**: Staged deployment with automated rollback\n- **Database Versioning**: Sequential migration scripts with version tracking\n- **Environment Synchronization**: Consistent environments across dev/staging/prod",
      "diagram": "graph TD\n    Dev[Developer] --> VC[Version Control]\n    VC --> CI[CI Pipeline]\n    CI --> Schema[Schema Validation]\n    CI --> Test[Automated Tests]\n    Schema --> Build[Build Artifacts]\n    Test --> Build\n    Build --> CD[CD Pipeline]\n    CD --> Stage[Staging Deploy]\n    Stage --> Verify[Data Verification]\n    Verify --> Prod[Production Deploy]\n    Prod --> Monitor[Database Monitoring]\n    Monitor --> Alert{Issues?}\n    Alert -->|Yes| Rollback[Automated Rollback]\n    Alert -->|No| Success[Deploy Success]\n    Rollback --> CD",
      "difficulty": "beginner",
      "tags": [
        "db",
        "devops"
      ],
      "lastUpdated": "2025-12-12T09:11:16.660Z"
    },
    "da-125": {
      "id": "da-125",
      "question": "Explain database indexing and when should you use it?",
      "answer": "Database indexes are data structures that improve query speed by maintaining sorted references to data locations, trading increased write overhead for faster read operations.",
      "explanation": "**How Indexes Work**:\n- Create sorted data structures (B-tree, Hash) that point to actual data\n- Enable efficient data location without full table scans\n- Trade write performance for read speed improvements\n\n**When to Use**:\n- Frequently queried columns\n- WHERE clause conditions\n- JOIN operations\n- ORDER BY and GROUP BY columns\n\n**When NOT to Use**:\n- Small tables (< 100 rows)\n- Frequently updated columns\n- Low cardinality columns\n- Write-heavy workloads",
      "diagram": "graph TD\n    Query[SQL Query] --> Index[(Database Index)]\n    Index --> DataPoint[Data Location Pointer]\n    DataPoint --> TableData[(Table Data)]\n    WriteOp[Write Operation] --> IndexUpdate[Update Index]\n    IndexUpdate --> TableUpdate[Update Table]\n    style Index fill:#4ade80\n    style TableData fill:#fbbf24",
      "difficulty": "intermediate",
      "tags": [
        "sql",
        "indexing"
      ],
      "lastUpdated": "2025-12-12T09:11:28.015Z"
    },
    "da-128": {
      "id": "da-128",
      "question": "You have a banking system where users can transfer money between accounts. Design a transaction to handle a transfer of $500 from Account A (balance: $1000) to Account B (balance: $200). What happens if the system crashes after debiting Account A but before crediting Account B? How would you ensure data consistency?",
      "answer": "Use database transactions with ACID properties. Wrap both operations in a single transaction that either commits both or rolls back both.",
      "explanation": "## Database Transaction for Money Transfer\n\nThis scenario illustrates the critical importance of **ACID properties** in database transactions:\n\n### The Problem\nWithout proper transaction handling:\n1. Debit $500 from Account A (balance becomes $500)\n2. **System crashes here**\n3. Credit to Account B never happens\n4. **Result: $500 disappears from the system**\n\n### The Solution: ACID Transaction\n\n```sql\nBEGIN TRANSACTION;\n\n-- Check sufficient funds\nSELECT balance FROM accounts WHERE id = 'A' FOR UPDATE;\n\n-- Perform both operations atomically\nUPDATE accounts SET balance = balance - 500 WHERE id = 'A';\nUPDATE accounts SET balance = balance + 500 WHERE id = 'B';\n\nCOMMIT;\n```\n\n### ACID Properties Explained\n\n- **Atomicity**: Both operations succeed together or fail together\n- **Consistency**: Total money in system remains constant\n- **Isolation**: Other transactions can't see intermediate states\n- **Durability**: Once committed, changes survive system crashes\n\n### Additional Safeguards\n\n1. **Deadlock Prevention**: Always acquire locks in consistent order (e.g., by account ID)\n2. **Timeout Handling**: Set transaction timeouts to prevent indefinite locks\n3. **Retry Logic**: Implement exponential backoff for transient failures\n4. **Audit Trail**: Log all transaction attempts for reconciliation",
      "diagram": "graph TD\n    A[Start Transaction] --> B[Lock Account A]\n    B --> C[Check Balance >= $500]\n    C -->|Yes| D[Debit $500 from Account A]\n    C -->|No| E[Rollback - Insufficient Funds]\n    D --> F[Credit $500 to Account B]\n    F --> G[Commit Transaction]\n    G --> H[Release Locks]\n    \n    D -->|System Crash| I[Automatic Rollback]\n    F -->|System Crash| I\n    I --> J[Both Accounts Restored]\n    \n    E --> K[Transaction Failed]\n    \n    style A fill:#e1f5fe\n    style G fill:#c8e6c9\n    style I fill:#ffcdd2\n    style E fill:#ffcdd2",
      "difficulty": "intermediate",
      "tags": [
        "acid",
        "transactions"
      ],
      "lastUpdated": "2025-12-12T09:07:04.186Z"
    },
    "da-129": {
      "id": "da-129",
      "question": "What is the main difference between SQL and NoSQL databases in terms of data structure?",
      "answer": "SQL uses structured tables with fixed schemas, NoSQL uses flexible document/key-value/graph structures without fixed schemas.",
      "explanation": "## SQL vs NoSQL Data Structure\n\n**SQL Databases:**\n- Store data in **tables** with rows and columns\n- Require a **predefined schema** (structure must be defined before inserting data)\n- Data must conform to the schema (same columns for all rows)\n- Examples: MySQL, PostgreSQL, Oracle\n\n**NoSQL Databases:**\n- Store data in flexible formats:\n  - **Document stores** (JSON-like documents) - MongoDB, CouchDB\n  - **Key-value pairs** - Redis, DynamoDB\n  - **Column-family** - Cassandra, HBase\n  - **Graph databases** - Neo4j, Amazon Neptune\n- **Schema-less** or **schema-flexible**\n- Can store different structures in the same collection/table\n- Better for rapidly changing requirements\n\n**When to use NoSQL:**\n- Rapidly evolving data structures\n- Large scale applications requiring horizontal scaling\n- Semi-structured or unstructured data\n- Real-time applications",
      "diagram": "graph TD\n    A[Database Types] --> B[SQL Databases]\n    A --> C[NoSQL Databases]\n    \n    B --> D[Fixed Schema]\n    B --> E[Tables with Rows/Columns]\n    B --> F[ACID Compliance]\n    \n    C --> G[Flexible Schema]\n    C --> H[Multiple Data Models]\n    \n    H --> I[Document Store]\n    H --> J[Key-Value]\n    H --> K[Column-Family]\n    H --> L[Graph]\n    \n    I --> M[MongoDB]\n    J --> N[Redis]\n    K --> O[Cassandra]\n    L --> P[Neo4j]",
      "difficulty": "beginner",
      "tags": [
        "nosql",
        "mongodb"
      ],
      "lastUpdated": "2025-12-12T09:07:04.186Z"
    },
    "da-134": {
      "id": "da-134",
      "question": "You have a banking system where Account A transfers $100 to Account B, but during the transaction, Account B gets deleted by another process. The transfer uses READ COMMITTED isolation. What happens to the $100, and how would you prevent data inconsistency?",
      "answer": "Money disappears into deleted account. Use SELECT FOR UPDATE or SERIALIZABLE isolation to prevent phantom reads and ensure referential integrity.",
      "explanation": "## Transaction Isolation and Phantom Reads\n\nThis scenario demonstrates a **phantom read** problem in READ COMMITTED isolation:\n\n### What Happens:\n1. **Transaction T1** (transfer): Reads Account A balance, debits $100\n2. **Transaction T2** (deletion): Deletes Account B \n3. **Transaction T1**: Attempts to credit Account B - but it no longer exists\n4. **Result**: $100 vanishes from the system\n\n### Why READ COMMITTED Fails:\n- Only prevents **dirty reads** and **non-repeatable reads**\n- Does **NOT** prevent **phantom reads** (rows appearing/disappearing)\n- Account B's existence isn't locked during the transfer\n\n### Solutions:\n\n#### 1. Row-Level Locking\n```sql\nBEGIN;\nSELECT balance FROM accounts WHERE id = 'B' FOR UPDATE;\n-- This locks Account B, preventing deletion\nUPDATE accounts SET balance = balance - 100 WHERE id = 'A';\nUPDATE accounts SET balance = balance + 100 WHERE id = 'B';\nCOMMIT;\n```\n\n#### 2. SERIALIZABLE Isolation\n```sql\nSET TRANSACTION ISOLATION LEVEL SERIALIZABLE;\n-- Prevents all anomalies including phantom reads\n```\n\n#### 3. Application-Level Validation\n```sql\nBEGIN;\nIF NOT EXISTS (SELECT 1 FROM accounts WHERE id = 'B') THEN\n    ROLLBACK;\nEND IF;\n-- Proceed with transfer\nCOMMIT;\n```\n\n### Best Practice:\nUse **SELECT FOR UPDATE** on target accounts before any transfer to ensure atomicity and prevent phantom deletions.",
      "diagram": "graph TD\n    A[Transaction T1: Transfer $100] --> B[Read Account A: $500]\n    A --> C[Read Account B: $200]\n    D[Transaction T2: Delete Account B] --> E[DELETE FROM accounts WHERE id='B']\n    B --> F[UPDATE Account A: $400]\n    C --> G[UPDATE Account B: ???]\n    E --> H[Account B Deleted]\n    G --> I[ERROR: Account B not found]\n    F --> J[Money Lost: $100 vanished]\n    \n    K[Solution: SELECT FOR UPDATE] --> L[Lock Account B]\n    L --> M[Prevent Deletion]\n    M --> N[Safe Transfer]",
      "difficulty": "advanced",
      "tags": [
        "acid",
        "transactions"
      ],
      "lastUpdated": "2025-12-12T09:14:57.084Z"
    },
    "da-145": {
      "id": "da-145",
      "question": "You have a table `orders` with columns (id, user_id, amount, created_at) and need to find users who made their first purchase in the last 30 days AND have made at least 3 purchases total, but their average order value is below the overall platform average. Write an optimized SQL query.",
      "answer": "Use window functions with ROW_NUMBER() to find first purchases, COUNT() for total orders, and subqueries for average comparisons with proper indexing.",
      "explanation": "## Solution\n\n```sql\nWITH user_stats AS (\n  SELECT \n    user_id,\n    COUNT(*) as total_orders,\n    AVG(amount) as user_avg_amount,\n    MIN(created_at) as first_order_date\n  FROM orders\n  GROUP BY user_id\n  HAVING COUNT(*) >= 3\n),\nplatform_avg AS (\n  SELECT AVG(amount) as overall_avg\n  FROM orders\n)\nSELECT DISTINCT us.user_id\nFROM user_stats us\nCROSS JOIN platform_avg pa\nWHERE us.first_order_date >= CURRENT_DATE - INTERVAL '30 days'\n  AND us.user_avg_amount < pa.overall_avg;\n```\n\n## Key Concepts\n\n- **CTEs (Common Table Expressions)**: Break complex logic into readable chunks\n- **Window Functions**: Efficient for ranking and aggregations\n- **Performance Optimization**: \n  - Index on `(user_id, created_at)` for grouping\n  - Index on `created_at` for date filtering\n  - HAVING clause filters before final result set\n- **Cross Join**: Efficiently compare user averages to platform average\n\n## Alternative Approach\n```sql\nSELECT user_id\nFROM orders o1\nWHERE (SELECT MIN(created_at) FROM orders o2 WHERE o2.user_id = o1.user_id) >= CURRENT_DATE - INTERVAL '30 days'\nGROUP BY user_id\nHAVING COUNT(*) >= 3\n  AND AVG(amount) < (SELECT AVG(amount) FROM orders);\n```\n\nThis tests understanding of aggregation functions, subqueries, date operations, and query optimization strategies.",
      "diagram": "graph TD\n    A[orders table] --> B[Group by user_id]\n    B --> C[Calculate user stats]\n    C --> D[Filter: total_orders >= 3]\n    D --> E[Filter: first_order in last 30 days]\n    E --> F[Compare user_avg < platform_avg]\n    F --> G[Return qualifying user_ids]\n    \n    H[Platform Average] --> F\n    \n    style A fill:#e1f5fe\n    style G fill:#c8e6c9\n    style F fill:#fff3e0",
      "difficulty": "advanced",
      "tags": [
        "sql",
        "indexing"
      ],
      "lastUpdated": "2025-12-12T10:05:38.475Z"
    },
    "da-156": {
      "id": "da-156",
      "question": "What is the difference between DELETE and TRUNCATE commands in SQL?",
      "answer": "DELETE removes rows one by one and can use WHERE; TRUNCATE removes all rows at once, faster but can't be filtered.",
      "explanation": "## DELETE vs TRUNCATE\n\n### DELETE Command\n- Removes rows one at a time\n- Can use WHERE clause to filter specific rows\n- Triggers are fired for each deleted row\n- Slower for large datasets\n- Transaction log records each row deletion\n- Can be rolled back\n\n```sql\nDELETE FROM users WHERE age < 18;\n```\n\n### TRUNCATE Command\n- Removes all rows at once\n- Cannot use WHERE clause (removes entire table data)\n- No triggers fired\n- Much faster for large datasets\n- Minimal transaction logging\n- Cannot be rolled back in most databases\n- Resets auto-increment counters\n\n```sql\nTRUNCATE TABLE users;\n```\n\n### When to Use Each\n- Use **DELETE** when you need to remove specific rows or need transaction safety\n- Use **TRUNCATE** when you need to quickly empty an entire table",
      "diagram": "graph TD\n    A[SQL Data Removal] --> B[DELETE]\n    A --> C[TRUNCATE]\n    B --> D[Row-by-row removal]\n    B --> E[WHERE clause supported]\n    B --> F[Slower, logged]\n    B --> G[Can rollback]\n    C --> H[Bulk removal]\n    C --> I[No WHERE clause]\n    C --> J[Faster, minimal log]\n    C --> K[Cannot rollback]\n    C --> L[Resets auto-increment]",
      "difficulty": "beginner",
      "tags": [
        "sql",
        "indexing"
      ],
      "lastUpdated": "2025-12-13T01:08:56.357Z"
    },
    "da-172": {
      "id": "da-172",
      "question": "In a distributed database system, how would you implement a two-phase commit protocol to ensure atomicity across multiple nodes, and what are the key failure scenarios you must handle?",
      "answer": "Use coordinator to prepare/commit phases, handle node failures, timeouts, and network partitions with recovery protocols.",
      "explanation": "## Two-Phase Commit Implementation\n\n### Phase 1: Prepare\n1. **Coordinator sends prepare** to all participants\n2. **Participants validate** they can commit (lock resources, write to log)\n3. **Participants respond** with 'vote-commit' or 'vote-abort'\n\n### Phase 2: Commit\n- If all vote-commit: coordinator sends commit, participants acknowledge\n- If any vote-abort: coordinator sends abort, participants rollback\n\n### Failure Scenarios\n1. **Coordinator failure during prepare**: Participants timeout and abort\n2. **Coordinator failure after decision**: Use transaction logs to recover\n3. **Participant failure**: Coordinator retries, other participants wait\n4. **Network partition**: Timeout mechanisms prevent indefinite blocking\n\n### Optimizations\n- **Three-phase commit** adds pre-commit phase to reduce blocking\n- **Paxos/Raft** for coordinator election\n- **Timeout handling** with exponential backoff",
      "diagram": "graph TD\n    A[Coordinator] -->|Prepare Request| B[Participant 1]\n    A -->|Prepare Request| C[Participant 2]\n    A -->|Prepare Request| D[Participant 3]\n    B -->|Vote-Commit| A\n    C -->|Vote-Commit| A\n    D -->|Vote-Commit| A\n    A -->|Global Commit| B\n    A -->|Global Commit| C\n    A -->|Global Commit| D\n    B -->|Ack| A\n    C -->|Ack| A\n    D -->|Ack| A\n    E[Failure Scenario] --> F[Coordinator Crash]\n    E --> G[Participant Timeout]\n    E --> H[Network Partition]",
      "difficulty": "advanced",
      "tags": [
        "acid",
        "transactions"
      ],
      "lastUpdated": "2025-12-14T01:18:08.909Z"
    },
    "da-170": {
      "id": "da-170",
      "question": "You're building a banking system where users can transfer money between accounts. How would you design the transaction handling to ensure no money is lost or created during transfers, especially when the system crashes mid-transfer?",
      "answer": "Use ACID transactions with BEGIN, UPDATE accounts, COMMIT/ROLLBACK. Ensure atomicity by debiting source and crediting destination in single transaction.",
      "explanation": "## Transaction Design for Money Transfers\n\n### Key Requirements\n- **Atomicity**: Both debit and credit must succeed or fail together\n- **Consistency**: Total money in system remains constant\n- **Isolation**: Concurrent transfers don't interfere\n- **Durability**: Completed transfers survive system crashes\n\n### Implementation Strategy\n\n```sql\nBEGIN TRANSACTION;\n\n-- Check sufficient funds\nSELECT balance FROM accounts WHERE id = :source_id FOR UPDATE;\n\n-- Debit source account\nUPDATE accounts \nSET balance = balance - :amount \nWHERE id = :source_id AND balance >= :amount;\n\n-- Credit destination account  \nUPDATE accounts \nSET balance = balance + :amount \nWHERE id = :dest_id;\n\n-- Verify both operations succeeded\nIF (rowcount_source = 1 AND rowcount_dest = 1) THEN\n    COMMIT;\nELSE\n    ROLLBACK;\nEND IF;\n```\n\n### Crash Recovery\n- **Before COMMIT**: Transaction is rolled back on restart\n- **After COMMIT**: Changes are durable due to write-ahead logging\n- **During COMMIT**: Database ensures atomic completion\n\n### Concurrency Control\n- Use row-level locks with `FOR UPDATE`\n- Implement deadlock detection and retry logic\n- Consider isolation levels (READ COMMITTED vs SERIALIZABLE)\n\n### Monitoring\n- Track transaction success/failure rates\n- Monitor lock contention and deadlock frequency\n- Audit trail for all financial transactions",
      "diagram": "graph TD\n    A[Client Initiates Transfer] --> B[BEGIN TRANSACTION]\n    B --> C[LOCK Source Account]\n    C --> D[CHECK Balance >= Amount]\n    D --> E{Sufficient Funds?}\n    E -->|No| F[ROLLBACK - Return Error]\n    E -->|Yes| G[DEBIT Source Account]\n    G --> H[LOCK Destination Account]\n    H --> I[CREDIT Destination Account]\n    I --> J{Both Updates Successful?}\n    J -->|No| K[ROLLBACK - Return Error]\n    J -->|Yes| L[COMMIT Transaction]\n    L --> M[Release Locks]\n    M --> N[Return Success]\n    \n    O[System Crash] --> P{Crash Timing}\n    P -->|Before COMMIT| Q[Automatic ROLLBACK on Restart]\n    P -->|After COMMIT| R[Changes Preserved via WAL]\n    P -->|During COMMIT| S[Database Ensures Atomic Completion]",
      "difficulty": "intermediate",
      "tags": [
        "acid",
        "transactions"
      ],
      "lastUpdated": "2025-12-14T01:18:46.794Z"
    },
    "do-2": {
      "id": "do-2",
      "question": "What are the key differences between Blue/Green and Canary deployment strategies?",
      "answer": "Blue/Green enables instant switching between environments, while Canary gradually shifts traffic for safer rollouts.",
      "explanation": "**Blue/Green Deployment:**\n- Maintains two identical production environments (Blue=current, Green=new)\n- Deploy new version to Green environment and test thoroughly\n- Switch load balancer to route 100% traffic to Green instantly\n- Enables immediate rollback but requires double infrastructure resources\n\n**Canary Deployment:**\n- Gradually routes small percentage of traffic to new version\n- Start with 5% traffic to new version, monitor metrics and errors\n- Incrementally increase to 25%, 50%, then 100% based on performance\n- Lower risk and resource usage but slower deployment process",
      "diagram": "graph TD\n    A[Users] --> B[Load Balancer]\n    B -->|100%| C[Blue Environment v1]\n    B -.->|Switch| D[Green Environment v2]\n    \n    E[Users] --> F[Load Balancer]\n    F -->|95%| G[Version 1]\n    F -->|5%| H[Version 2 Canary]",
      "difficulty": "intermediate",
      "tags": [
        "deployment",
        "strategy",
        "cicd",
        "jenkins"
      ],
      "lastUpdated": "2025-12-12T09:10:00.382Z"
    },
    "do-3": {
      "id": "do-3",
      "question": "What is Infrastructure as Code (IaC)? Why use Terraform?",
      "answer": "Managing infrastructure through code (git) rather than manual console clicks.",
      "explanation": "**Benefits**:\n- **Reproducibility**: Same env every time.\n- **Version Control**: Git history of infra changes.\n- **Automation**: One click deploy.\n\n**Terraform vs Ansible**:\n- Terraform: Provisioning (Building the server/VPC).\n- Ansible: Configuration (Installing software on the server).",
      "diagram": "\ngraph LR\n    Code[IaC Code] --> Git[Git Repo]\n    Git --> TF[Terraform]\n    TF --> Cloud[Cloud Infra]\n",
      "difficulty": "beginner",
      "tags": [
        "infra",
        "automation",
        "terraform"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-1": {
      "id": "gh-1",
      "question": "What are the core principles and practices of DevOps, and how does it bridge the gap between development and operations teams?",
      "answer": "DevOps combines development and operations through automation, continuous integration/delivery, and shared responsibility for the entire software lifecycle.",
      "explanation": "DevOps is a cultural and technical movement that breaks down silos between development and operations teams. Key principles include:\n\n• **Automation**: Automating build, test, deployment, and infrastructure provisioning processes\n• **Continuous Integration/Continuous Delivery (CI/CD)**: Frequent code integration and automated deployment pipelines\n• **Infrastructure as Code (IaC)**: Managing infrastructure through version-controlled code\n• **Monitoring and Observability**: Real-time monitoring of applications and infrastructure\n• **Collaboration**: Shared ownership and responsibility across the entire software lifecycle\n• **Feedback Loops**: Quick feedback from production back to development teams\n\nDevOps practices enable faster delivery, improved quality, reduced deployment risks, and better alignment between business objectives and technical implementation.",
      "diagram": "graph TD\n    A[Code Commit] --> B[Build & Test]\n    B --> C[Deploy to Staging]\n    C --> D[Automated Testing]\n    D --> E[Deploy to Production]\n    E --> F[Monitor & Log]\n    F --> G[Feedback]\n    G --> A\n    H[Infrastructure as Code] --> C\n    I[Security Scanning] --> D",
      "difficulty": "beginner",
      "tags": [
        "basics"
      ],
      "lastUpdated": "2025-12-12T09:10:12.989Z"
    },
    "gh-2": {
      "id": "gh-2",
      "question": "What are the benefits of DevOps?",
      "answer": "The main benefits of DevOps include:",
      "explanation": "The main benefits of DevOps include:\n\n1. Faster delivery of features\n2. More stable operating environments\n3. Improved communication and collaboration\n4. More time to innovate (rather than fix/maintain)\n5. Reduced deployment failures and rollbacks\n6. Shorter mean time to recovery",
      "diagram": "\ngraph TD\n    DevOps --> Speed[Faster Delivery]\n    DevOps --> Stable[Stability]\n    DevOps --> Collab[Collaboration]\n    DevOps --> MTTR[Lower MTTR]\n",
      "difficulty": "beginner",
      "tags": [
        "basics"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-3": {
      "id": "gh-3",
      "question": "What is Continuous Integration and how does it improve software development quality?",
      "answer": "Continuous Integration (CI) automates building and testing code changes frequently to catch bugs early and maintain code quality in collaborative development.",
      "explanation": "Continuous Integration (CI) is a development practice where developers integrate code into a shared repository frequently, with each integration verified by automated builds and tests.\n\n## Key Benefits:\n- **Early bug detection** - Issues caught immediately after commit\n- **Reduced integration conflicts** - Small, frequent changes prevent merge hell\n- **Improved code quality** - Automated tests enforce standards\n- **Faster feedback loops** - Developers know quickly if changes work\n- **Consistent builds** - Automated process eliminates environment differences\n\n## Core Practices:\n- Maintain single source repository\n- Automate build and test processes\n- Commit to mainline daily\n- Keep builds fast and reliable\n- Test in production-like environment\n- Make results visible to entire team",
      "diagram": "graph TD\n    Dev[Developer] --> Commit[Commit Code]\n    Commit --> Repo[Central Repository]\n    Repo --> CI[CI Pipeline]\n    CI --> Build[Automated Build]\n    Build --> Test[Automated Tests]\n    Test --> Quality[Code Quality Check]\n    Quality --> Success{Tests Pass?}\n    Success -->|Yes| Deploy[Ready for Deployment]\n    Success -->|No| Feedback[Immediate Feedback]",
      "difficulty": "beginner",
      "tags": [
        "basics"
      ],
      "lastUpdated": "2025-12-12T09:11:40.451Z"
    },
    "gh-4": {
      "id": "gh-4",
      "question": "What is Docker and how does containerization differ from traditional virtualization?",
      "answer": "Docker is a containerization platform that packages applications with their dependencies into isolated containers. Unlike VMs, containers share the host OS kernel, making them lightweight and faster to start up.",
      "explanation": "# Docker Containerization\n\n## Key Concepts:\n- **Containers**: Isolated runtime environments that bundle applications with all dependencies\n- **Images**: Read-only templates used to create containers\n- **Dockerfile**: Text file with instructions to build Docker images\n\n## Benefits over Virtual Machines:\n- **Lightweight**: Share host OS kernel (no guest OS needed)\n- **Fast startup**: Seconds vs minutes for VMs\n- **Resource efficient**: Lower memory and CPU overhead\n- **Portability**: Run consistently across different environments\n\n## Common Use Cases:\n- Application deployment and scaling\n- Development environment consistency\n- Microservices architecture\n- CI/CD pipeline integration",
      "diagram": "graph TD\n    A[Developer Code] --> B[Dockerfile]\n    B --> C[Docker Build]\n    C --> D[Docker Image]\n    D --> E[Container Runtime]\n    E --> F[Running Container]\n    \n    G[Host OS] --> H[Docker Engine]\n    H --> E\n    \n    I[Application] --> J[Libraries]\n    J --> K[Dependencies]\n    K --> F\n    \n    L[VM] --> M[Guest OS]\n    M --> N[App + Deps]\n    \n    style F fill:#e1f5fe\n    style H fill:#f3e5f5\n    style N fill:#ffebee",
      "difficulty": "beginner",
      "tags": [
        "docker",
        "containers"
      ],
      "lastUpdated": "2025-12-12T09:11:47.712Z"
    },
    "gh-5": {
      "id": "gh-5",
      "question": "What is the difference between Docker Image and Docker Container?",
      "answer": "Docker Image is a read-only template/blueprint, while Docker Container is a running instance of that image.",
      "explanation": "- **Docker Image:** A read-only template containing application code, runtime, libraries, dependencies, and system tools. Images are built from Dockerfiles and stored in registries. They serve as blueprints for creating containers.\n\n- **Docker Container:** A runnable instance of an image. Containers are isolated processes that can be created, started, stopped, and deleted. Multiple containers can run from the same image simultaneously.\n\n- **Key Differences:**\n  - Images are immutable templates; containers are mutable runtime instances\n  - Images are stored; containers are executed\n  - One image can spawn multiple containers\n  - Containers have their own filesystem layer on top of the image",
      "diagram": "graph TD\n    A[Dockerfile] --> B[Docker Image]\n    B --> C[Container 1]\n    B --> D[Container 2]\n    B --> E[Container 3]\n    F[Registry] --> B\n    C --> G[Running Process]\n    D --> H[Running Process]\n    E --> I[Running Process]",
      "difficulty": "beginner",
      "tags": [
        "docker",
        "containers"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-6": {
      "id": "gh-6",
      "question": "What is a Dockerfile and how does it enable containerized application deployment?",
      "answer": "A Dockerfile is a text file containing instructions to build Docker images, defining the environment, dependencies, and commands needed to run an application in a container.",
      "explanation": "A Dockerfile is a declarative text document that automates Docker image creation through a series of instructions:\n\n• **Base Image**: Starts with a foundation OS or runtime using `FROM`\n• **Environment Setup**: Configures working directories, environment variables, and system dependencies\n• **Application Code**: Copies source files and installs application-specific dependencies\n• **Runtime Configuration**: Exposes ports and defines the default command to execute\n• **Layer Caching**: Each instruction creates a new layer, enabling efficient builds and updates\n• **Reproducibility**: Ensures consistent environments across development, testing, and production\n\n**Common Dockerfile Instructions:**\n```dockerfile\nFROM node:18-alpine          # Base image\nWORKDIR /usr/src/app         # Set working directory\nCOPY package*.json ./        # Copy dependency files\nRUN npm ci --only=production # Install dependencies\nCOPY . .                     # Copy application code\nEXPOSE 3000                  # Document port usage\nUSER node                    # Run as non-root user\nCMD [\"npm\", \"start\"]         # Default command\n```\n\n**Best Practices:**\n• Use multi-stage builds to reduce image size\n• Leverage .dockerignore to exclude unnecessary files\n• Run containers as non-root users for security\n• Minimize layers by combining RUN commands\n• Use specific base image tags for consistency",
      "diagram": "graph TD\n    A[FROM base:tag] --> B[WORKDIR /app]\n    B --> C[COPY package.json]\n    C --> D[RUN install deps]\n    D --> E[COPY source code]\n    E --> F[EXPOSE ports]\n    F --> G[USER non-root]\n    G --> H[CMD start app]\n    \n    I[Build Context] --> C\n    I --> E\n    \n    H --> J[Container Runtime]\n    \n    style A fill:#e1f5fe\n    style H fill:#c8e6c9\n    style J fill:#fff3e0",
      "difficulty": "beginner",
      "tags": [
        "docker",
        "containers"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-7": {
      "id": "gh-7",
      "question": "What is Kubernetes and how does it orchestrate containerized applications at scale?",
      "answer": "Kubernetes is an open-source container orchestration platform that automates deployment, scaling, and management of containerized applications across clusters of hosts.",
      "explanation": "**Kubernetes (K8s)** is a production-grade container orchestration platform that manages containerized applications at scale.\n\n## Core Capabilities:\n- **Automated Deployment**: Roll out new versions with zero downtime\n- **Self-Healing**: Restart failed containers and replace unhealthy nodes\n- **Horizontal Scaling**: Automatically adjust application instances based on load\n- **Service Discovery**: Enable containers to find and communicate with each other\n- **Load Balancing**: Distribute traffic across multiple container instances\n- **Storage Orchestration**: Manage persistent storage for stateful applications\n\n## Key Components:\n- **Master Node**: Control plane (API Server, Scheduler, Controller Manager)\n- **Worker Nodes**: Run containerized applications (Kubelet, Container Runtime)\n- **Pods**: Smallest deployable units containing one or more containers\n- **Services**: Network endpoints for accessing pods\n- **Deployments**: Manage pod replicas and rolling updates\n\n## Why Use Kubernetes:\n- **Portability**: Run anywhere (on-prem, cloud, hybrid)\n- **Scalability**: Handle thousands of containers and nodes\n- **Reliability**: Built-in fault tolerance and recovery mechanisms\n- **Ecosystem**: Extensive tooling and community support",
      "diagram": "graph TD\n    Master[Master Node] --> API[API Server]\n    Master --> Scheduler[Scheduler]\n    Master --> Controller[Controller Manager]\n    \n    Worker1[Worker Node 1] --> Kubelet1[Kubelet]\n    Worker1 --> Runtime1[Container Runtime]\n    Worker1 --> Pod1[Pod 1]\n    Worker1 --> Pod2[Pod 2]\n    \n    Worker2[Worker Node 2] --> Kubelet2[Kubelet]\n    Worker2 --> Runtime2[Container Runtime]\n    Worker2 --> Pod3[Pod 3]\n    \n    API --> Kubelet1\n    API --> Kubelet2\n    Scheduler --> Kubelet1\n    Scheduler --> Kubelet2\n    \n    Service[Service] --> Pod1\n    Service --> Pod2\n    Service --> Pod3\n    \n    Client[Client] --> Service",
      "difficulty": "beginner",
      "tags": [
        "k8s",
        "orchestration"
      ],
      "lastUpdated": "2025-12-12T09:59:12.880Z"
    },
    "gh-8": {
      "id": "gh-8",
      "question": "What are the main components of Kubernetes architecture?",
      "answer": "Kubernetes architecture consists of the following main components:",
      "explanation": "Kubernetes architecture consists of the following main components:\n\n1. **Master Node Components:**\n- API Server\n- etcd\n- Controller Manager\n- Scheduler\n\n2. **Worker Node Components:**\n- Kubelet\n- Container Runtime\n- Kube Proxy",
      "diagram": "\ngraph TD\n    Master[Master Node] --> API[API Server]\n    Master --> etcd[(etcd)]\n    Worker[Worker Node] --> Kubelet\n    Worker --> Runtime[Container Runtime]\n",
      "difficulty": "intermediate",
      "tags": [
        "k8s",
        "orchestration"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-9": {
      "id": "gh-9",
      "question": "What is a Pod in Kubernetes and why is it considered the smallest deployable unit?",
      "answer": "A Pod is Kubernetes' smallest deployable unit that encapsulates one or more containers with shared storage, networking, and runtime specifications.",
      "explanation": "A Pod is the fundamental building block of Kubernetes applications that represents a single running process in your cluster.\n\n## Key Components\n- **Containers**: One or more tightly coupled containers that work together\n- **Shared Network**: All containers share the same IP address and network namespace\n- **Shared Storage**: Access to shared volumes for data persistence and communication\n- **Runtime Context**: Configuration for how containers should run together\n\n## Why Smallest Deployable Unit?\n- **Atomic Operation**: Pods are scheduled, created, and destroyed as a single unit\n- **Resource Sharing**: Containers in a Pod can efficiently share resources via localhost\n- **Co-location**: Ensures related processes run on the same node for performance\n- **Lifecycle Management**: All containers in a Pod share the same fate (start/stop together)\n\n## Common Use Cases\n- **Single Container**: Most common scenario (web server, database)\n- **Multi-Container**: Main app + sidecar proxy for logging/monitoring\n- **Helper Processes**: Application + background worker processes\n\n## Example YAML\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: web-app\nspec:\n  containers:\n  - name: nginx\n    image: nginx:latest\n    ports:\n    - containerPort: 80\n  - name: log-agent\n    image: fluentd:latest\n    volumeMounts:\n    - name: shared-logs\n      mountPath: /var/log/nginx\n```",
      "diagram": "graph TD\n    A[Pod: Smallest Unit] --> B[Shared Network Namespace]\n    A --> C[Shared Storage Volumes]\n    A --> D[Container 1: nginx]\n    A --> E[Container 2: log-agent]\n    B --> F[Single IP Address]\n    B --> G[Shared Port Space]\n    C --> H[Persistent Data]\n    I[Kubernetes Scheduler] --> A\n    J[Worker Node] --> A\n    K[Deployment Controller] --> A",
      "difficulty": "beginner",
      "tags": [
        "k8s",
        "orchestration"
      ],
      "lastUpdated": "2025-12-12T10:15:00.000Z"
    },
    "gh-10": {
      "id": "gh-10",
      "question": "What is CI/CD Pipeline?",
      "answer": "A CI/CD Pipeline is a series of steps that must be performed in order to deliver a new version of software. A pipeline typically includes stages for:",
      "explanation": "A CI/CD Pipeline is a series of steps that must be performed in order to deliver a new version of software. A pipeline typically includes stages for:\n\n1. Building the code\n2. Running automated tests\n3. Deploying to staging/production environments\n\nExample of a basic Jenkins Pipeline:\n```groovy\npipeline {\nagent any\nstages {\nstage('Build') {\nsteps {\nsh 'npm install'\nsh 'npm run build'\n}\n}\nstage('Test') {\nsteps {\nsh 'npm run test'\n}\n}\nstage('Deploy') {\nsteps {\nsh './deploy.sh'\n}\n}\n}\n}\n```",
      "diagram": "\ngraph LR\n    Build[Build] --> Test[Test]\n    Test --> Stage[Staging]\n    Stage --> Prod[Production]\n",
      "difficulty": "beginner",
      "tags": [
        "cicd",
        "automation"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-11": {
      "id": "gh-11",
      "question": "What is Jenkins and how does it facilitate continuous integration and continuous delivery (CI/CD) in modern software development workflows?",
      "answer": "Jenkins is an open-source automation server that enables CI/CD by automating build, test, and deployment processes through extensible plugins and distributed architecture.",
      "explanation": "Jenkins is a powerful automation server that plays a crucial role in DevOps practices by enabling continuous integration and continuous delivery (CI/CD) pipelines.\n\n**Key Features:**\n- **Extensible Plugin System**: 1000+ plugins for integration with various tools\n- **Distributed Builds**: Master-agent architecture for scaling workloads\n- **Pipeline as Code**: Jenkinsfile for defining build workflows\n- **Built-in GUI**: Intuitive web interface for configuration and monitoring\n- **Automated Testing**: Integration with testing frameworks for quality assurance\n- **Deployment Automation**: Seamless integration with deployment tools\n\n**Architecture Components:**\n- **Jenkins Master**: Orchestrates builds and manages agents\n- **Jenkins Agents**: Execute build tasks in distributed environments\n- **Pipeline Engine**: Processes Jenkinsfile definitions\n- **Plugin Ecosystem**: Extends functionality for various tools and platforms",
      "diagram": "graph TD\n    Dev[Developer Pushes Code] --> SCM[SCM Repository]\n    SCM --> Webhook[Webhook Trigger]\n    Webhook --> Jenkins[Jenkins Master]\n    Jenkins --> Pipeline[CI/CD Pipeline]\n    Pipeline --> Build[Build Stage]\n    Pipeline --> Test[Test Stage]\n    Pipeline --> Deploy[Deploy Stage]\n    Build --> Agent1[Jenkins Agent 1]\n    Test --> Agent2[Jenkins Agent 2]\n    Deploy --> Agent3[Jenkins Agent 3]\n    Jenkins --> Monitor[Build Monitoring]\n    Jenkins --> Notif[Notifications]",
      "difficulty": "advanced",
      "tags": [
        "cicd",
        "automation"
      ],
      "lastUpdated": "2025-12-12T10:04:08.627Z"
    },
    "gh-12": {
      "id": "gh-12",
      "question": "What is cloud computing and what are its key service models?",
      "answer": "Cloud computing delivers computing services over the internet, offering on-demand access to servers, storage, databases, and software.",
      "explanation": "Cloud computing is the delivery of computing services—including servers, storage, databases, networking, software, analytics, and intelligence—over the Internet (\"the cloud\") to offer faster innovation, flexible resources, and economies of scale.\n\n**Key Benefits:**\n• **Cost Efficiency** - Pay only for what you use, no upfront infrastructure costs\n• **Scalability** - Easily scale resources up or down based on demand\n• **Accessibility** - Access services from anywhere with internet connection\n• **Reliability** - Built-in redundancy and disaster recovery\n\n**Service Models:**\n• **IaaS** (Infrastructure as a Service) - Virtual machines, storage, networks\n• **PaaS** (Platform as a Service) - Development platforms and tools\n• **SaaS** (Software as a Service) - Ready-to-use applications",
      "diagram": "graph TD\n    Cloud[Cloud Computing] --> IaaS[Infrastructure as a Service]\n    Cloud --> PaaS[Platform as a Service]\n    Cloud --> SaaS[Software as a Service]\n    IaaS --> VM[Virtual Machines]\n    IaaS --> Storage[(Storage)]\n    IaaS --> Network[Networking]\n    PaaS --> DevTools[Development Tools]\n    PaaS --> Runtime[Runtime Environment]\n    SaaS --> Email[Email Services]\n    SaaS --> CRM[CRM Applications]",
      "difficulty": "beginner",
      "tags": [
        "cloud",
        "aws",
        "azure",
        "gcp"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-13": {
      "id": "gh-13",
      "question": "What is AWS (Amazon Web Services)?",
      "answer": "AWS is a comprehensive and widely adopted cloud platform, offering over 200 fully featured services from data centers globally. Key services include:",
      "explanation": "AWS is a comprehensive and widely adopted cloud platform, offering over 200 fully featured services from data centers globally. Key services include:\n\n1. **Compute:**\n- EC2 (Elastic Compute Cloud)\n- Lambda (Serverless Computing)\n- ECS (Elastic Container Service)\n\n2. **Storage:**\n- S3 (Simple Storage Service)\n- EBS (Elastic Block Store)\n- EFS (Elastic File System)\n\n3. **Database:**\n- RDS (Relational Database Service)\n- DynamoDB (NoSQL Database)\n- Redshift (Data Warehouse)",
      "diagram": "\ngraph TD\n    AWS --> EC2[EC2 Compute]\n    AWS --> S3[(S3 Storage)]\n    AWS --> RDS[(RDS Database)]\n    AWS --> Lambda[Lambda]\n",
      "difficulty": "beginner",
      "tags": [
        "cloud",
        "aws",
        "azure",
        "gcp"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-14": {
      "id": "gh-14",
      "question": "What are the core service categories in Microsoft Azure and how do they work together to support cloud applications?",
      "answer": "Azure provides compute, storage, networking, and database services that integrate to build scalable cloud solutions.",
      "explanation": "Microsoft Azure is a comprehensive cloud computing platform offering multiple service categories:\n\n**Compute Services:**\n- Virtual Machines - IaaS for custom OS environments\n- App Service - PaaS for web applications\n- Azure Functions - Serverless compute for event-driven workloads\n- Container Instances - Containerized application hosting\n\n**Storage Services:**\n- Blob Storage - Object storage for unstructured data\n- File Storage - Managed file shares\n- Queue Storage - Message queuing service\n- Table Storage - NoSQL key-value store\n\n**Networking Services:**\n- Virtual Network - Isolated network environments\n- Load Balancer - Traffic distribution\n- Application Gateway - Web traffic load balancer with SSL termination\n- VPN Gateway - Secure connections to on-premises\n\n**Database Services:**\n- SQL Database - Managed relational database\n- Cosmos DB - Multi-model NoSQL database\n- Redis Cache - In-memory data store\n\nThese services integrate seamlessly, allowing applications to scale horizontally across regions while maintaining security and performance.",
      "diagram": "graph TD\n    Azure[Microsoft Azure]\n    Azure --> Compute[Compute Services]\n    Azure --> Storage[Storage Services]\n    Azure --> Network[Networking]\n    Azure --> Database[Database Services]\n    \n    Compute --> VM[Virtual Machines]\n    Compute --> AppSvc[App Service]\n    Compute --> Functions[Azure Functions]\n    \n    Storage --> Blob[Blob Storage]\n    Storage --> Files[File Storage]\n    Storage --> Queue[Queue Storage]\n    \n    Network --> VNet[Virtual Network]\n    Network --> LB[Load Balancer]\n    Network --> Gateway[App Gateway]\n    \n    Database --> SQL[SQL Database]\n    Database --> Cosmos[Cosmos DB]\n    Database --> Redis[Redis Cache]",
      "difficulty": "beginner",
      "tags": [
        "cloud",
        "aws",
        "azure",
        "gcp"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-15": {
      "id": "gh-15",
      "question": "What are the different types of cloud services?",
      "answer": "The main types of cloud services are:",
      "explanation": "The main types of cloud services are:\n\n1. **IaaS (Infrastructure as a Service):**\n- Provides virtualized computing resources\n- Examples: AWS EC2, Azure VMs\n\n2. **PaaS (Platform as a Service):**\n- Provides platform allowing customers to develop, run, and manage applications\n- Examples: Heroku, Google App Engine\n\n3. **SaaS (Software as a Service):**\n- Provides software applications over the internet\n- Examples: Salesforce, Google Workspace\n\n4. **FaaS (Function as a Service):**\n- Provides serverless computing capabilities\n- Examples: AWS Lambda, Azure Functions",
      "diagram": "\ngraph TD\n    IaaS[IaaS - Infra] --> PaaS[PaaS - Platform]\n    PaaS --> SaaS[SaaS - Software]\n    FaaS[FaaS - Serverless]\n",
      "difficulty": "intermediate",
      "tags": [
        "cloud",
        "aws",
        "azure",
        "gcp"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-16": {
      "id": "gh-16",
      "question": "What is Infrastructure as Code and why has it become essential for modern DevOps practices?",
      "answer": "Infrastructure as Code (IaC) manages and provisions infrastructure through version-controlled definition files instead of manual configuration or interactive tools.",
      "explanation": "**Infrastructure as Code (IaC)** transforms infrastructure management by treating infrastructure like application code:\n\n• **Declarative Configuration**: Define desired infrastructure state using code (HCL, YAML, JSON)\n• **Version Control**: Track all infrastructure changes in Git with full audit history\n• **Automation**: Provision and manage resources programmatically without manual intervention\n• **Reproducibility**: Create identical environments consistently across development, staging, and production\n• **Collaboration**: Enable teams to review infrastructure changes through pull requests\n\n**Key Benefits:**\n- Eliminates configuration drift between environments\n- Reduces human errors from manual setup\n- Enables rapid environment provisioning\n- Provides audit trails for compliance\n- Supports disaster recovery through code recreation",
      "diagram": "graph TD\n    A[Developer writes IaC] --> B[Git Repository]\n    B --> C[CI/CD Pipeline]\n    C --> D[terraform plan]\n    D --> E[Review Changes]\n    E --> F[terraform apply]\n    F --> G[Cloud Resources]\n    G --> H[Infrastructure Ready]\n    I[Monitor & Validate] --> J[Feedback Loop]\n    J --> A",
      "difficulty": "beginner",
      "tags": [
        "iac",
        "terraform",
        "ansible"
      ],
      "lastUpdated": "2025-12-12T10:05:12.467Z"
    },
    "gh-17": {
      "id": "gh-17",
      "question": "What is Terraform and how does it implement Infrastructure as Code (IaC) workflows?",
      "answer": "Terraform is an open-source Infrastructure as Code (IaC) tool by HashiCorp that allows you to define, provision, and manage cloud infrastructure using declarative configuration files.",
      "explanation": "Terraform enables infrastructure management through:\n\n- **Declarative Configuration**: Uses HCL (HashiCorp Configuration Language) to define desired infrastructure state\n- **Provider Architecture**: Supports multiple cloud providers (AWS, Azure, GCP, etc.) through plugins\n- **State Management**: Maintains a state file to track infrastructure resources and changes\n- **Workflow**: Follows plan-apply-destroy lifecycle for safe infrastructure changes\n- **Modularity**: Supports modules for reusable infrastructure components\n\n**Key Benefits**:\n- Version control infrastructure alongside application code\n- Automated provisioning and consistent deployments\n- Cost management through resource tracking\n- Multi-cloud and hybrid cloud support",
      "diagram": "graph TD\n    A[Write HCL Configuration] --> B[terraform init]\n    B --> C[terraform plan]\n    C --> D{Review Changes}\n    D -->|Approved| E[terraform apply]\n    D -->|Reject| F[Modify Configuration]\n    E --> G[Provision Resources]\n    G --> H[Update State File]\n    F --> A\n    H --> I[terraform destroy]\n    I --> J[Clean up Resources]",
      "difficulty": "beginner",
      "tags": [
        "iac",
        "terraform",
        "ansible"
      ],
      "lastUpdated": "2025-12-12T10:05:18.550Z"
    },
    "gh-18": {
      "id": "gh-18",
      "question": "What is Ansible and how does it work for infrastructure automation?",
      "answer": "Ansible is an agentless automation tool that uses SSH and YAML playbooks to manage infrastructure configuration and deployment.",
      "explanation": "Ansible is a powerful open-source automation platform that simplifies IT infrastructure management through several key features:\n\n• **Agentless Architecture**: No need to install agents on target machines - uses SSH for Linux/Unix and WinRM for Windows\n• **YAML Playbooks**: Human-readable automation scripts that define desired system states\n• **Idempotent Operations**: Running the same playbook multiple times produces consistent results\n• **Inventory Management**: Organizes and groups target hosts for efficient automation\n• **Module System**: Extensive library of pre-built modules for common tasks\n\n**Key Use Cases:**\n• Configuration management and system setup\n• Application deployment and updates\n• Infrastructure provisioning\n• Security compliance and patching\n• Orchestration of complex multi-tier applications\n\n**Example Ansible Playbook:**\n```yaml\n---\n- name: Configure web servers\n  hosts: webservers\n  become: yes\n  tasks:\n    - name: Install nginx\n      apt:\n        name: nginx\n        state: present\n        update_cache: yes\n    \n    - name: Start and enable nginx\n      systemd:\n        name: nginx\n        state: started\n        enabled: yes\n    \n    - name: Deploy website content\n      copy:\n        src: /local/website/\n        dest: /var/www/html/\n        owner: www-data\n        group: www-data\n```\n\n**Advantages:**\n• Simple learning curve with YAML syntax\n• No additional infrastructure required\n• Strong community and enterprise support\n• Integration with cloud platforms and CI/CD pipelines",
      "diagram": "graph TD\n    A[Control Node] --> B[Inventory File]\n    A --> C[Playbook YAML]\n    B --> D[Target Hosts]\n    C --> E[Tasks & Modules]\n    A --> F[SSH Connection]\n    F --> D\n    E --> G[Idempotent Execution]\n    G --> H[Desired State]\n    D --> H",
      "difficulty": "beginner",
      "tags": [
        "iac",
        "terraform",
        "ansible"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-19": {
      "id": "gh-19",
      "question": "What is monitoring in DevOps and how does it differ from observability?",
      "answer": "Monitoring is the practice of collecting and analyzing system metrics to detect issues, while observability provides deeper insights into system behavior.",
      "explanation": "**Monitoring in DevOps** involves systematically collecting, analyzing, and acting on telemetry data to ensure system reliability and performance.\n\n## Key Components:\n\n### 1. Infrastructure Monitoring\n- **Metrics**: CPU, memory, disk usage, network throughput\n- **Health**: Server uptime, service availability, resource capacity\n- **Tools**: Prometheus, DataDog, New Relic\n\n### 2. Application Monitoring\n- **Performance**: Response times, latency, throughput\n- **Errors**: Exception rates, failure patterns, error budgets\n- **Business**: User engagement, conversion rates, feature adoption\n\n### 3. Log Management\n- **Collection**: Centralized log aggregation from all services\n- **Analysis**: Pattern recognition, root cause analysis\n- **Retention**: Compliance and forensic investigation\n\n### 4. Alerting Systems\n- **Thresholds**: Predefined limits trigger notifications\n- **Anomaly Detection**: ML-based identification of unusual patterns\n- **Escalation**: Tiered response procedures\n\n## Monitoring vs Observability:\n- **Monitoring**: Answers 'what' is happening (known metrics)\n- **Observability**: Answers 'why' it's happening (deep insights)\n- **Complementary**: Monitoring detects issues, observability explains them",
      "diagram": "graph TD\n    A[Applications] --> B[Metrics Collection]\n    C[Infrastructure] --> B\n    D[User Experience] --> B\n    \n    B --> E[Time Series Database]\n    B --> F[Log Aggregation]\n    \n    E --> G[Visualization Dashboards]\n    F --> H[Log Analysis Tools]\n    \n    G --> I[Alerting System]\n    H --> I\n    \n    I --> J[DevOps Team]\n    J --> K[Incident Response]\n    K --> L[System Improvement]\n    L --> A\n    \n    style A fill:#e1f5fe\n    style E fill:#fff3e0\n    style G fill:#c8e6c9\n    style I fill:#ffebee",
      "difficulty": "beginner",
      "tags": [
        "observability",
        "monitoring",
        "logging"
      ],
      "lastUpdated": "2025-12-12T10:04:39.942Z"
    },
    "gh-20": {
      "id": "gh-20",
      "question": "What are the components of the ELK Stack and how do they work together to process log data?",
      "answer": "The ELK Stack is a centralized logging solution consisting of Elasticsearch (search engine), Logstash (data processing), and Kibana (visualization).",
      "explanation": "The ELK Stack is a comprehensive log management and analytics platform:\n\n## Components:\n\n* **Elasticsearch:** Distributed search and analytics engine that stores and indexes data\n* **Logstash:** Data collection and transformation pipeline that processes logs from multiple sources\n* **Kibana:** Visualization dashboard for exploring and analyzing data stored in Elasticsearch\n\n## Data Flow:\n\n1. Logstash collects logs from various sources (applications, servers, services)\n2. Logstash processes, filters, and transforms the log data\n3. Processed data is indexed and stored in Elasticsearch\n4. Kibana provides real-time dashboards and visualizations for analysis\n\n## Common Use Cases:\n\n* Centralized log aggregation\n* Security monitoring and threat detection\n* Application performance monitoring\n* Business intelligence and analytics\n* Debugging and troubleshooting",
      "diagram": "graph TD\n    A[Application Logs] --> B[Logstash]\n    C[Server Logs] --> B\n    D[System Metrics] --> B\n    B --> E[Elasticsearch]\n    E --> F[Kibana Dashboard]\n    G[Beats] --> B\n    F --> H[Visualizations]\n    F --> I[Alerts]\n    F --> J[Reports]",
      "difficulty": "beginner",
      "tags": [
        "observability",
        "monitoring",
        "logging"
      ],
      "lastUpdated": "2025-12-12T10:03:43.564Z"
    },
    "gh-21": {
      "id": "gh-21",
      "question": "How does Prometheus implement a pull-based monitoring system, and what are the key components in its architecture?",
      "answer": "Prometheus uses pull-based metric collection with a time-series database, query language (PromQL), and alerting system for monitoring cloud-native applications.",
      "explanation": "Prometheus is a cloud-native monitoring system that scrapes metrics from HTTP endpoints:\n\n## Core Components\n- **Prometheus Server**: Collects and stores time-series data\n- **Exporters**: Expose metrics in Prometheus format\n- **Service Discovery**: Automatically finds monitoring targets\n- **Alertmanager**: Manages alert routing and notification\n- **PromQL**: Query language for time-series analysis\n\n## Key Features\n- Pull-based metric collection (configurable scrape intervals)\n- Multi-dimensional data model with labels\n- Powerful query capabilities for aggregation and filtering\n- Built-in alerting with notification integration\n- Time-series data compression and retention policies",
      "diagram": "graph TD\n    Apps[Applications] --> Exporters[Exporters]\n    Exporters --> Metrics[Metrics Endpoints]\n    Prometheus[Prometheus Server] --> Metrics\n    Prometheus --> TSDB[(Time Series DB)]\n    Prometheus --> PromQL[PromQL Queries]\n    Prometheus --> AlertManager[Alertmanager]\n    AlertManager --> Slack[Slack/Email/PagerDuty]\n    Grafana[Grafana] --> Prometheus\n    ServiceDiscovery[Service Discovery] --> Prometheus",
      "difficulty": "beginner",
      "tags": [
        "observability",
        "monitoring",
        "logging"
      ],
      "lastUpdated": "2025-12-12T10:03:55.884Z"
    },
    "gh-22": {
      "id": "gh-22",
      "question": "What is Grafana and how does it integrate with different data sources for monitoring and visualization?",
      "answer": "Grafana is an open-source analytics and monitoring platform that queries, visualizes, and alerts on metrics from multiple data sources.",
      "explanation": "Grafana is a comprehensive open-source analytics and monitoring solution that provides powerful visualization and alerting capabilities for metrics and logs from various data sources.\n\n**Key Features:**\n• **Multi-source integration** - Connects to 60+ data sources including Prometheus, InfluxDB, Elasticsearch, MySQL, and cloud services\n• **Rich visualizations** - Offers graphs, heatmaps, histograms, geomaps, and custom panels for data representation\n• **Interactive dashboards** - Create dynamic, shareable dashboards with drill-down capabilities and variables\n• **Alerting system** - Set up notifications via email, Slack, PagerDuty when metrics exceed thresholds\n• **User management** - Role-based access control, teams, and organization management\n• **Templating** - Dynamic dashboards using variables for different environments or services\n• **Plugins ecosystem** - Extend functionality with community and enterprise plugins\n\n**Common Use Cases:**\n• Infrastructure monitoring and observability\n• Application performance monitoring (APM)\n• Business intelligence and analytics\n• IoT data visualization\n• Log analysis and correlation",
      "diagram": "graph TD\n    A[Data Sources] --> B[Grafana Server]\n    C[Prometheus] --> B\n    D[InfluxDB] --> B\n    E[Elasticsearch] --> B\n    F[MySQL] --> B\n    B --> G[Query Engine]\n    G --> H[Visualization Engine]\n    H --> I[Dashboards]\n    H --> J[Panels]\n    B --> K[Alert Manager]\n    K --> L[Notifications]\n    M[Users] --> N[Web Interface]\n    N --> I\n    I --> O[Graphs]\n    I --> P[Tables]\n    I --> Q[Heatmaps]",
      "difficulty": "beginner",
      "tags": [
        "observability",
        "monitoring",
        "logging"
      ],
      "lastUpdated": "2025-12-12T10:04:15.574Z"
    },
    "gh-23": {
      "id": "gh-23",
      "question": "Explain the key differences between monitoring and logging in DevOps, and when would you use each?",
      "answer": "Monitoring tracks system health and performance metrics in real-time, while logging records discrete events for troubleshooting and analysis.",
      "explanation": "## Key Differences\n\n### **Monitoring**\n- **Purpose**: Real-time system health tracking\n- **Data Type**: Metrics, performance indicators\n- **Usage**: Alerting, trend analysis, SLA compliance\n- **Examples**: CPU usage, response times, error rates\n- **Tools**: Prometheus, Grafana, Datadog\n\n### **Logging**\n- **Purpose**: Event recording and debugging\n- **Data Type**: Discrete events, messages\n- **Usage**: Root cause analysis, security auditing\n- **Examples**: Application errors, user actions, system events\n- **Tools**: ELK Stack, Splunk, Graylog\n\n### **When to Use Each**\n- **Monitoring**: System health dashboards, proactive alerts\n- **Logging**: Debugging specific issues, audit trails",
      "diagram": "graph TD\n    subgraph \"Monitoring\"\n        M[Metrics Collection] --> A[Real-time Alerts]\n        M --> D[Performance Dashboards]\n        A --> T[Threshold Alerts]\n    end\n    \n    subgraph \"Logging\"\n        L[Event Recording] --> S[Log Aggregation]\n        S --> R[Root Cause Analysis]\n        S --> Audit[Audit Trail]\n    end\n    \n    I[Infrastructure] --> M\n    I --> L\n    A --> C[Corrective Action]\n    R --> C",
      "difficulty": "intermediate",
      "tags": [
        "observability",
        "monitoring",
        "logging"
      ],
      "lastUpdated": "2025-12-12T10:04:23.386Z"
    },
    "gh-24": {
      "id": "gh-24",
      "question": "What is DevSecOps and how does it differ from traditional DevOps security approaches?",
      "answer": "DevSecOps integrates security throughout the entire software development lifecycle by making security a shared responsibility embedded in CI/CD pipelines, rather than a separate final-stage gate.",
      "explanation": "DevSecOps represents a cultural and technical shift where security becomes an integral part of development operations rather than an afterthought. Unlike traditional approaches where security teams review code just before deployment, DevSecOps implements automated security checks throughout the development process.\n\n**Key differences from traditional DevOps security:**\n- **Shift-left approach**: Security testing begins early in development, not just before production\n- **Automated security integration**: Security tools are embedded directly in CI/CD pipelines\n- **Shared responsibility**: Developers own security alongside their feature development\n- **Continuous monitoring**: Ongoing security assessment rather than periodic audits\n\n**Core DevSecOps principles:**\n- Security as code: Treating security policies and configurations as version-controlled artifacts\n- Infrastructure as code security: Automated validation of cloud resource configurations\n- Static and dynamic analysis: Code scanning integrated into build processes\n- Secret management: Automated detection and secure handling of credentials\n- Compliance as code: Automated verification against security standards and regulations\n- Continuous security testing: Regular penetration testing and vulnerability assessments",
      "diagram": "graph TD\n    A[Developer] --> B[Code Commit]\n    B --> C[Automated Security Scan]\n    C --> D[Static Analysis]\n    D --> E[Dependency Check]\n    E --> F[Build & Test]\n    F --> G[Dynamic Analysis]\n    G --> H[Deploy to Staging]\n    H --> I[Security Validation]\n    I --> J[Deploy to Production]\n    J --> K[Continuous Monitoring]\n    K --> L[Vulnerability Detection]\n    L --> M[Automated Remediation]\n    M --> A\n    \n    style C fill:#ffeb3b\n    style D fill:#ffeb3b\n    style E fill:#ffeb3b\n    style G fill:#ffeb3b\n    style I fill:#ffeb3b\n    style K fill:#ffeb3b\n    style L fill:#ffeb3b",
      "difficulty": "advanced",
      "tags": [
        "security",
        "devsecops"
      ],
      "lastUpdated": "2025-12-12T10:04:46.787Z"
    },
    "gh-25": {
      "id": "gh-25",
      "question": "What is Infrastructure Security?",
      "answer": "Infrastructure Security involves securing all infrastructure components including:",
      "explanation": "Infrastructure Security involves securing all infrastructure components including:\n\n1. **Network Security:**\n- Firewalls\n- VPNs\n- Network segmentation\n- DDoS protection\n\n2. **Cloud Security:**\n- Identity and Access Management (IAM)\n- Encryption\n- Security groups\n- Network ACLs\n\n3. **Host Security:**\n- OS hardening\n- Patch management\n- Antivirus\n- Host-based firewalls",
      "diagram": "\ngraph TD\n    Infra[Infrastructure] --> Net[Network Security]\n    Infra --> Cloud[Cloud Security]\n    Infra --> Host[Host Security]\n",
      "difficulty": "advanced",
      "tags": [
        "security",
        "devsecops"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-26": {
      "id": "gh-26",
      "question": "What are the basic Linux commands every DevOps engineer should know?",
      "answer": "Essential Linux commands include:",
      "explanation": "Essential Linux commands include:\n\n1. **File Operations:**\n```bash\nls      # List files and directories\ncd      # Change directory\npwd     # Print working directory\ncp      # Copy files\nmv      # Move/rename files\nrm      # Remove files\nmkdir   # Create directory\n```\n\n2. **System Information:**\n```bash\ntop     # Show processes\ndf      # Show disk usage\nfree    # Show memory usage\nps      # Show process status\n```\n\n3. **Text Processing:**\n```bash\ngrep    # Search text\nsed     # Stream editor\nawk     # Text processing\ncat     # View file contents\n```",
      "diagram": "\ngraph TD\n    Linux --> Files[File Ops: ls, cp, mv]\n    Linux --> Sys[System: top, df, ps]\n    Linux --> Text[Text: grep, awk, sed]\n",
      "difficulty": "beginner",
      "tags": [
        "linux",
        "shell"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-27": {
      "id": "gh-27",
      "question": "What is Git and how does it facilitate collaborative software development through its distributed architecture?",
      "answer": "Git is a distributed version control system that tracks changes in source code, enabling multiple developers to work simultaneously on projects while maintaining a complete history of all modification",
      "explanation": "Git is a distributed version control system that revolutionized collaborative software development by providing a decentralized approach to code management.\n\n**Key Concepts:**\n- **Repository**: Complete project history with all files and metadata\n- **Commit**: Snapshot of changes with unique hash identifier\n- **Branch**: Independent line of development for features or fixes\n- **Merge**: Integration of changes from different branches\n- **Pull Request**: Mechanism for code review and integration\n- **Clone**: Local copy of remote repository\n- **Push/Pull**: Synchronize changes between local and remote repositories\n\n**Distributed Architecture Benefits:**\n- Every developer has full repository copy\n- Offline work capability\n- Faster operations (no network dependency)\n- Better backup and redundancy\n- Flexible collaboration workflows",
      "diagram": "graph TD\n    A[Working Directory] -->|git add| B[Staging Area]\n    B -->|git commit| C[Local Repository]\n    C -->|git push| D[Remote Repository]\n    D -->|git pull/fetch| C\n    E[Developer 1] --> A\n    F[Developer 2] --> A\n    G[Feature Branch] -->|git merge| C\n    H[Main Branch] -->|git checkout| A",
      "difficulty": "advanced",
      "tags": [
        "git",
        "vcs"
      ],
      "lastUpdated": "2025-12-12T10:04:58.385Z"
    },
    "gh-28": {
      "id": "gh-28",
      "question": "What is Git Branching Strategy?",
      "answer": "A Git branching strategy is a convention or set of rules that specify how and when branches should be created and merged. Common strategies include:",
      "explanation": "A Git branching strategy is a convention or set of rules that specify how and when branches should be created and merged. Common strategies include:\n\n1. **Git Flow:**\n- Main branches: master, develop\n- Supporting branches: feature, release, hotfix\n\n2. **Trunk-Based Development:**\n- Single main branch (trunk)\n- Short-lived feature branches\n- Frequent integration\n\nExample of creating a feature branch:\n```bash\n# Create and switch to a new feature branch\ngit checkout -b feature/new-feature\n\n# Make changes and commit\ngit add .\ngit commit -m \"Add new feature\"\n\n# Push to remote\ngit push origin feature/new-feature\n```",
      "diagram": "\ngraph LR\n    Main[main] --> Dev[develop]\n    Dev --> Feat[feature]\n    Feat --> Dev\n    Dev --> Main\n",
      "difficulty": "beginner",
      "tags": [
        "git",
        "vcs"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-29": {
      "id": "gh-29",
      "question": "What is Configuration Management?",
      "answer": "Configuration Management is the process of maintaining systems, such as computer systems and servers, in a desired state. It's a way to make sure that...",
      "explanation": "Configuration Management is the process of maintaining systems, such as computer systems and servers, in a desired state. It's a way to make sure that a system performs as it's supposed to as changes are made over time.\n\nKey aspects include:\n- System configuration\n- Application configuration\n- Dependencies management\n- Version control\n- Compliance and security",
      "diagram": "\ngraph LR\n    Config[Config Code] --> Tool[CM Tool]\n    Tool --> S1[Server 1]\n    Tool --> S2[Server 2]\n",
      "difficulty": "beginner",
      "tags": [
        "config-mgmt",
        "ansible",
        "chef"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-30": {
      "id": "gh-30",
      "question": "What is Puppet and how does it manage infrastructure configuration?",
      "answer": "Puppet is a configuration management tool that automates infrastructure provisioning using a declarative language to define desired system states.",
      "explanation": "Puppet is a configuration management tool that helps you automate the provisioning and management of your infrastructure. It uses a declarative language to describe system configurations, where you specify the desired state rather than the steps to achieve it.\n\n## Key Concepts\n\n- **Declarative Language**: Define what the system should look like, not how to configure it\n- **Idempotent**: Running the same configuration multiple times produces the same result\n- **Agent-Server Architecture**: Puppet agents periodically check in with the Puppet server for configuration updates\n- **Resources**: Basic units of configuration (packages, files, services, users, etc.)\n\n## Example Puppet Manifest\n\n```puppet\nclass apache {\n  package { 'apache2':\n    ensure => installed,\n  }\n\n  service { 'apache2':\n    ensure  => running,\n    enable  => true,\n    require => Package['apache2'],\n  }\n\n  file { '/var/www/html/index.html':\n    ensure  => file,\n    content => 'Hello, World!',\n    require => Package['apache2'],\n  }\n}\n```\n\nThis manifest ensures Apache is installed, running, and serving a simple HTML page. The `require` parameter creates dependencies between resources.\n\n## Common Use Cases\n\n- Standardizing server configurations across environments\n- Enforcing security policies and compliance\n- Managing configuration drift\n- Automating software deployments",
      "diagram": "graph TB\n    A[Puppet Server] -->|Catalog| B[Agent: Web Server]\n    A -->|Catalog| C[Agent: DB Server]\n    A -->|Catalog| D[Agent: App Server]\n    B -->|Facts| A\n    C -->|Facts| A\n    D -->|Facts| A\n    E[Puppet Code/Manifests] --> A\n    B --> F[Apply Configuration]\n    C --> G[Apply Configuration]\n    D --> H[Apply Configuration]\n    F --> I[Desired State]\n    G --> J[Desired State]\n    H --> K[Desired State]",
      "difficulty": "beginner",
      "tags": [
        "config-mgmt",
        "ansible",
        "chef"
      ],
      "lastUpdated": "2025-12-13T06:29:18.865Z"
    },
    "gh-35": {
      "id": "gh-35",
      "question": "How do Backup and Disaster Recovery strategies ensure business continuity after system failures?",
      "answer": "BDR combines automated data backups with failover systems to restore operations quickly after disasters.",
      "explanation": "Backup and Disaster Recovery (BDR) is a comprehensive strategy that protects organizations from data loss and system downtime through coordinated backup and recovery processes.\n\n## Key Components\n\n### 1. **Data Backup**\n- **Regular backups**: Automated daily/weekly/monthly schedules\n- **Multiple locations**: On-site, off-site, and cloud storage\n- **Backup types**: Full, incremental, and differential backups\n- **Retention policies**: Short-term and long-term data preservation\n\n### 2. **Disaster Recovery**\n- **Recovery Time Objective (RTO)**: Maximum acceptable downtime\n- **Recovery Point Objective (RPO)**: Maximum data loss tolerance\n- **Failover systems**: Hot, warm, and cold standby sites\n- **Recovery procedures**: Step-by-step restoration processes\n\n## Implementation Strategy\n\n```yaml\n# Example BDR Configuration\nbackup_strategy:\n  schedule: \"daily at 2 AM\"\n  retention: \"30 days daily, 12 months weekly\"\n  locations: [\"local\", \"cloud\", \"offsite\"]\n  \ndisaster_recovery:\n  rto: \"4 hours\"\n  rpo: \"1 hour\"\n  failover_type: \"warm_standby\"\n  testing_frequency: \"quarterly\"\n```\n\n## Best Practices\n- **3-2-1 Rule**: 3 copies, 2 different media, 1 off-site location\n- **Regular testing**: Validate backup integrity and recovery procedures\n- **Documentation**: Maintain detailed runbooks for disaster scenarios\n- **Automation**: Reduce human error in backup and recovery processes",
      "diagram": "graph TD\n    A[Production Systems] --> B[Automated Backups]\n    B --> C[Local Storage]\n    B --> D[Cloud Storage]\n    B --> E[Offsite Storage]\n    \n    F[Disaster Event] --> G[Failover Trigger]\n    G --> H[Recovery Procedures]\n    H --> I[Restore from Backups]\n    I --> J[Systems Operational]\n    \n    K[Regular Testing] --> L[Backup Validation]\n    L --> M[Recovery Drills]\n    M --> N[Process Improvement]\n    \n    style A fill:#e1f5fe\n    style J fill:#c8e6c9\n    style F fill:#ffebee\n    style G fill:#ffcdd2",
      "difficulty": "beginner",
      "tags": [
        "backup",
        "dr"
      ],
      "lastUpdated": "2025-12-13T06:29:40.130Z"
    },
    "gh-36": {
      "id": "gh-36",
      "question": "How do different backup strategies balance storage efficiency, backup speed, and recovery time?",
      "answer": "Full backups copy everything, incremental saves only changes since last backup, differential saves changes since last full backup.",
      "explanation": "Backup strategies balance three key factors: storage space, backup duration, and recovery speed.\n\n## **Full Backup**\n- **What**: Complete copy of all data\n- **Storage**: Highest usage (100% of data size)\n- **Speed**: Slowest backup process\n- **Recovery**: Fastest - single restore operation\n- **Use case**: Weekly/monthly baseline, critical systems\n\n## **Incremental Backup**\n- **What**: Only changes since last backup (any type)\n- **Storage**: Lowest usage (only changed data)\n- **Speed**: Fastest backup process\n- **Recovery**: Slowest - need full + all incremental backups\n- **Use case**: Daily backups, large datasets with limited change\n\n## **Differential Backup**\n- **What**: Changes since last full backup\n- **Storage**: Medium usage (grows until next full)\n- **Speed**: Medium backup process\n- **Recovery**: Medium - need full + latest differential\n- **Use case**: When faster recovery needed than incremental\n\n## **Strategy Examples**\n- **Grandfather-Father-Son**: Monthly full + weekly differential + daily incremental\n- **Tower of Hanoi**: Rotating backup schedule with different retention periods\n- **3-2-1 Rule**: 3 copies, 2 different media, 1 offsite location",
      "diagram": "graph TD\n    subgraph \"Backup Strategy Comparison\"\n        A[Full Backup] --> A1[100% Storage]\n        A --> A2[Slow Backup]\n        A --> A3[Fast Recovery]\n        \n        B[Incremental] --> B1[Minimal Storage]\n        B --> B2[Fast Backup]\n        B --> B3[Slow Recovery]\n        \n        C[Differential] --> C1[Medium Storage]\n        C --> C2[Medium Backup]\n        C --> C3[Medium Recovery]\n    end\n    \n    subgraph \"Recovery Process\"\n        D[Full Recovery] --> E[Single File]\n        F[Incremental Recovery] --> G[Full + All Incrementals]\n        H[Differential Recovery] --> I[Full + Latest Differential]\n    end\n    \n    style A fill:#e1f5fe\n    style B fill:#fff3e0\n    style C fill:#f3e5f5",
      "difficulty": "intermediate",
      "tags": [
        "backup",
        "dr"
      ],
      "lastUpdated": "2025-12-13T06:30:02.658Z"
    },
    "gh-37": {
      "id": "gh-37",
      "question": "What is Cloud Native Architecture?",
      "answer": "Cloud Native Architecture is an approach to designing and building applications that exploits the advantages of the cloud computing delivery model. It...",
      "explanation": "Cloud Native Architecture is an approach to designing and building applications that exploits the advantages of the cloud computing delivery model. It emphasizes:\n\n1. **Characteristics:**\n- Scalability\n- Containerization\n- Automation\n- Orchestration\n- Microservices\n\n2. **Key Principles:**\n- Design for automation\n- Build for resilience\n- Enable scalability\n- Embrace containerization\n- Practice continuous delivery",
      "diagram": "\ngraph TD\n    Cloud[Cloud Native] --> Containers\n    Cloud --> Microservices\n    Cloud --> K8s[Orchestration]\n    Cloud --> CI[CI/CD]\n",
      "difficulty": "beginner",
      "tags": [
        "cloud-native",
        "microservices"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-38": {
      "id": "gh-38",
      "question": "What are Microservices?",
      "answer": "Microservices is an architectural style that structures an application as a collection of small autonomous services, modeled around a business domain.",
      "explanation": "Microservices is an architectural style that structures an application as a collection of small autonomous services, modeled around a business domain.\n\nKey characteristics:\n1. **Independence:**\n- Separate codebases\n- Independent deployment\n- Different technology stacks\n\n2. **Communication:**\n- API-based interaction\n- Event-driven\n- Service discovery\n\nExample of a microservice API:\n```yaml\nopenapi: 3.0.0\ninfo:\ntitle: User Service API\nversion: 1.0.0\npaths:\n/users:\nget:\nsummary: List users\nresponses:\n'200':\ndescription: List of users\npost:\nsummary: Create user\nresponses:\n'201':\ndescription: User created\n```",
      "diagram": "\ngraph LR\n    API[API Gateway] --> User[User Service]\n    API --> Order[Order Service]\n    API --> Pay[Payment Service]\n",
      "difficulty": "intermediate",
      "tags": [
        "cloud-native",
        "microservices"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-39": {
      "id": "gh-39",
      "question": "What is Service Mesh?",
      "answer": "A service mesh is a dedicated infrastructure layer for handling service-to-service communication in microservices architectures.",
      "explanation": "A service mesh is a dedicated infrastructure layer for handling service-to-service communication in microservices architectures.\n\nKey components:\n1. **Data Plane:**\n- Service proxies (sidecars)\n- Traffic handling\n- Security enforcement\n\n2. **Control Plane:**\n- Configuration management\n- Policy enforcement\n- Service discovery\n\nExample of Istio configuration:\n```yaml\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\nname: reviews-route\nspec:\nhosts:\n- reviews\nhttp:\n- route:\n- destination:\nhost: reviews\nsubset: v1\nweight: 75\n- destination:\nhost: reviews\nsubset: v2\nweight: 25\n```",
      "diagram": "\ngraph TD\n    CP[Control Plane] --> DP[Data Plane]\n    DP --> S1[Sidecar] --> Svc1[Service 1]\n    DP --> S2[Sidecar] --> Svc2[Service 2]\n",
      "difficulty": "beginner",
      "tags": [
        "cloud-native",
        "microservices"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-40": {
      "id": "gh-40",
      "question": "What is Performance Testing and how does it differ from Load and Stress Testing?",
      "answer": "Performance testing evaluates system responsiveness, stability, and scalability under various workloads to identify bottlenecks and validate requirements.",
      "explanation": "Performance Testing is a comprehensive testing approach that evaluates how a system performs under different conditions. It encompasses several testing types:\n\n**Key Performance Testing Types:**\n1. **Load Testing:** Tests system performance under expected user loads\n2. **Stress Testing:** Pushes system beyond normal capacity to find breaking points\n3. **Endurance Testing:** Validates performance over extended periods\n4. **Spike Testing:** Tests response to sudden traffic increases\n\n**Essential Performance Metrics:**\n- **Response Time:** Time taken to process requests\n- **Throughput:** Number of transactions per time unit\n- **Resource Utilization:** CPU, memory, disk, network usage\n- **Concurrency:** Number of simultaneous users handled\n- **Error Rate:** Percentage of failed requests\n\n**Common Tools:**\n- Apache JMeter, Gatling, k6 for load generation\n- New Relic, Datadog for monitoring\n- Grafana for visualization\n\n**Real-world Example:**\nAn e-commerce site performs load testing before Black Friday to ensure it can handle 10,000 concurrent users with <2 second response times.",
      "diagram": "graph TD\n    A[Performance Testing] --> B[Load Testing]\n    A --> C[Stress Testing]\n    A --> D[Endurance Testing]\n    A --> E[Spike Testing]\n    \n    B --> F[Expected Load]\n    C --> G[Beyond Capacity]\n    D --> H[Extended Duration]\n    E --> I[Sudden Traffic Spikes]\n    \n    F --> J[Response Time < 2s]\n    G --> K[Find Breaking Point]\n    H --> L[Memory Leaks]\n    I --> M[Auto-scaling]\n    \n    style A fill:#e1f5fe\n    style B fill:#f3e5f5\n    style C fill:#ffebee\n    style D fill:#e8f5e8\n    style E fill:#fff3e0",
      "difficulty": "beginner",
      "tags": [
        "perf",
        "testing"
      ],
      "lastUpdated": "2025-12-13T06:30:09.394Z"
    },
    "gh-41": {
      "id": "gh-41",
      "question": "What are the different types of performance testing and when would you use each?",
      "answer": "Load, stress, spike, volume, endurance, and scalability testing - each validates different performance aspects under varying conditions.",
      "explanation": "Performance testing encompasses several specialized types:\n\n• **Load Testing** - Validates system behavior under expected user load to ensure it meets performance requirements\n• **Stress Testing** - Pushes system beyond normal capacity to identify breaking points and failure modes\n• **Spike Testing** - Tests sudden load increases to verify system handles traffic spikes gracefully\n• **Volume Testing** - Validates performance with large amounts of data to identify database bottlenecks\n• **Endurance Testing** - Runs extended tests to detect memory leaks and resource degradation over time\n• **Scalability Testing** - Determines system's ability to scale up/down with varying loads\n\nExample JMeter configuration:\n```xml\n<ThreadGroup>\n  <stringProp name=\"ThreadGroup.num_threads\">100</stringProp>\n  <stringProp name=\"ThreadGroup.ramp_time\">60</stringProp>\n  <stringProp name=\"ThreadGroup.duration\">300</stringProp>\n</ThreadGroup>\n```",
      "diagram": "graph TD\n    A[Performance Testing] --> B[Load Testing]\n    A --> C[Stress Testing]\n    A --> D[Spike Testing]\n    A --> E[Volume Testing]\n    A --> F[Endurance Testing]\n    A --> G[Scalability Testing]\n    B --> H[Expected Load]\n    C --> I[Peak Load]\n    D --> J[Sudden Spikes]\n    E --> K[Large Data Sets]\n    F --> L[Extended Duration]\n    G --> M[Variable Capacity]",
      "difficulty": "intermediate",
      "tags": [
        "perf",
        "testing"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-42": {
      "id": "gh-42",
      "question": "What is an API Gateway?",
      "answer": "An API Gateway acts as a reverse proxy to accept all API calls, aggregate various services, and return the appropriate result.",
      "explanation": "An API Gateway acts as a reverse proxy to accept all API calls, aggregate various services, and return the appropriate result.\n\nKey features:\n1. **Request Handling:**\n- Authentication\n- SSL termination\n- Rate limiting\n\n2. **Integration:**\n- Service discovery\n- Request routing\n- Response transformation\n\nExample of Kong API Gateway configuration:\n```yaml\nservices:\n- name: user-service\nurl: http://user-service:8000\nroutes:\n- name: user-route\npaths:\n- /users\nplugins:\n- name: rate-limiting\nconfig:\nminute: 5\npolicy: local\n```",
      "diagram": "\ngraph LR\n    Client --> GW[API Gateway]\n    GW --> Auth[Auth Service]\n    GW --> User[User Service]\n    GW --> Data[Data Service]\n",
      "difficulty": "beginner",
      "tags": [
        "api",
        "service-mesh"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-43": {
      "id": "gh-43",
      "question": "What are the key benefits of implementing an API Gateway in a microservices architecture, and how does it address common distributed system challenges?",
      "answer": "API Gateway provides centralized security, traffic management, monitoring, and service abstraction for microservices.",
      "explanation": "Key benefits include:\n\n**Security & Access Control:**\n- Centralized authentication and authorization\n- SSL/TLS termination and certificate management\n- API key management and validation\n- Protection against common attacks (DDoS, injection)\n\n**Traffic Management:**\n- Load balancing across service instances\n- Rate limiting and throttling\n- Request/response transformation and validation\n- Circuit breaker patterns for fault tolerance\n\n**Monitoring & Analytics:**\n- Centralized logging and metrics collection\n- Real-time API usage analytics\n- Performance monitoring and alerting\n- Request tracing across services\n\n**Developer Experience:**\n- Single entry point for all APIs\n- API versioning and backward compatibility\n- Documentation and testing interfaces\n- Simplified client integration",
      "diagram": "graph TD\n    A[Client Applications] --> B[API Gateway]\n    B --> C[Authentication Service]\n    B --> D[User Service]\n    B --> E[Order Service]\n    B --> F[Payment Service]\n    B --> G[Notification Service]\n    H[Load Balancer] --> B\n    B --> I[Rate Limiter]\n    B --> J[Cache Layer]\n    B --> K[Monitoring & Logs]\n    L[Admin Dashboard] --> K",
      "difficulty": "intermediate",
      "tags": [
        "api",
        "service-mesh"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-44": {
      "id": "gh-44",
      "question": "How do you implement a comprehensive API security strategy that protects against common vulnerabilities while maintaining developer productivity?",
      "answer": "API security combines authentication, authorization, encryption, and monitoring to protect endpoints from threats while enabling secure access.",
      "explanation": "API security requires a multi-layered approach to protect against various attack vectors:\n\n## Core Security Components\n\n### 1. **Authentication & Authorization**\n- **OAuth 2.0 + OpenID Connect**: Industry standard for delegated access\n- **JWT Tokens**: Stateless authentication with claims-based authorization\n- **API Keys**: Simple authentication for service-to-service communication\n- **mTLS**: Mutual TLS for zero-trust service communication\n\n### 2. **Input Validation & Sanitization**\n- **Schema Validation**: Validate request bodies against OpenAPI/Swagger schemas\n- **SQL Injection Prevention**: Use parameterized queries and ORMs\n- **XSS Protection**: Sanitize user input and encode output\n- **Rate Limiting**: Prevent abuse and DoS attacks\n\n### 3. **Transport Security**\n- **HTTPS Only**: Enforce TLS 1.2+ for all API communications\n- **Certificate Pinning**: Prevent man-in-the-middle attacks\n- **HSTS Headers**: Force secure connections\n\n### 4. **API Gateway Security**\n```yaml\n# Kong API Gateway Security Configuration\nservices:\n- name: user-api\n  url: http://user-service:8000\n  plugins:\n  - name: oauth2\n    config:\n      scopes: [\"read\", \"write\"]\n  - name: rate-limiting\n    config:\n      minute: 100\n      policy: local\n  - name: request-size-limiting\n    config:\n      allowed_payload_size: 10\n```\n\n### 5. **Monitoring & Threat Detection**\n- **API Logging**: Comprehensive audit trails\n- **Anomaly Detection**: ML-based threat identification\n- **Security Headers**: CORS, CSP, X-Frame-Options\n- **Vulnerability Scanning**: Regular security assessments\n\n## Best Practices\n- **Principle of Least Privilege**: Grant minimal necessary permissions\n- **Defense in Depth**: Multiple security layers\n- **Regular Security Reviews**: Penetration testing and code audits\n- **Security by Design**: Build security into API design from the start",
      "diagram": "graph TD\n    Client[Client Application] --> GW[API Gateway]\n    GW --> Auth[Authentication Service]\n    GW --> Rate[Rate Limiter]\n    GW --> Valid[Input Validator]\n    GW --> API[Backend API]\n    \n    Auth --> OAuth[OAuth 2.0/JWT]\n    Auth --> mTLS[mutual TLS]\n    \n    API --> DB[(Database)]\n    API --> Cache[(Cache)]\n    \n    GW --> Monitor[Security Monitoring]\n    Monitor --> Alert[Threat Detection]\n    Monitor --> Log[Audit Logging]\n    \n    style GW fill:#e1f5fe\n    style Auth fill:#f3e5f5\n    style Monitor fill:#fff3e0\n    style Alert fill:#ffebee",
      "difficulty": "beginner",
      "tags": [
        "api",
        "service-mesh"
      ],
      "lastUpdated": "2025-12-13T06:30:21.829Z"
    },
    "gh-45": {
      "id": "gh-45",
      "question": "How do rate limiting algorithms like Token Bucket and Leaky Bucket control API request flow?",
      "answer": "Rate limiting controls request processing speed using algorithms like Token Bucket (burst allowed) and Leaky Bucket (smooth flow).",
      "explanation": "Rate limiting prevents system overload by controlling request processing rates using different algorithms:\n\n**Token Bucket Algorithm:**\n- Bucket holds fixed number of tokens\n- Tokens replenish at constant rate\n- Each request consumes one token\n- Allows burst traffic when tokens available\n\n**Leaky Bucket Algorithm:**\n- Requests enter bucket at variable rate\n- Processed at fixed rate (leak)\n- Smooths out traffic spikes\n- Drops requests when bucket full\n\n**Implementation Example (Nginx):**\n```nginx\nhttp {\n  limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;\n  \n  server {\n    location /api/ {\n      limit_req zone=api burst=20 nodelay;\n    }\n  }\n}\n```\n\n**Use Cases:**\n- API throttling\n- DDoS protection\n- Resource management\n- Fair usage enforcement",
      "diagram": "graph TD\n    A[Incoming Requests] --> B{Rate Limiter}\n    B -->|Within Limit| C[Process Request]\n    B -->|Exceeds Limit| D[Reject/Queue Request]\n    C --> E[Response]\n    F[Token Bucket] --> B\n    G[Leaky Bucket] --> B\n    H[Configuration Rules] --> B",
      "difficulty": "beginner",
      "tags": [
        "api",
        "service-mesh"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-46": {
      "id": "gh-46",
      "question": "What is API Documentation?",
      "answer": "API Documentation is a set of documents that describe how to use an API. It includes:",
      "explanation": "API Documentation is a set of documents that describe how to use an API. It includes:\n\n1. **API Reference:**\n- Detailed description of each API endpoint\n- Request and response formats\n- Example requests and responses\n\n2. **API Usage Examples:**\n- Code samples\n- API client libraries\n- API testing tools\n\nExample of Swagger API Documentation:\n```yaml\nswagger: '2.0'\ninfo:\ntitle: User Service API\nversion: 1.0.0\npaths:\n/users:\nget:\nsummary: List users\nresponses:\n'200':\ndescription: List of users\npost:\nsummary: Create user\nresponses:\n'201':\ndescription: User created\n```",
      "difficulty": "beginner",
      "tags": [
        "api",
        "service-mesh"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-47": {
      "id": "gh-47",
      "question": "What are StatefulSets in Kubernetes?",
      "answer": "StatefulSets are used to manage stateful applications, providing guarantees about the ordering and uniqueness of Pods.",
      "explanation": "StatefulSets are used to manage stateful applications, providing guarantees about the ordering and uniqueness of Pods.\n\nKey features:\n1. **Stable Network Identity:**\n- Predictable Pod names\n- Stable hostnames\n\n2. **Ordered Deployment:**\n- Sequential creation\n- Sequential scaling\n- Sequential deletion\n\nExample of StatefulSet:\n```yaml\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\nname: web\nspec:\nserviceName: \"nginx\"\nreplicas: 3\nselector:\nmatchLabels:\napp: nginx\ntemplate:\nmetadata:\nlabels:\napp: nginx\nspec:\ncontainers:\n- name: nginx\nimage: nginx:1.14.2\nports:\n- containerPort: 80\nvolumeMounts:\n- name: www\nmountPath: /usr/share/nginx/html\nvolumeClaimTemplates:\n- metadata:\nname: www\nspec:\naccessModes: [ \"ReadWriteOnce\" ]\nresources:\nrequests:\nstorage: 1Gi\n```",
      "difficulty": "advanced",
      "tags": [
        "k8s",
        "advanced"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-48": {
      "id": "gh-48",
      "question": "What are DaemonSets in Kubernetes?",
      "answer": "DaemonSets ensure that all (or some) nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them.",
      "explanation": "DaemonSets ensure that all (or some) nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them.\n\nUse cases:\n1. **Monitoring Agents**\n2. **Log Collectors**\n3. **Node-level Storage**\n4. **Network Plugins**\n\nExample of DaemonSet:\n```yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\nname: fluentd-elasticsearch\nspec:\nselector:\nmatchLabels:\nname: fluentd-elasticsearch\ntemplate:\nmetadata:\nlabels:\nname: fluentd-elasticsearch\nspec:\ncontainers:\n- name: fluentd-elasticsearch\nimage: quay.io/fluentd_elasticsearch/fluentd:v2.5.2\n```",
      "difficulty": "advanced",
      "tags": [
        "k8s",
        "advanced"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-49": {
      "id": "gh-49",
      "question": "What is Helm?",
      "answer": "Helm is a package manager for Kubernetes that helps you manage Kubernetes applications through Helm Charts.",
      "explanation": "Helm is a package manager for Kubernetes that helps you manage Kubernetes applications through Helm Charts.\n\nKey concepts:\n1. **Charts:**\n- Package format\n- Collection of files\n- Template mechanism\n\n2. **Repositories:**\n- Chart storage\n- Version control\n- Distribution\n\nExample of Helm Chart:\n```yaml\napiVersion: v2\nname: my-app\ndescription: A Helm chart for my application\nversion: 0.1.0\ndependencies:\n- name: mysql\nversion: 8.8.3\nrepository: https://charts.bitnami.com/bitnami\n```",
      "diagram": "\ngraph LR\n    Chart[Helm Chart] --> Helm[Helm CLI]\n    Helm --> K8s[Kubernetes]\n    K8s --> App[Application]\n",
      "difficulty": "advanced",
      "tags": [
        "k8s",
        "advanced"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-50": {
      "id": "gh-50",
      "question": "What is Istio?",
      "answer": "Istio is an open-source service mesh that provides a way to control how services communicate with one another. It includes:",
      "explanation": "Istio is an open-source service mesh that provides a way to control how services communicate with one another. It includes:\n\n1. **Traffic Management:**\n- Load balancing\n- Traffic routing\n- Fault injection\n- Traffic mirroring\n\n2. **Security:**\n- Authentication\n- Authorization\n- Encryption\n- Mutual TLS\n\n3. **Observability:**\n- Telemetry\n- Metrics\n- Tracing\n- Logging",
      "difficulty": "advanced",
      "tags": [
        "k8s",
        "advanced"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-51": {
      "id": "gh-51",
      "question": "What is Container Runtime Interface (CRI)?",
      "answer": "Container Runtime Interface (CRI) is an API that allows container runtimes to interact with the container orchestrator. It includes:",
      "explanation": "Container Runtime Interface (CRI) is an API that allows container runtimes to interact with the container orchestrator. It includes:\n\n1. **Image Management:**\n- Pulling images\n- Pushing images\n- Listing images\n- Deleting images\n\n2. **Container Management:**\n- Creating containers\n- Starting containers\n- Stopping containers\n- Killing containers\n- Inspecting containers\n\n3. **Container Runtime:**\n- Running containers\n- Pausing containers\n- Resuming containers\n- Executing commands in containers",
      "difficulty": "advanced",
      "tags": [
        "k8s",
        "advanced"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-52": {
      "id": "gh-52",
      "question": "What is Infrastructure Automation?",
      "answer": "Infrastructure Automation is the process of scripting environments - from installing an operating system, to installing and configuring servers on ins...",
      "explanation": "Infrastructure Automation is the process of scripting environments - from installing an operating system, to installing and configuring servers on instances, to configuring how the instances and software communicate with one another.\n\nKey components:\n1. **Provisioning:**\n- Resource creation\n- Configuration management\n- Application deployment\n\n2. **Orchestration:**\n- Workflow automation\n- Service coordination\n- Resource scheduling",
      "difficulty": "beginner",
      "tags": [
        "automation",
        "tools"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-53": {
      "id": "gh-53",
      "question": "What is GitOps?",
      "answer": "GitOps is a way of implementing Continuous Deployment for cloud native applications. It focuses on a developer-centric experience when operating infra...",
      "explanation": "GitOps is a way of implementing Continuous Deployment for cloud native applications. It focuses on a developer-centric experience when operating infrastructure, by using tools developers are already familiar with, including Git and Continuous Deployment tools.\n\nPrinciples:\n1. **Declarative:**\n- Infrastructure as code\n- Application configuration as code\n\n2. **Version Controlled:**\n- Git as single source of truth\n- Audit trail for changes\n\n3. **Automated:**\n- Pull-based deployment\n- Continuous reconciliation",
      "diagram": "\ngraph LR\n    Git[Git Repo] --> Operator[GitOps Operator]\n    Operator --> K8s[Kubernetes]\n    K8s --> App[Application]\n",
      "difficulty": "beginner",
      "tags": [
        "automation",
        "tools"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-54": {
      "id": "gh-54",
      "question": "What is ArgoCD?",
      "answer": "ArgoCD is a declarative, GitOps continuous delivery tool for Kubernetes. It allows you to declaratively manage your Kubernetes applications by using G...",
      "explanation": "ArgoCD is a declarative, GitOps continuous delivery tool for Kubernetes. It allows you to declaratively manage your Kubernetes applications by using Git repositories as the source of truth.\n\nKey features:\n1. **Declarative:**\n- Infrastructure as code\n- Application configuration as code\n\n2. **Version Controlled:**\n- Git as single source of truth\n- Audit trail for changes\n\n3. **Automated:**\n- Pull-based deployment\n- Continuous reconciliation",
      "diagram": "\ngraph LR\n    Git[Git] --> Argo[ArgoCD]\n    Argo --> Sync[Sync]\n    Sync --> K8s[Kubernetes]\n",
      "difficulty": "beginner",
      "tags": [
        "automation",
        "tools"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-55": {
      "id": "gh-55",
      "question": "How does Tekton provide a cloud-native framework for building CI/CD pipelines on Kubernetes?",
      "answer": "Tekton is a Kubernetes-native CI/CD framework that uses custom resources to define pipeline components as container-based tasks.",
      "explanation": "Tekton is a cloud-native, open-source CI/CD framework built specifically for Kubernetes. It provides a flexible, container-based approach to building pipelines through Kubernetes Custom Resources.\n\n**Key Components:**\n- **Tasks**: Individual steps that execute in containers\n- **Pipelines**: Sequences of tasks that form complete workflows\n- **TaskRuns**: Executed instances of tasks\n- **PipelineRuns**: Executed instances of pipelines\n\n**Core Benefits:**\n- **Container-native**: Each step runs in its own container\n- **Kubernetes integration**: Leverages K8s scheduling and scaling\n- **Declarative**: Pipeline definitions as YAML manifests\n- **Portable**: Works across any Kubernetes cluster\n- **Extensible**: Custom tasks and integrations via community",
      "diagram": "graph TD\n    A[Pipeline YAML] --> B[Tekton Controller]\n    B --> C[Task 1 Container]\n    B --> D[Task 2 Container]\n    B --> E[Task 3 Container]\n    C --> F[Results/Artifacts]\n    D --> F\n    E --> F\n    G[Kubernetes API] --> B\n    B --> G",
      "difficulty": "beginner",
      "tags": [
        "automation",
        "tools"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-56": {
      "id": "gh-56",
      "question": "What are Deployment Strategies?",
      "answer": "Deployment Strategies are methods used to deploy applications to Kubernetes clusters. Common strategies include:",
      "explanation": "Deployment Strategies are methods used to deploy applications to Kubernetes clusters. Common strategies include:\n\n1. **Blue-Green Deployment:**\n- Deploy a new version of the application\n- Traffic is routed to the new version\n- Old version is kept running\n\n2. **Canary Deployment:**\n- Deploy a new version of the application\n- Traffic is routed to the new version\n- Old version is kept running\n\n3. **Rolling Update:**\n- Deploy a new version of the application\n- Old version is gradually replaced\n- Traffic is routed to the new version\n\n4. **Blue-Green with Rolling Update:**\n- Deploy a new version of the application\n- Traffic is routed to the new version\n- Old version is gradually replaced",
      "difficulty": "intermediate",
      "tags": [
        "automation",
        "tools"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-57": {
      "id": "gh-57",
      "question": "What is Cloud Cost Optimization and what are the key strategies to reduce cloud spending?",
      "answer": "Cloud Cost Optimization reduces overall cloud spend by identifying waste, right-sizing resources, and leveraging pricing models effectively.",
      "explanation": "Cloud Cost Optimization is the process of reducing your overall cloud spend by identifying mismanaged resources, eliminating waste, reserving capacity for higher discounts, and right-sizing computing services to scale.\n\n## Key Strategies:\n\n### 1. Resource Optimization\n- **Right-sizing instances** - Match compute resources to actual workload requirements\n- **Shutting down unused resources** - Eliminate idle instances, storage, and services\n- **Using auto-scaling effectively** - Scale resources based on demand patterns\n- **Resource scheduling** - Stop non-production environments during off-hours\n\n### 2. Pricing Optimization\n- **Reserved Instances** - Commit to long-term usage for significant discounts (up to 75%)\n- **Spot Instances** - Use spare capacity at reduced rates for fault-tolerant workloads\n- **Savings Plans** - Flexible pricing model with commitment-based discounts\n- **Volume discounts** - Negotiate better rates for high usage\n\n### 3. Monitoring & Governance\n- **Cost monitoring dashboards** - Track spending patterns and trends\n- **Budget alerts** - Set spending thresholds and notifications\n- **Tagging strategies** - Organize resources for better cost allocation\n- **Regular cost reviews** - Periodic analysis and optimization cycles",
      "diagram": "graph TD\n    A[Cloud Cost Optimization] --> B[Resource Optimization]\n    A --> C[Pricing Optimization]\n    A --> D[Monitoring & Governance]\n    \n    B --> E[Right-sizing]\n    B --> F[Eliminate Waste]\n    B --> G[Auto-scaling]\n    \n    C --> H[Reserved Instances]\n    C --> I[Spot Instances]\n    C --> J[Savings Plans]\n    \n    D --> K[Cost Monitoring]\n    D --> L[Budget Alerts]\n    D --> M[Regular Reviews]",
      "difficulty": "beginner",
      "tags": [
        "finops",
        "cost"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-58": {
      "id": "gh-58",
      "question": "What are AWS Reserved Instances and how do they compare to On-Demand pricing?",
      "answer": "Reserved Instances provide up to 75% discount vs On-Demand pricing in exchange for 1-3 year commitment to specific instance configuration.",
      "explanation": "Reserved Instances (RIs) provide significant cost savings compared to On-Demand pricing in exchange for a commitment to use a specific instance configuration for a one or three-year term.\n\n## Types of Reserved Instances:\n\n**Standard RIs:**\n- Highest discount (up to 75%)\n- Least flexibility - cannot change instance attributes\n- Best for steady-state workloads with predictable usage\n- Can be sold in RI Marketplace\n\n**Convertible RIs:**\n- Lower discount (up to 54%)\n- More flexibility - can exchange for different instance families, OS, tenancy\n- Good for workloads that may change over time\n- Cannot be sold in RI Marketplace\n\n**Scheduled RIs:**\n- For predictable recurring schedules (daily, weekly, monthly)\n- Match capacity reservation to specific usage patterns\n- Available in limited regions and instance types\n\n## Payment Options:\n- **All Upfront:** Highest discount, pay entire term upfront\n- **Partial Upfront:** Medium discount, pay portion upfront + monthly\n- **No Upfront:** Lowest discount, pay monthly only\n\n## Key Benefits:\n- Significant cost reduction for predictable workloads\n- Capacity reservation in specific AZ\n- Can be shared across accounts in organization",
      "diagram": "graph TD\n    A[AWS EC2 Pricing] --> B[On-Demand]\n    A --> C[Reserved Instances]\n    A --> D[Spot Instances]\n    \n    C --> E[Standard RI<br/>Up to 75% discount]\n    C --> F[Convertible RI<br/>Up to 54% discount]\n    C --> G[Scheduled RI<br/>Recurring patterns]\n    \n    E --> H[All Upfront]\n    E --> I[Partial Upfront]\n    E --> J[No Upfront]\n    \n    F --> K[Can Exchange<br/>Instance Types]\n    G --> L[Time-based<br/>Reservations]",
      "difficulty": "intermediate",
      "tags": [
        "finops",
        "cost"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-64": {
      "id": "gh-64",
      "question": "What are DevOps Metrics?",
      "answer": "DevOps metrics are measurements used to evaluate the performance and efficiency of DevOps practices and processes.",
      "explanation": "DevOps metrics are measurements used to evaluate the performance and efficiency of DevOps practices and processes.\n\nKey categories:\n1. **Velocity Metrics:**\n- Deployment frequency\n- Lead time for changes\n- Time to market\n\n2. **Quality Metrics:**\n- Change failure rate\n- Bug detection rate\n- Test coverage\n\n3. **Operational Metrics:**\n```yaml\nPerformance:\n- Application response time\n- Error rates\n- Resource utilization\n\nReliability:\n- System uptime\n- MTTR\n- MTBF\n```",
      "difficulty": "intermediate",
      "tags": [
        "metrics",
        "kpi"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-65": {
      "id": "gh-65",
      "question": "What is Mean Time to Recovery (MTTR)?",
      "answer": "MTTR is the average time it takes to recover from a system failure or incident.",
      "explanation": "MTTR is the average time it takes to recover from a system failure or incident.\n\nCalculation:\n```\nMTTR = Total Recovery Time / Number of Incidents\n```\n\nComponents of MTTR:\n1. **Detection Time:**\n- Time to identify the issue\n- Monitoring alerts\n\n2. **Response Time:**\n- Time to begin addressing the issue\n- Team mobilization\n\n3. **Resolution Time:**\n- Time to fix the issue\n- System restoration",
      "difficulty": "beginner",
      "tags": [
        "metrics",
        "kpi"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-66": {
      "id": "gh-66",
      "question": "What is Serverless Computing?",
      "answer": "Serverless computing is a cloud computing execution model where the cloud provider manages the infrastructure and automatically allocates resources ba...",
      "explanation": "Serverless computing is a cloud computing execution model where the cloud provider manages the infrastructure and automatically allocates resources based on demand.\n\nKey characteristics:\n1. **No Server Management:**\n- Zero infrastructure maintenance\n- Automatic scaling\n- Pay-per-use billing\n\n2. **Event-Driven:**\n- Function triggers\n- Automatic execution\n- Stateless operations\n\nExample AWS Lambda function:\n```javascript\nexports.handler = async (event) => {\ntry {\nconst result = await processEvent(event);\nreturn {\nstatusCode: 200,\nbody: JSON.stringify(result)\n};\n} catch (error) {\nreturn {\nstatusCode: 500,\nbody: JSON.stringify({ error: error.message })\n};\n}\n};\n```",
      "difficulty": "beginner",
      "tags": [
        "serverless",
        "lambda"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-68": {
      "id": "gh-68",
      "question": "What is Network Security in DevOps?",
      "answer": "Network Security in DevOps involves implementing security measures throughout the development and deployment pipeline to protect applications and infr...",
      "explanation": "Network Security in DevOps involves implementing security measures throughout the development and deployment pipeline to protect applications and infrastructure.\n\nKey components:\n1. **Infrastructure Security:**\n- Firewalls\n- VPNs\n- Network segmentation\n\n2. **Application Security:**\n- TLS encryption\n- API security\n- Authentication/Authorization\n\nExample of security group configuration:\n```yaml\nSecurityGroup:\nType: AWS::EC2::SecurityGroup\nProperties:\nGroupDescription: Web tier security group\nSecurityGroupIngress:\n- IpProtocol: tcp\nFromPort: 443\nToPort: 443\nCidrIp: 0.0.0.0/0\n- IpProtocol: tcp\nFromPort: 80\nToPort: 80\nCidrIp: 0.0.0.0/0\n```",
      "difficulty": "advanced",
      "tags": [
        "security",
        "network"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-69": {
      "id": "gh-69",
      "question": "What is Zero Trust Security?",
      "answer": "Zero Trust Security is a security model that requires strict identity verification for every person and device trying to access resources in a private...",
      "explanation": "Zero Trust Security is a security model that requires strict identity verification for every person and device trying to access resources in a private network.\n\nPrinciples:\n1. **Never Trust, Always Verify:**\n- Identity-based access\n- Continuous verification\n- Least privilege access\n\n2. **Implementation:**\n```yaml\nAccess Control:\n- Multi-factor authentication\n- Identity and access management\n- Device verification\n\nNetwork Security:\n- Micro-segmentation\n- Network isolation\n- Encrypted communications\n```",
      "difficulty": "advanced",
      "tags": [
        "security",
        "network"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-70": {
      "id": "gh-70",
      "question": "What is SSL/TLS?",
      "answer": "SSL/TLS is a cryptographic protocol used to secure communications between a client and a server.",
      "explanation": "SSL/TLS is a cryptographic protocol used to secure communications between a client and a server.\n\nKey concepts:\n1. **Encryption:**\n- Data is encrypted before transmission\n- Data is decrypted after transmission\n\n2. **Authentication:**\n- Verifies the identity of the communicating parties\n\nExample of SSL/TLS configuration:\n```yaml\nsecurity:\nssl:\nenabled: true\nprotocol: TLSv1.2\nciphers:\n- ECDHE-RSA-AES256-GCM-SHA384\n- ECDHE-RSA-AES128-GCM-SHA256\n```",
      "difficulty": "advanced",
      "tags": [
        "security",
        "network"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-71": {
      "id": "gh-71",
      "question": "What is a Web Application Firewall (WAF)?",
      "answer": "A Web Application Firewall (WAF) is a security device that monitors incoming traffic to a web application and blocks malicious traffic.",
      "explanation": "A Web Application Firewall (WAF) is a security device that monitors incoming traffic to a web application and blocks malicious traffic.\n\nKey features:\n1. **Filtering:**\n- Filters out malicious traffic\n- Allows legitimate traffic\n\n2. **Authentication:**\n- Verifies the identity of the communicating parties\n\nExample of WAF configuration:\n```yaml\nsecurity:\nwaf:\nenabled: true\nrules:\n- rule1\n- rule2\n```",
      "difficulty": "advanced",
      "tags": [
        "security",
        "network"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-72": {
      "id": "gh-72",
      "question": "What is Network Segmentation?",
      "answer": "Network Segmentation is the practice of dividing a network into smaller, more manageable segments to improve security and performance.",
      "explanation": "Network Segmentation is the practice of dividing a network into smaller, more manageable segments to improve security and performance.\n\nKey concepts:\n1. **Segmentation:**\n- Divides the network into smaller segments\n- Each segment is isolated from other segments\n\n2. **Security:**\n- Prevents unauthorized access to sensitive data\n- Improves network performance\n\nExample of network segmentation configuration:\n```yaml\nsecurity:\nnetwork:\nsegmentation:\nenabled: true\nrules:\n- rule1\n- rule2\n```",
      "difficulty": "advanced",
      "tags": [
        "security",
        "network"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-74": {
      "id": "gh-74",
      "question": "What is DevOps Culture?",
      "answer": "DevOps Culture is a set of practices and values that promotes collaboration between Development and Operations teams.",
      "explanation": "DevOps Culture is a set of practices and values that promotes collaboration between Development and Operations teams.\n\nKey principles:\n1. **Collaboration:**\n- Shared responsibility\n- Cross-functional teams\n- Open communication\n\n2. **Continuous Improvement:**\n- Learning from failures\n- Experimentation\n- Feedback loops\n\n3. **Automation:**\n- Automate repetitive tasks\n- Infrastructure as Code\n- Continuous Integration/Delivery",
      "difficulty": "beginner",
      "tags": [
        "culture",
        "soft-skills"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-75": {
      "id": "gh-75",
      "question": "What are the core DevOps practices that enable continuous delivery and collaboration?",
      "answer": "DevOps practices include CI/CD, Infrastructure as Code, automated testing, monitoring, and fostering collaboration between development and operations teams.",
      "explanation": "DevOps practices enable faster, more reliable software delivery through automation and collaboration:\n\n## Technical Practices\n- **Continuous Integration/Continuous Deployment (CI/CD)** - Automated build, test, and deployment pipelines\n- **Infrastructure as Code (IaC)** - Managing infrastructure through version-controlled code\n- **Automated Testing** - Unit, integration, and end-to-end test automation\n- **Monitoring & Observability** - Real-time system health and performance tracking\n- **Configuration Management** - Consistent environment setup and management\n\n## Cultural Practices\n- **Collaboration** - Breaking down silos between Dev and Ops teams\n- **Shared Responsibility** - Joint ownership of application lifecycle\n- **Blameless Post-mortems** - Learning from failures without blame\n- **Continuous Learning** - Regular skill development and knowledge sharing\n\n## Process Practices\n- **Version Control** - Git-based code and configuration management\n- **Agile Methodology** - Iterative development with short feedback loops\n- **Release Management** - Controlled, predictable software releases\n- **Incident Response** - Structured approach to handling production issues",
      "diagram": "graph TD\n    A[Development Team] --> B[Version Control]\n    B --> C[CI/CD Pipeline]\n    C --> D[Automated Testing]\n    D --> E[Staging Environment]\n    E --> F[Production Deployment]\n    F --> G[Monitoring & Alerts]\n    G --> H[Operations Team]\n    H --> I[Incident Response]\n    I --> A\n    J[Infrastructure as Code] --> E\n    J --> F\n    K[Configuration Management] --> E\n    K --> F",
      "difficulty": "intermediate",
      "tags": [
        "culture",
        "soft-skills"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-81": {
      "id": "gh-81",
      "question": "What is Cloud Migration?",
      "answer": "Cloud Migration is the process of moving digital assets — applications, data, IT resources — from on-premises infrastructure to cloud infrastructure.",
      "explanation": "Cloud Migration is the process of moving digital assets — applications, data, IT resources — from on-premises infrastructure to cloud infrastructure.\n\nKey aspects:\n1. **Planning:**\n- Assessment\n- Strategy development\n- Resource planning\n\n2. **Execution:**\n```yaml\nMigration Steps:\n- Data migration\n- Application migration\n- Testing\n- Validation\n- Cutover\n```",
      "difficulty": "beginner",
      "tags": [
        "migration",
        "cloud"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-82": {
      "id": "gh-82",
      "question": "What are Cloud Migration Strategies?",
      "answer": "Common cloud migration strategies (6 R's):",
      "explanation": "Common cloud migration strategies (6 R's):\n\n1. **Rehosting (Lift and Shift):**\n- Moving applications without changes\n- Quickest migration method\n- Minimal optimization\n\n2. **Replatforming (Lift, Tinker and Shift):**\n- Minor optimizations\n- Cloud-specific improvements\n- Maintaining core architecture\n\n3. **Refactoring/Re-architecting:**\n```yaml\nBenefits:\n- Better cloud-native features\n- Improved scalability\n- Enhanced performance\nChallenges:\n- More time-consuming\n- Higher initial costs\n- Required expertise\n```",
      "difficulty": "intermediate",
      "tags": [
        "migration",
        "cloud"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-83": {
      "id": "gh-83",
      "question": "What is Cloud Assessment?",
      "answer": "Cloud Assessment is the process of evaluating the suitability of cloud services for a specific use case or workload.",
      "explanation": "Cloud Assessment is the process of evaluating the suitability of cloud services for a specific use case or workload.\n\nKey components:\n1. **Assessment Criteria:**\n- Cloud service capabilities\n- Cost and pricing\n- Security and compliance\n- Performance and scalability\n- Disaster recovery and high availability\n\n2. **Assessment Methodology:**\n- Cloud service comparison\n- Risk assessment\n- Cost-benefit analysis",
      "difficulty": "advanced",
      "tags": [
        "migration",
        "cloud"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-84": {
      "id": "gh-84",
      "question": "What is Application Modernization?",
      "answer": "Application Modernization is the process of transforming existing applications to leverage cloud-native features and capabilities.",
      "explanation": "Application Modernization is the process of transforming existing applications to leverage cloud-native features and capabilities.\n\nKey components:\n1. **Application Analysis:**\n- Current application state\n- Application architecture\n- Technology stack\n\n2. **Modernization Strategy:**\n- Cloud-native architecture\n- Microservices\n- Containerization\n- Serverless computing\n\n3. **Migration:**\n- Data migration\n- Application migration\n- Testing\n- Validation\n- Cutover",
      "difficulty": "beginner",
      "tags": [
        "migration",
        "cloud"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-85": {
      "id": "gh-85",
      "question": "What are Cloud Migration Tools?",
      "answer": "Cloud Migration Tools are software tools that help automate the migration of applications and data to cloud platforms.",
      "explanation": "Cloud Migration Tools are software tools that help automate the migration of applications and data to cloud platforms.\n\nKey components:\n1. **Data Migration Tools:**\n- Database migration tools\n- Application migration tools\n- Data synchronization tools\n\n2. **Application Migration Tools:**\n- Application packaging tools\n- Application containerization tools\n- Application serverless tools\n\n3. **Migration Orchestration Tools:**\n- Workflow automation tools\n- Service coordination tools\n- Resource scheduling tools",
      "difficulty": "intermediate",
      "tags": [
        "migration",
        "cloud"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-86": {
      "id": "gh-86",
      "question": "What is Platform Engineering?",
      "answer": "Platform Engineering is the discipline of designing, building, and maintaining an Internal Developer Platform (IDP). An IDP provides a self-service la...",
      "explanation": "Platform Engineering is the discipline of designing, building, and maintaining an Internal Developer Platform (IDP). An IDP provides a self-service layer that enables development teams to autonomously manage the lifecycle of their applications without needing deep expertise in underlying infrastructure, CI/CD, or operational tooling. The goal is to enhance developer experience, productivity, and velocity while ensuring standardization, compliance, and operational excellence.\n\n**Key Aspects of Platform Engineering:**\n1.  **Internal Developer Platform (IDP):** The core product created by a platform engineering team. It typically includes:\n*   **Self-Service Capabilities:** Developers can provision infrastructure, set up CI/CD pipelines, deploy applications, and access monitoring/logging tools through a user-friendly interface or API.\n*   **Golden Paths:** Pre-configured, validated workflows and toolchains for common tasks (e.g., creating a new microservice, deploying to Kubernetes).\n*   **Abstraction:** Hides the complexity of underlying tools and infrastructure.\n*   **Standardization:** Enforces best practices, security policies, and compliance across teams.\n2.  **Developer Experience (DevEx):** A primary focus is to reduce cognitive load on developers and streamline their workflows.\n3.  **Automation:** Automating as much of the application lifecycle as possible.\n4.  **Collaboration:** Platform teams work closely with development teams to understand their needs and gather feedback.\n5.  **Product Mindset:** Treating the IDP as a product with users (developers), requiring continuous iteration and improvement.\n\n**Benefits:**\n*   **Increased Developer Velocity & Productivity:** Developers spend less time on infrastructure and operational tasks.\n*   **Improved Reliability & Stability:** Standardized and automated processes reduce human error.\n*   **Enhanced Security & Compliance:** Policies are embedded into the platform.\n*   **Faster Time to Market:** Streamlined workflows accelerate the delivery of new features.\n*   **Scalability:** Enables organizations to scale their development efforts more effectively.\n\n**Example IDP Components:**\n```mermaid\ngraph TD\nsubgraph IDP [Internal Developer Platform]\nA[Developer Portal/CLI] --> B{Self-Service APIs}\nB --> C[Service Catalog]\nB --> D[CI/CD Automation]\nB --> E[Infrastructure Provisioning]\nB --> F[Monitoring & Observability Tools]\nB --> G[Security & Compliance Policies]\nend\nDev[Developer] --> A\nD --> H[Deployment Targets e.g., Kubernetes]\nE --> I[Cloud Providers/On-prem Infra]\nF --> J[Logging & Metrics Systems]\nG --> D\nG --> E\n```",
      "difficulty": "advanced",
      "tags": [
        "advanced",
        "cloud"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-87": {
      "id": "gh-87",
      "question": "What is FinOps?",
      "answer": "FinOps (Cloud Financial Operations) is an evolving cloud financial management discipline and cultural practice that enables organizations to get maxim...",
      "explanation": "FinOps (Cloud Financial Operations) is an evolving cloud financial management discipline and cultural practice that enables organizations to get maximum business value by helping engineering, finance, technology, and business teams to collaborate on data-driven spending decisions. It focuses on understanding cloud costs, optimizing spending, and implementing governance.\n\n**Core Principles of FinOps:**\n1.  **Collaboration:** Teams need to collaborate. Engineering, finance, product, and leadership must work together.\n2.  **Ownership:** Decisions are driven by the business value of cloud. Teams take ownership of their cloud usage, cost, and efficiency.\n3.  **Centralized Team:** A centralized FinOps team (often a CCoE - Cloud Center of Excellence subset) drives governance and best practices.\n4.  **Reporting & Visibility:** Timely, accessible, and accurate reports are crucial for understanding cloud spend.\n5.  **Cost Optimization:** Teams are empowered to optimize for cost, balancing performance, quality, and speed.\n6.  **Predictable Economics:** Strive for predictable cloud economics through forecasting, budgeting, and managing variances.\n\n**Phases of FinOps Lifecycle:**\n1.  **Inform:** Provide visibility into cloud spending through allocation, tagging, showback, and chargeback.\n*   Tools: Cloud provider cost management tools (AWS Cost Explorer, Azure Cost Management, GCP Billing), third-party tools (Cloudability, Apptio Cloudability, Flexera One).\n2.  **Optimize:** Implement cost-saving measures.\n*   Examples: Right-sizing instances, using reserved instances/savings plans, identifying and terminating idle resources, implementing auto-scaling, choosing appropriate storage tiers.\n3.  **Operate:** Define and enforce policies, establish budgets, and continuously monitor and improve.\n*   Examples: Setting budget alerts, automating cost control measures, performing regular cost reviews.\n\n**Benefits of FinOps:**\n*   Improved financial control and predictability of cloud costs.\n*   Increased ROI from cloud investments.\n*   Better alignment between cloud spending and business objectives.\n*   Enhanced collaboration between finance and engineering teams.\n*   Data-driven decision-making for cloud resource utilization.",
      "difficulty": "advanced",
      "tags": [
        "advanced",
        "cloud"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-88": {
      "id": "gh-88",
      "question": "What is Policy as Code?",
      "answer": "Policy as Code (PaC) is the practice of defining, managing, and automating policies using code and version control systems, similar to Infrastructure ...",
      "explanation": "Policy as Code (PaC) is the practice of defining, managing, and automating policies using code and version control systems, similar to Infrastructure as Code (IaC). Instead of manually configuring policies through UIs or disparate systems, PaC allows organizations to express policies in a high-level, human-readable language, store them in a Git repository, and apply them automatically throughout the development lifecycle and in production environments.\n\n**Key Concepts:**\n1.  **Policy Definition:** Policies are written in a declarative language (e.g., Rego for Open Policy Agent, Sentinel for HashiCorp tools).\n2.  **Version Control:** Policies are stored in Git, enabling versioning, auditing, and collaboration.\n3.  **Automation:** Policies are automatically enforced at various stages (e.g., CI/CD pipeline, infrastructure provisioning, Kubernetes admission control).\n4.  **Shift Left:** Enables early detection and prevention of policy violations during development.\n5.  **Auditability:** Provides a clear audit trail of policy changes and enforcement.\n\n**Use Cases:**\n*   **Security:** Enforcing security best practices, such as disallowing public S3 buckets or ensuring encryption.\n*   **Compliance:** Meeting regulatory requirements (e.g., GDPR, HIPAA) by codifying compliance rules.\n*   **Cost Management:** Preventing the creation of overly expensive resources.\n*   **Operational Consistency:** Ensuring standardized configurations across environments.\n*   **Kubernetes Governance:** Controlling what can be deployed to a Kubernetes cluster (e.g., required labels, resource limits, image sources).\n\n**Popular Tools:**\n*   **Open Policy Agent (OPA):** An open-source, general-purpose policy engine.\n*   **HashiCorp Sentinel:** A policy as code framework embedded in HashiCorp enterprise products (Terraform, Vault, Nomad, Consul).\n*   **Kyverno:** A policy engine designed specifically for Kubernetes.\n*   Cloud provider specific tools (e.g., AWS Config Rules, Azure Policy).\n\n**Example (Conceptual OPA/Rego):**\n```rego\npackage main\n\n# Deny deployments if an image is not from a trusted registry\ndeny[msg] {\ninput.kind == \"Deployment\"\nimage_name := input.spec.template.spec.containers[_].image\nnot startswith(image_name, \"trusted.registry.io/\")\nmsg := sprintf(\"Image '%v' is not from a trusted registry\", [image_name])\n}\n```",
      "difficulty": "advanced",
      "tags": [
        "advanced",
        "cloud"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-89": {
      "id": "gh-89",
      "question": "What is Chaos Engineering?",
      "answer": "Chaos Engineering is the discipline of experimenting on a distributed system in production in order to build confidence in the system's capability to ...",
      "explanation": "Chaos Engineering is the discipline of experimenting on a distributed system in production in order to build confidence in the system's capability to withstand turbulent and unexpected conditions. It's a proactive approach to identifying weaknesses by intentionally injecting failures and observing the system's response.\n\n**Principles of Chaos Engineering:**\n1.  **Build a Hypothesis around Steady State Behavior:** Define what normal system behavior looks like (e.g., key performance indicators, SLIs).\n2.  **Vary Real-world Events:** Simulate failures that can occur in production (e.g., server crashes, network latency, disk failures, dependency unavailability).\n3.  **Run Experiments in Production (or a Production-like Environment):** Testing in production is crucial as it's the only way to understand how the system behaves under real-world load and conditions. Start with staging environments if needed.\n4.  **Automate Experiments to Run Continuously:** Integrate chaos experiments into CI/CD pipelines or run them regularly to ensure ongoing resilience.\n5.  **Minimize Blast Radius:** Start with small, controlled experiments and gradually increase the scope to limit potential negative impact.\n\n**Process of a Chaos Experiment:**\n1.  **Define Steady State:** Identify measurable metrics that indicate normal system behavior.\n2.  **Hypothesize:** Formulate a hypothesis about how the system will respond to a specific failure. (e.g., \"If we introduce 100ms latency to the database, the API response time will increase by no more than 150ms, and there will be no errors.\")\n3.  **Design Experiment:** Determine the type of failure to inject, the scope, and the duration.\n4.  **Execute Experiment:** Inject the failure.\n5.  **Measure and Analyze:** Observe the system's behavior and compare it to the hypothesis.\n6.  **Learn and Improve:** If the system didn't behave as expected, identify the weakness and implement fixes. If it did, increase confidence or expand the experiment.\n\n**Benefits:**\n*   Uncovers hidden issues and weaknesses before they cause major outages.\n*   Improves system resilience and fault tolerance.\n*   Increases confidence in the system's ability to handle failures.\n*   Reduces incident response time and mean time to recovery (MTTR).\n*   Validates monitoring, alerting, and auto-remediation mechanisms.\n\n**Common Tools:**\n*   **Chaos Monkey (Netflix):** Randomly terminates virtual machine instances.\n*   **Gremlin:** A \"Failure-as-a-Service\" platform offering various chaos experiments.\n*   **Chaos Mesh:** A cloud-native chaos engineering platform for Kubernetes.\n*   **AWS Fault Injection Simulator (FIS):** A managed service for running fault injection experiments on AWS.\n*   **LitmusChaos:** An open-source chaos engineering framework for Kubernetes.",
      "difficulty": "advanced",
      "tags": [
        "advanced",
        "cloud"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-90": {
      "id": "gh-90",
      "question": "What is Blue/Green Deployment?",
      "answer": "Blue/Green Deployment is a continuous deployment strategy that aims to minimize downtime and risk by maintaining two identical production environments...",
      "explanation": "Blue/Green Deployment is a continuous deployment strategy that aims to minimize downtime and risk by maintaining two identical production environments, referred to as \"Blue\" and \"Green.\" Only one environment serves live production traffic at any given time.\n\n**How it Works:**\n1.  **Live Environment (Blue):** The current production environment handling all user traffic.\n2.  **Staging/New Environment (Green):** An identical environment where the new version of the application is deployed and thoroughly tested.\n3.  **Traffic Switch:** Once the Green environment is verified, a router or load balancer redirects all incoming traffic from Blue to Green. The Green environment now becomes the live production environment.\n4.  **Rollback:** If issues are detected in the Green environment after the switch, traffic can be quickly routed back to the Blue environment (which still runs the old, stable version).\n5.  **Promotion:** After a period of monitoring the new Green environment, the Blue environment can be updated to the new version to become the staging environment for the next release, or it can be decommissioned.\n\n```mermaid\ngraph TD\n    LB[Load Balancer] -->|Switch| Blue[\"Blue Env<br/>v1\"]\n    LB -->|Switch| Green[\"Green Env<br/>v2\"]\n    Blue -.->|Rollback| LB\n    style Blue fill:#3b82f6,stroke:#fff\n    style Green fill:#22c55e,stroke:#fff\n```\n\n**Benefits:**\n*   **Near-Zero Downtime:** Traffic is switched instantaneously.\n*   **Reduced Risk:** The new version is fully tested in an identical production environment before going live.\n*   **Rapid Rollback:** Reverting to the previous version is as simple as switching traffic back.\n*   **Simplified Release Process:** The process is straightforward and well-understood.\n\n**Considerations:**\n*   **Resource Costs:** Requires maintaining two full production environments, which can be expensive.\n*   **Database Compatibility:** Managing database schema changes and data synchronization between Blue and Green environments can be complex. Strategies like using backward-compatible changes or separate database instances are often employed.\n*   **Stateful Applications:** Handling user sessions and other stateful components requires careful planning during the switch.\n*   **Long-running Transactions:** Can be affected during the switchover.",
      "difficulty": "advanced",
      "tags": [
        "advanced",
        "cloud"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-91": {
      "id": "gh-91",
      "question": "What is Feature Flagging?",
      "answer": "Feature Flagging (also known as Feature Toggles or Feature Switches) is a software development technique that allows teams to modify system behavior w...",
      "explanation": "Feature Flagging (also known as Feature Toggles or Feature Switches) is a software development technique that allows teams to modify system behavior without changing code and redeploying. It involves wrapping new features in conditional logic (the \"flag\") that can be toggled on or off in a running application, often via a configuration service.\n\n**Core Concepts:**\n1.  **Decoupling Deployment from Release:** Code can be deployed to production environments with new features \"turned off\" (hidden behind a flag). The feature is then \"released\" (turned on) for users at a later time, independently of the deployment.\n2.  **Conditional Logic:** Code paths for the new feature are executed only if the corresponding flag is enabled.\n3.  **Configuration Service:** A central service or configuration file is often used to manage the state of feature flags, allowing dynamic updates without code changes.\n\n**Types of Feature Flags:**\n*   **Release Toggles:** Used to enable or disable features for all users, often for canary releases or to quickly disable a problematic feature.\n*   **Experiment Toggles (A/B Testing):** Used to show different versions of a feature to different segments of users to measure impact.\n*   **Ops Toggles:** Used to control operational aspects of the system, like enabling detailed logging or switching to a backup system during an incident.\n*   **Permission Toggles:** Used to control access to features for specific user groups (e.g., beta testers, premium users).\n\n**Benefits:**\n*   **Reduced Risk:** New features can be tested in production with a limited audience (canary release) or turned off quickly if issues arise (\"kill switch\").\n*   **Continuous Delivery/Trunk-Based Development:** Allows developers to merge code to the main branch more frequently, even if features are incomplete, by keeping them hidden behind flags.\n*   **A/B Testing and Experimentation:** Facilitates testing different feature variations with real users.\n*   **Gradual Rollouts:** Features can be rolled out to progressively larger groups of users.\n*   **Operational Control:** Provides levers to manage system behavior in production.\n*   **Faster Feedback Loops:** Get feedback on features from a subset of users before a full release.\n\n**Considerations:**\n*   **Flag Management Complexity:** A large number of flags can become difficult to manage. Requires a clear strategy for naming, organizing, and retiring flags.\n*   **Testing Overhead:** Need to test code paths with flags both on and off.\n*   **Technical Debt:** Old flags that are no longer needed should be removed to avoid cluttering the codebase.\n*   **Performance:** Checking flag states might add a small overhead, though usually negligible.",
      "difficulty": "advanced",
      "tags": [
        "advanced",
        "cloud"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-92": {
      "id": "gh-92",
      "question": "What is a Service Catalog?",
      "answer": "A Service Catalog is a centralized, curated list of IT services that an organization offers to its employees or customers. In the context of DevOps an...",
      "explanation": "A Service Catalog is a centralized, curated list of IT services that an organization offers to its employees or customers. In the context of DevOps and Platform Engineering, it's a key component of an Internal Developer Platform (IDP), providing developers with a self-service portal to discover, request, and provision standardized resources, tools, and environments.\n\n**Key Characteristics & Purpose:**\n1.  **Discoverability:** Provides a single place for users (typically developers) to find available services (e.g., databases, CI/CD pipeline templates, Kubernetes clusters, monitoring dashboards).\n2.  **Standardization:** Offers pre-configured, vetted, and compliant versions of services, ensuring consistency and adherence to organizational best practices.\n3.  **Self-Service:** Enables users to request and provision services on-demand without manual intervention from IT operations or platform teams.\n4.  **Automation:** Behind the scenes, service requests from the catalog trigger automated provisioning workflows.\n5.  **Lifecycle Management:** Can include information about service versions, support, and decommissioning.\n6.  **Transparency:** Often includes details about service SLAs, costs, and usage guidelines.\n\n**Benefits:**\n*   **Increased Developer Productivity:** Developers can quickly access the resources they need without waiting for manual fulfillment.\n*   **Improved Governance & Compliance:** Ensures that only approved and compliant services are used.\n*   **Reduced Operational Overhead:** Automates service provisioning, freeing up operations teams.\n*   **Enhanced Consistency:** Standardized services reduce configuration drift and compatibility issues.\n*   **Cost Control:** Can provide visibility into service costs and help manage cloud spend by offering optimized options.\n*   **Better User Experience:** Simplifies the process of obtaining IT resources.\n\n**Examples of Services in a Developer-Focused Service Catalog:**\n*   New Microservice Template (with CI/CD pipeline)\n*   Managed PostgreSQL Database (various sizes)\n*   Kubernetes Namespace with pre-defined quotas\n*   On-demand Test Environment\n*   Access to a specific logging or monitoring tool\n*   Vulnerability Scanning Service\n\n**Tools:**\n*   **Backstage (CNCF):** An open platform for building developer portals, often used to create service catalogs.\n*   **Port:** A developer portal platform.\n*   IT Service Management (ITSM) tools (e.g., ServiceNow, Jira Service Management) can also be adapted.\n*   Custom-built portals.",
      "difficulty": "advanced",
      "tags": [
        "advanced",
        "cloud"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-93": {
      "id": "gh-93",
      "question": "What is a Service Level Agreement (SLA)?",
      "answer": "A Service Level Agreement (SLA) is a formal, externally-facing contract or commitment between a service provider and its customers (or users). It defi...",
      "explanation": "A Service Level Agreement (SLA) is a formal, externally-facing contract or commitment between a service provider and its customers (or users). It defines the specific level of service that will be provided, including metrics, responsibilities, and remedies or penalties if the agreed-upon service levels are not met.\n\n**Key Components of an SLA:**\n1.  **Service Description:** Clearly defines the service being provided.\n2.  **Parties Involved:** Identifies the service provider and the customer.\n3.  **Agreement Period:** Specifies the duration for which the SLA is valid.\n4.  **Service Availability:** Defines the expected uptime or availability of the service (e.g., 99.9% uptime per month).\n5.  **Performance Metrics:** Specifies key performance indicators (KPIs) and their targets (e.g., API response time, data processing throughput).\n6.  **Responsibilities:** Outlines the duties of both the service provider and the customer.\n7.  **Support and Escalation Procedures:** Details how support will be provided, response times for issues, and how problems will be escalated.\n8.  **Exclusions:** Lists conditions or events that are not covered by the SLA (e.g., scheduled maintenance, force majeure).\n9.  **Remedies or Penalties (Service Credits):** Describes the compensation or actions (e.g., service credits, discounts) if the provider fails to meet the SLA terms.\n10. **Reporting and Monitoring:** Specifies how service performance will be tracked and reported to the customer.\n\n**Purpose in DevOps/SRE:**\n*   **Sets Expectations:** Clearly communicates to users what level of service they can expect.\n*   **Drives Reliability Efforts:** While SLAs are external, they often drive internal targets (SLOs) to ensure commitments are met.\n*   **Accountability:** Provides a basis for holding the service provider accountable for performance.\n*   **Business Alignment:** Helps align IT services with business needs and user expectations.\n\n**Distinction from SLOs and SLIs:**\n*   **SLA (Agreement):** The formal contract with consequences.\n*   **SLO (Objective):** Internal targets set by the service provider to meet or exceed the SLA. SLOs are typically stricter than SLAs to provide a buffer.\n*   **SLI (Indicator):** The actual measurements of service performance (e.g., measured uptime, actual response time). SLIs are used to track performance against SLOs.\n\n**Example SLA Clause for Availability:**\n\"The Service Provider guarantees 99.9% Uptime for the Service during any calendar month. Uptime is defined as the percentage of time the Service is accessible and functioning correctly. If Uptime falls below 99.9% in a given month, the Customer will be eligible for a Service Credit of 5% of their monthly service fee for that month.\"",
      "difficulty": "advanced",
      "tags": [
        "advanced",
        "cloud"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-94": {
      "id": "gh-94",
      "question": "What is a Service Level Objective (SLO)?",
      "answer": "A Service Level Objective (SLO) is a specific, measurable, and achievable internal target for a particular aspect of service performance or reliabilit...",
      "explanation": "A Service Level Objective (SLO) is a specific, measurable, and achievable internal target for a particular aspect of service performance or reliability. SLOs are a key component of Site Reliability Engineering (SRE) practices and are used to guide engineering decisions and balance reliability work with feature development.\n\n**Key Characteristics of an SLO:**\n1.  **Service-Specific:** Defined for a particular user-facing service or critical internal system.\n2.  **User-Focused:** Based on what matters to users (e.g., availability, latency, correctness).\n3.  **Measurable:** Quantifiable using specific metrics (SLIs).\n4.  **Target Value:** A specific numerical goal (e.g., 99.9% availability, 99th percentile latency < 200ms).\n5.  **Measurement Window:** The period over which the SLO is evaluated (e.g., rolling 28 days, calendar month).\n6.  **Internal Target:** Used by the team providing the service to manage and improve reliability. SLOs are typically stricter than any corresponding SLAs to provide a safety margin.\n\n**Purpose of SLOs:**\n*   **Data-Driven Decisions:** Provide a quantitative basis for making decisions about reliability, such as when to invest in more resilient infrastructure or when to prioritize bug fixes over new features.\n*   **Error Budgets:** SLOs directly define error budgets. An error budget is the amount of time or number of events a service can fail to meet its SLO without breaching it. For example, an SLO of 99.9% availability over 30 days allows for approximately 43 minutes of downtime (the error budget).\n*   **Balancing Reliability and Innovation:** If the service is consistently meeting its SLOs (i.e., not consuming its error budget), the team can focus more on feature development. If the error budget is being consumed rapidly, the team must prioritize reliability work.\n*   **Shared Understanding:** Creates a common language and understanding of reliability goals across development, operations, and product teams.\n*   **Alerting:** SLO burn rates (how quickly the error budget is being consumed) are often used to trigger alerts, prompting action before the SLO is breached.\n\n**How to Define Good SLOs:**\n1.  **Identify Critical User Journeys (CUJs):** What are the most important things users do with the service?\n2.  **Choose Appropriate SLIs:** Select metrics that accurately reflect the user experience for those CUJs (e.g., request success rate, latency at a specific percentile).\n3.  **Set Achievable Targets:** Consider historical performance, user expectations, and business requirements. Don't aim for 100% if it's not necessary or feasible, as it can be prohibitively expensive and stifle innovation.\n4.  **Document and Communicate:** Ensure SLOs are well-documented and understood by all stakeholders.\n5.  **Iterate:** Regularly review and refine SLOs based on new data and changing requirements.\n\n**Example SLO:**\n*   **Service:** User Login API\n*   **SLI:** Percentage of successful login requests (HTTP 200 responses) over all valid login attempts.\n*   **Target:** 99.95%\n*   **Period:** Measured over a rolling 28-day window.\n*   **Consequence (Internal):** If the error budget (0.05%) is exceeded, new feature development for the login service is paused, and all engineering effort is directed towards reliability improvements until the service is back within SLO.",
      "difficulty": "advanced",
      "tags": [
        "advanced",
        "cloud"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-95": {
      "id": "gh-95",
      "question": "What is a Service Level Indicator (SLI)?",
      "answer": "A Service Level Indicator (SLI) is a quantitative measure of some aspect of the level of service provided to users. SLIs are the raw data points or me...",
      "explanation": "A Service Level Indicator (SLI) is a quantitative measure of some aspect of the level of service provided to users. SLIs are the raw data points or metrics used to assess performance against Service Level Objectives (SLOs). They are crucial for objectively understanding how a service is performing from a user's perspective.\n\n**Key Characteristics of an SLI:**\n1.  **Quantitative Measure:** A specific, numerical value derived from system telemetry.\n2.  **User-Centric:** Reflects an aspect of service performance that directly impacts user experience.\n3.  **Directly Measurable:** Can be obtained from monitoring systems, logs, or other data sources.\n4.  **Good Proxy for User Happiness:** A change in the SLI should correlate with a change in user satisfaction.\n5.  **Reliably Measured:** The measurement itself should be accurate and dependable.\n\n**Common Types of SLIs:**\n*   **Availability:** Measures the proportion of time the service is usable or the percentage of successful requests.\n*   *Example:* (Number of successful HTTP requests / Total valid HTTP requests) * 100%.\n*   **Latency:** Measures the time taken to serve a request. Often measured at specific percentiles (e.g., 95th, 99th percentile) to understand typical and worst-case performance.\n*   *Example:* The 99th percentile of API response times for the `/users` endpoint over the last 5 minutes.\n*   **Error Rate:** Measures the proportion of requests that result in errors.\n*   *Example:* (Number of HTTP 5xx responses / Total valid HTTP requests) * 100%.\n*   **Throughput:** Measures the rate at which the system processes requests or data.\n*   *Example:* Requests per second (RPS) handled by the shopping cart service.\n*   **Durability:** Measures the likelihood that data stored in the system will be retained over a long period without corruption.\n*   *Example:* Probability of a stored object remaining intact and accessible after one year.\n*   **Correctness/Quality:** Measures if the service provides the right answer or performs the right action.\n*   *Example:* Percentage of search queries that return relevant results, or proportion of financial transactions processed without data errors.\n\n**How to Choose Good SLIs:**\n1.  **Focus on User Experience:** What aspects of performance or reliability are most important to your users?\n2.  **Keep it Simple:** Choose a small number of meaningful SLIs rather than trying to track everything.\n3.  **Ensure it's Actionable:** The SLI should provide data that can lead to improvements or inform decisions.\n4.  **Distinguish from Raw Metrics:** While SLIs are derived from metrics, they are specifically chosen and often processed (e.g., aggregated, percentiled) to represent service level.\n\n**Relationship with SLOs and SLAs:**\n*   SLIs are the **measurements**.\n*   SLOs are the **targets** for those measurements (e.g., SLI for availability >= 99.9%).\n*   SLAs are the **agreements** with users, often based on achieving certain SLOs, and typically include consequences if not met.\n\n**Example:**\n*   **User Journey:** User uploads a photo.\n*   **Possible SLIs:**\n*   `upload_success_rate`: (Number of successful photo uploads / Total photo upload attempts) * 100%\n*   `upload_latency_p95`: 95th percentile of time taken from initiating upload to confirmation.\n*   **Corresponding SLO for `upload_success_rate` might be:** 99.9% over a 7-day window.",
      "difficulty": "advanced",
      "tags": [
        "advanced",
        "cloud"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-96": {
      "id": "gh-96",
      "question": "What is a Runbook?",
      "answer": "A Runbook is a detailed document or a collection of procedures that outlines the steps required to perform a specific operational task or to respond t...",
      "explanation": "A Runbook is a detailed document or a collection of procedures that outlines the steps required to perform a specific operational task or to respond to a particular situation or alert. Traditionally, runbooks were manual guides for system administrators and operators. In modern DevOps and SRE practices, there's a strong emphasis on automating runbooks wherever possible (Runbook Automation).\n\n**Key Characteristics and Purpose of Runbooks:**\n1.  **Standardization:** Provides a consistent and repeatable way to perform routine tasks or respond to incidents, reducing human error.\n2.  **Documentation:** Serves as a knowledge base for operational procedures, especially for less common tasks or for new team members.\n3.  **Efficiency:** Streamlines operations by providing clear, step-by-step instructions, reducing the time taken to resolve issues or complete tasks.\n4.  **Incident Response:** Crucial for quickly addressing known issues, system failures, or alerts by providing pre-defined diagnostic and remediation steps.\n5.  **Training:** Useful for training new operations staff or for cross-training team members.\n6.  **Automation Target:** Well-defined manual runbooks are excellent candidates for automation. Each step in a runbook can potentially be scripted.\n\n**Common Contents of a Runbook:**\n*   **Title/Purpose:** Clear description of the task or situation the runbook addresses.\n*   **Triggers/Symptoms:** When to use this runbook (e.g., specific alert, error message, user report).\n*   **Prerequisites:** Any conditions that must be met or tools/access required before starting.\n*   **Step-by-Step Procedures:** Detailed instructions for diagnosis, remediation, or task execution.\n*   **Verification Steps:** How to confirm the task was successful or the issue is resolved.\n*   **Rollback Procedures:** Steps to revert any changes if the procedure fails or causes unintended consequences.\n*   **Escalation Points:** Who to contact if the runbook doesn't resolve the issue or if further assistance is needed.\n*   **Expected Outcomes:** What the system state should be after successful execution.\n*   **Associated Logs/Metrics:** Pointers to relevant logs or dashboards for investigation.\n\n**Evolution to Runbook Automation:**\nThe goal is to automate as many runbook procedures as possible to reduce manual toil, improve response times, and ensure consistency. This involves using scripting languages (Python, Bash), configuration management tools (Ansible), orchestration tools (Kubernetes operators), or specialized runbook automation platforms.\n\n**Example Scenario for a Runbook: High CPU Utilization on a Web Server**\n1.  **Trigger:** Alert: \"CPU utilization on webserver-01 > 90% for 5 minutes.\"\n2.  **Diagnosis Steps:**\n*   SSH into `webserver-01`.\n*   Run `top` or `htop` to identify high-CPU processes.\n*   Check application logs for errors related to the identified process (`/var/log/app/error.log`).\n*   Check web server access logs for unusual traffic patterns (`/var/log/nginx/access.log`).\n3.  **Possible Remediation Steps (based on diagnosis):**\n*   If it's a known memory leak in the application: Restart the application service (`sudo systemctl restart myapp`).\n*   If it's a sudden traffic spike: Consider temporarily scaling out if auto-scaling hasn't kicked in.\n*   If it's a rogue process: Identify and kill the process (use with caution).\n4.  **Verification:** Monitor CPU utilization for the next 15 minutes to ensure it returns to normal levels.\n5.  **Escalation:** If the issue persists, escalate to the on-call SRE for the web application.\n\n**Benefits of Well-Maintained Runbooks:**\n*   Faster Mean Time To Resolution (MTTR).\n*   Reduced operator errors.\n*   Improved operational consistency.\n*   Better knowledge sharing within the team.\n*   Facilitates automation efforts.",
      "difficulty": "advanced",
      "tags": [
        "advanced",
        "cloud"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-97": {
      "id": "gh-97",
      "question": "What is a Playbook in Incident Response?",
      "answer": "An Incident Response Playbook is a specialized type of runbook focused specifically on guiding the actions of a response team during and after a secur...",
      "explanation": "An Incident Response Playbook is a specialized type of runbook focused specifically on guiding the actions of a response team during and after a security incident or significant operational outage. It provides a predefined and structured set of steps to detect, analyze, contain, eradicate, and recover from specific types of incidents.\n\n**Key Differences from General Runbooks:**\n*   **Focus:** Primarily on security incidents (e.g., data breach, malware infection, DDoS attack) or major service outages, whereas runbooks can cover routine operational tasks as well.\n*   **Goal:** To minimize the impact of an incident, restore service quickly and securely, and gather information for post-incident analysis and learning.\n*   **Audience:** Often used by security teams (CSIRT - Computer Security Incident Response Team), SREs, and operations staff involved in incident handling.\n\n**Core Components of an Incident Response Playbook:**\n1.  **Incident Type:** Clearly defines the specific incident the playbook addresses (e.g., \"Phishing Attack Leading to Credential Compromise,\" \"Ransomware Outbreak,\" \"Database Unavailability\").\n2.  **Roles and Responsibilities:** Identifies who is responsible for each action (e.g., Incident Commander, Communications Lead, Technical Lead).\n3.  **Preparation/Prerequisites:** Steps taken before an incident occurs (e.g., ensuring logging is enabled, access to necessary tools).\n4.  **Detection and Identification:** How to recognize that this specific type of incident is occurring (e.g., specific alerts, user reports, anomalous behavior).\n5.  **Containment Strategy:** Steps to limit the scope and impact of the incident (e.g., isolating affected systems, blocking malicious IPs, disabling compromised accounts).\n6.  **Eradication:** How to remove the cause of the incident (e.g., removing malware, patching vulnerabilities).\n7.  **Recovery:** Steps to restore affected systems and services to normal operation safely.\n8.  **Post-Incident Activities (Postmortem):** Procedures for analyzing the incident, documenting lessons learned, and improving defenses and response capabilities. This includes evidence preservation.\n9.  **Communication Plan:** Guidelines for internal and external communication (e.g., notifying stakeholders, legal, PR, customers if necessary).\n10. **Checklists and Decision Trees:** To guide responders through complex scenarios.\n11. **Tools and Resources:** List of necessary tools, contact information, and knowledge base articles.\n\n**Benefits of Incident Response Playbooks:**\n*   **Faster Response Times:** Enables quicker, more decisive action during high-stress situations.\n*   **Consistency:** Ensures a standardized approach to incident handling, regardless of who is responding.\n*   **Reduced Human Error:** Minimizes mistakes made under pressure.\n*   **Improved Decision Making:** Provides a framework for making critical decisions.\n*   **Compliance and Legal Adherence:** Helps meet regulatory requirements for incident response.\n*   **Effective Training Tool:** Can be used for drills and exercises to prepare teams.\n*   **Continuous Improvement:** Forms the basis for learning from incidents and refining response strategies.\n\n**Example Playbook Scenario: DDoS Attack Mitigation**\n*   **Detection:** Monitoring alerts for unusually high traffic volumes, high server load, and service unavailability.\n*   **Initial Triage:** Confirm it's a DDoS attack and not a legitimate traffic spike. Identify attack vectors (e.g., volumetric, protocol, application layer).\n*   **Containment/Mitigation:**\n*   Engage DDoS mitigation service (e.g., Cloudflare, AWS Shield).\n*   Implement rate limiting and IP blocking at edge firewalls/load balancers.\n*   Scale out backend resources if applicable.\n*   **Recovery:** Monitor traffic and service health. Gradually remove mitigation measures once the attack subsides.\n*   **Post-Incident:** Analyze attack patterns, identify vulnerabilities, update mitigation strategies, and document the incident.",
      "difficulty": "advanced",
      "tags": [
        "advanced",
        "cloud"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-98": {
      "id": "gh-98",
      "question": "What is Observability?",
      "answer": "Observability is a measure of how well you can understand the internal state or condition of a complex system based only on knowledge of its external ...",
      "explanation": "Observability is a measure of how well you can understand the internal state or condition of a complex system based only on knowledge of its external outputs (logs, metrics, traces). It's about being able to ask arbitrary questions about your system's behavior without having to pre-define all possible failure modes or dashboards in advance. While monitoring tells you *whether* a system is working, observability helps you understand *why* it isn't (or is) working.\n\n**Three Pillars of Observability:**\n1.  **Logs:**\n*   **What:** Immutable, timestamped records of discrete events that happened over time. Logs provide detailed, context-rich information about specific occurrences.\n*   **Use Cases:** Debugging specific errors, auditing, understanding event sequences.\n*   **Examples:** Application logs (e.g., stack traces), system logs, audit logs, web server access logs.\n2.  **Metrics:**\n*   **What:** Aggregated numerical representations of data about your system measured over intervals of time. Metrics are good for understanding trends, patterns, and overall system health.\n*   **Use Cases:** Dashboarding, alerting on thresholds, capacity planning, trend analysis.\n*   **Examples:** CPU utilization, memory usage, request counts, error rates, queue lengths, latency percentiles.\n3.  **Traces (Distributed Tracing):**\n*   **What:** Show the lifecycle of a request as it flows through a distributed system. A single trace is composed of multiple \"spans,\" where each span represents a unit of work (e.g., an API call, a database query) within a service.\n*   **Use Cases:** Understanding request paths, identifying bottlenecks in distributed systems, debugging latency issues, visualizing service dependencies.\n*   **Examples:** A trace showing a user request hitting an API gateway, then an authentication service, then a product service, and finally a database.\n\n**Diagram: The Three Pillars**\n```mermaid\ngraph TD\nO[Observability] --> L[Logs]\nO --> M[Metrics]\nO --> T[Traces]\n\nL --Provides--> LD[Detailed Event Context]\nM --Provides--> MA[Aggregated System Health & Trends]\nT --Provides--> TP[Request Flow & Bottleneck Analysis]\n\nsubgraph System\nApp1[Application/Service 1]\nApp2[Application/Service 2]\nApp3[Infrastructure]\nend\n\nApp1 --> L\nApp1 --> M\nApp1 -- Generates Spans For --> T\nApp2 --> L\nApp2 --> M\nApp2 -- Generates Spans For --> T\nApp3 --> L\nApp3 --> M\n```\n\n**Why is Observability Important?**\n*   **Complex Systems:** Modern applications are often distributed, microservice-based, and run on dynamic infrastructure, making them harder to understand and debug.\n*   **Unknown Unknowns:** Observability helps investigate issues you didn't anticipate or for which you don't have pre-built dashboards.\n*   **Faster Debugging & MTTR:** Enables quicker root cause analysis when incidents occur.\n*   **Better Performance Understanding:** Provides deep insights into how different parts of the system interact and perform.\n*   **Proactive Issue Detection:** While often used reactively, rich observability data can help identify anomalies before they become major problems.\n\n**Monitoring vs. Observability:**\n*   **Monitoring:** Typically involves collecting predefined sets of metrics and alerting when these metrics cross certain thresholds. It answers known questions (e.g., \"Is the CPU over 80%?\").\n*   **Observability:** Provides the tools and data to explore and understand system behavior, enabling you to answer new questions about states you didn't predict. It helps explore the unknown unknowns.\nMonitoring is a part of observability, but observability encompasses a broader capability to interrogate your system.\n\n**Key Enablers for Observability:**\n*   **Rich Instrumentation:** Applications and infrastructure must be thoroughly instrumented to emit quality logs, metrics, and traces.\n*   **Correlation:** The ability to correlate data across logs, metrics, and traces is crucial (e.g., linking a specific log entry to a trace ID and relevant metrics).\n*   **High Cardinality Data:** Ability to analyze data with many unique attribute values (e.g., user IDs, request IDs).\n*   **Querying & Analytics:** Powerful tools to query, visualize, and analyze the collected telemetry data.",
      "difficulty": "advanced",
      "tags": [
        "advanced",
        "cloud"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-99": {
      "id": "gh-99",
      "question": "What is Tracing in Observability?",
      "answer": "Tracing is the process of tracking the flow of requests through a distributed system, helping to identify bottlenecks and performance issues. Tools li...",
      "explanation": "Tracing is the process of tracking the flow of requests through a distributed system, helping to identify bottlenecks and performance issues. Tools like Jaeger and Zipkin are commonly used.",
      "difficulty": "advanced",
      "tags": [
        "advanced",
        "cloud"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-100": {
      "id": "gh-100",
      "question": "What is a Sidecar Pattern?",
      "answer": "The Sidecar Pattern is a container-based design pattern where an auxiliary container (the \"sidecar\") is deployed alongside the main application contai...",
      "explanation": "The Sidecar Pattern is a container-based design pattern where an auxiliary container (the \"sidecar\") is deployed alongside the main application container within the same deployment unit (e.g., a Kubernetes Pod). The sidecar container enhances or extends the functionality of the main application container by providing supporting features, and they share resources like networking and storage.\n\n**Key Characteristics:**\n1.  **Co-location:** The main application container and the sidecar container(s) run together in the same Pod (in Kubernetes) or task definition (in ECS).\n2.  **Shared Lifecycle:** Sidecars are typically started and stopped with the main application container.\n3.  **Shared Resources:** They share the same network namespace (can communicate via `localhost`) and can share volumes for data exchange.\n4.  **Encapsulation & Separation of Concerns:** The sidecar encapsulates common functionalities (like logging, monitoring, proxying) that would otherwise need to be built into each application or run as separate agents on the host.\n5.  **Language Agnostic:** Sidecars can be written in different languages than the main application, allowing teams to use the best tool for the job for auxiliary tasks.\n\n**Diagram: Sidecar Pattern in a Kubernetes Pod**\n```mermaid\ngraph TD\nsubgraph Kubernetes Pod\ndirection LR\nAppContainer[Main Application Container]\nSidecarContainer[Sidecar Container]\nAppContainer -- localhost --> SidecarContainer\nSidecarContainer -- localhost --> AppContainer\nsubgraph Shared Resources\nNetwork[Shared Network Namespace]\nVolumes[Shared Volumes]\nend\nAppContainer --> Network\nSidecarContainer --> Network\nAppContainer --> Volumes\nSidecarContainer --> Volumes\nend\nExternalTraffic --> Network\nNetwork --> ExternalServices\n```\n\n**Common Use Cases for Sidecars:**\n*   **Log Aggregation:** A sidecar (e.g., Fluentd, Fluent Bit) collects logs from the main application container (e.g., from stdout/stderr or a shared volume) and forwards them to a centralized logging system.\n*   **Metrics Collection:** A sidecar exports metrics from the application (e.g., Prometheus exporter) or provides a metrics endpoint.\n*   **Service Mesh Proxy:** In a service mesh (e.g., Istio, Linkerd), a sidecar proxy (e.g., Envoy) runs alongside each application instance to manage network traffic, enforce policies, provide security (mTLS), and collect telemetry.\n*   **Configuration Management:** A sidecar can fetch configuration updates from a central store and make them available to the main application, or reload the application when configuration changes.\n*   **Secrets Management:** A sidecar can fetch secrets from a vault and inject them into the application environment or a shared volume.\n*   **Network Utilities:** Providing network-related functions like SSL/TLS termination, circuit breaking, or acting as a reverse proxy.\n*   **File Synchronization:** Syncing files from a remote source (like Git or S3) to a shared volume for the application to use.\n\n**Benefits:**\n*   **Modularity and Reusability:** Common functionalities can be developed and deployed as separate sidecar containers, reusable across multiple applications.\n*   **Reduced Application Complexity:** Keeps the main application focused on its core business logic.\n*   **Independent Upgrades:** Sidecar functionalities can be updated independently of the main application.\n*   **Polyglot Environments:** Allows auxiliary functions to be written in different languages/technologies.\n*   **Encapsulation:** Isolates auxiliary tasks from the main application.\n\n**Considerations:**\n*   **Resource Overhead:** Each sidecar consumes additional resources (CPU, memory).\n*   **Increased Complexity (Deployment Unit):** While simplifying the application, it makes the deployment unit (Pod) more complex with multiple containers.\n*   **Inter-Process Communication:** Communication between the app and sidecar (though often via localhost or shared volumes) needs to be efficient.",
      "difficulty": "advanced",
      "tags": [
        "advanced",
        "cloud"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-101": {
      "id": "gh-101",
      "question": "What is a Service Mesh Control Plane?",
      "answer": "In a service mesh architecture, the Control Plane is the centralized component responsible for configuring, managing, and monitoring the behavior of t...",
      "explanation": "In a service mesh architecture, the **Control Plane** is the centralized component responsible for configuring, managing, and monitoring the behavior of the data plane proxies (typically sidecar proxies like Envoy) that run alongside each service instance. It does not handle any of the actual request traffic between services; that is the role of the data plane.\n\n**Key Responsibilities of a Service Mesh Control Plane:**\n1.  **Configuration Distribution:**\n*   It pushes configuration updates (e.g., routing rules, traffic policies, security policies, telemetry configurations) to all the sidecar proxies in the mesh.\n*   This allows dynamic changes to traffic flow and policies without restarting services or proxies.\n2.  **Service Discovery:**\n*   Provides an up-to-date registry of all services and their instances within the mesh, enabling proxies to know where to route traffic.\n*   Often integrates with the underlying platform's service discovery (e.g., Kubernetes DNS, Consul).\n3.  **Policy Enforcement Configuration:**\n*   Defines and distributes policies related to security (e.g., mTLS requirements, authorization rules), traffic management (e.g., retries, timeouts, circuit breakers), and rate limiting.\n*   The control plane tells the proxies *what* policies to enforce; the proxies do the actual enforcement.\n4.  **Certificate Management:**\n*   Manages the lifecycle of TLS certificates used for mutual TLS (mTLS) authentication between services, ensuring secure communication.\n*   Distributes certificates and keys to the proxies.\n5.  **Telemetry Aggregation (or Configuration for it):**\n*   While proxies collect raw telemetry data (metrics, logs, traces), the control plane often provides a central point to configure what telemetry is collected and where it should be sent. Some control planes may also aggregate certain metrics.\n6.  **API for Operators:**\n*   Exposes APIs and CLIs for operators to interact with the service mesh, define configurations, and observe its state.\n\n**Interaction with Data Plane:**\n```mermaid\ngraph TD\nCP[\"Control Plane\"] -->|Config| DP1[\"Proxy 1\"]\nCP -->|Config| DP2[\"Proxy 2\"]\nS1[Service A] <--> DP1\nS2[Service B] <--> DP2\nDP1 <-->|Traffic| DP2\nDP1 -->|Telemetry| O[\"Observability\"]\nDP2 -->|Telemetry| O\n```\n*   The Control Plane configures the Data Plane proxies.\n*   The Data Plane proxies handle all request traffic between services based on the configuration received from the Control Plane.\n*   The Data Plane proxies send telemetry data back to monitoring/observability systems (often configured via the Control Plane).\n\n**Popular Service Mesh Control Planes:**\n*   **Istio:** `istiod` is the control plane daemon.\n*   **Linkerd:** The control plane is composed of several components (e.g., `controller`, `destination`).\n*   **Consul Connect:** Consul servers act as the control plane.\n*   **Kuma/Kong Mesh:** `kuma-cp` is the control plane.\n\n**Benefits of a Separate Control Plane:**\n*   **Centralized Management:** Provides a single point of control and visibility over the entire service mesh.\n*   **Decoupling:** Separates the management logic from the request processing logic, making the system more modular and resilient.\n*   **Scalability:** The control plane can be scaled independently of the data plane.\n*   **Dynamic Configuration:** Enables runtime changes to traffic management and policies without service restarts.",
      "difficulty": "advanced",
      "tags": [
        "advanced",
        "cloud"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-102": {
      "id": "gh-102",
      "question": "What is GitHub Actions?",
      "answer": "GitHub Actions is a CI/CD and automation platform built into GitHub that allows you to automate workflows for building, testing, and deploying code di...",
      "explanation": "GitHub Actions is a CI/CD and automation platform built into GitHub that allows you to automate workflows for building, testing, and deploying code directly from your repository.",
      "difficulty": "advanced",
      "tags": [
        "advanced",
        "cloud"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-103": {
      "id": "gh-103",
      "question": "What is a Self-Healing System?",
      "answer": "A Self-Healing System is an architecture that can automatically detect and recover from failures, often using automation, monitoring, and orchestratio...",
      "explanation": "A Self-Healing System is an architecture that can automatically detect and recover from failures, often using automation, monitoring, and orchestration tools to maintain availability.",
      "difficulty": "advanced",
      "tags": [
        "advanced",
        "cloud"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-104": {
      "id": "gh-104",
      "question": "What is Canary Analysis?",
      "answer": "Canary Analysis is a deployment strategy that releases changes to a small subset of users or servers before rolling out to the entire infrastructure, ...",
      "explanation": "Canary Analysis is a deployment strategy that releases changes to a small subset of users or servers before rolling out to the entire infrastructure, allowing for early detection of issues.",
      "difficulty": "advanced",
      "tags": [
        "advanced",
        "cloud"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "gh-105": {
      "id": "gh-105",
      "question": "What is Infrastructure Drift?",
      "answer": "Infrastructure Drift occurs when the actual state of infrastructure diverges from the desired state defined in code, often due to manual changes or co...",
      "explanation": "Infrastructure Drift occurs when the actual state of infrastructure diverges from the desired state defined in code, often due to manual changes or configuration errors. Tools like Terraform and Ansible can help detect and correct drift.",
      "difficulty": "advanced",
      "tags": [
        "advanced",
        "cloud"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "de-127": {
      "id": "de-127",
      "question": "How would you implement a multi-stage Docker build to optimize image size while maintaining the ability to debug production issues? Explain the trade-offs between build-time and runtime optimization.",
      "answer": "Use multi-stage builds with separate compile and runtime stages, keeping debug symbols in intermediate layer for production debugging.",
      "explanation": "**Multi-Stage Docker Build Strategy:**\n\n**Stage 1 (Builder):**\n- Contains full build tools (compilers, debug symbols)\n- Builds application with debug information\n- Creates optimized binary\n\n**Stage 2 (Runtime):**\n- Minimal base image (alpine/slim)\n- Copies only compiled binary and runtime dependencies\n- Excludes build tools and debug symbols\n\n**Trade-offs:**\n- **Build-time optimization:** Larger builder image, longer build process\n- **Runtime optimization:** Smaller production image, faster deployment\n- **Debugging capability:** Need to maintain builder artifacts or use separate debug image\n\n**Implementation:**\n```dockerfile\n# Build stage\nFROM golang:1.19 AS builder\nWORKDIR /app\nCOPY . .\nRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o main .\n\n# Runtime stage\nFROM alpine:latest\nRUN apk --no-cache add ca-certificates\nWORKDIR /root/\nCOPY --from=builder /app/main .\nCMD [\"./main\"]\n```\n\n**Advanced Debugging:**\n- Use separate debug image with symbols\n- Implement health checks and logging\n- Consider build cache optimization",
      "diagram": "graph TD\n    A[Source Code] --> B[Builder Stage]\n    B --> C[Compile with Debug]\n    C --> D[Optimized Binary]\n    D --> E[Runtime Stage]\n    E --> F[Minimal Image]\n    F --> G[Production Container]\n    B -.-> H[Debug Image]\n    H --> I[Debug Symbols]",
      "difficulty": "advanced",
      "tags": [
        "docker",
        "containers"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "de-135": {
      "id": "de-135",
      "question": "You have a Helm chart that needs to deploy different configurations for staging and production environments. The staging environment should use 2 replicas with 512Mi memory limit, while production should use 5 replicas with 2Gi memory limit. How would you structure your values files and templates to handle this requirement?",
      "answer": "Use values-staging.yaml and values-production.yaml files with environment-specific configs, then deploy with helm install -f values-{env}.yaml",
      "explanation": "## Environment-Specific Helm Configurations\n\n### Solution Structure\n\n1. **Base values.yaml**:\n```yaml\napp:\n  name: myapp\n  image:\n    repository: myapp\n    tag: latest\n\ndeployment:\n  replicas: 3\n  resources:\n    limits:\n      memory: 1Gi\n    requests:\n      memory: 512Mi\n```\n\n2. **values-staging.yaml**:\n```yaml\ndeployment:\n  replicas: 2\n  resources:\n    limits:\n      memory: 512Mi\n    requests:\n      memory: 256Mi\n```\n\n3. **values-production.yaml**:\n```yaml\ndeployment:\n  replicas: 5\n  resources:\n    limits:\n      memory: 2Gi\n    requests:\n      memory: 1Gi\n```\n\n4. **Deployment template** (templates/deployment.yaml):\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ .Values.app.name }}\nspec:\n  replicas: {{ .Values.deployment.replicas }}\n  template:\n    spec:\n      containers:\n      - name: {{ .Values.app.name }}\n        image: {{ .Values.app.image.repository }}:{{ .Values.app.image.tag }}\n        resources:\n          {{- toYaml .Values.deployment.resources | nindent 10 }}\n```\n\n### Deployment Commands\n\n```bash\n# Staging deployment\nhelm install myapp-staging ./mychart -f values-staging.yaml\n\n# Production deployment\nhelm install myapp-prod ./mychart -f values-production.yaml\n```\n\n### Key Benefits\n\n- **Value Inheritance**: Environment files override base values\n- **DRY Principle**: Common configurations in base values.yaml\n- **Environment Isolation**: Clear separation of concerns\n- **Template Reusability**: Single template works for all environments",
      "diagram": "graph TD\n    A[Base values.yaml] --> B[Common Config]\n    C[values-staging.yaml] --> D[Staging Overrides]\n    E[values-production.yaml] --> F[Production Overrides]\n    \n    B --> G[Helm Template Engine]\n    D --> G\n    F --> G\n    \n    G --> H[Staging Deployment]\n    G --> I[Production Deployment]\n    \n    H --> J[2 Replicas<br/>512Mi Memory]\n    I --> K[5 Replicas<br/>2Gi Memory]\n    \n    style A fill:#e1f5fe\n    style C fill:#fff3e0\n    style E fill:#ffebee\n    style H fill:#fff3e0\n    style I fill:#ffebee",
      "difficulty": "intermediate",
      "tags": [
        "helm",
        "k8s"
      ],
      "lastUpdated": "2025-12-12T09:15:12.319Z"
    },
    "de-136": {
      "id": "de-136",
      "question": "You have a GitOps workflow where application configs are stored in a separate config repository from the application code. A developer pushes code changes that require updating both the application image tag and adding a new environment variable. Describe the complete GitOps flow from code commit to deployment, including how you handle the dependency between application and config changes.",
      "answer": "Code push triggers CI to build image, update config repo with new tag/env var, ArgoCD detects config changes and deploys to cluster.",
      "explanation": "## GitOps Flow with Separate Config Repository\n\n### 1. Initial Code Push\n- Developer commits application code changes to the **application repository**\n- CI/CD pipeline is triggered (GitHub Actions, Jenkins, etc.)\n\n### 2. Build and Image Management\n- CI builds new container image with updated code\n- Image is tagged (e.g., `v1.2.3` or commit SHA) and pushed to container registry\n- CI runs tests and security scans\n\n### 3. Config Repository Update\n- CI pipeline makes automated commit to **config repository**\n- Updates include:\n  - New image tag in deployment manifests\n  - Addition of new environment variable in ConfigMap/Secret\n- This can be done via:\n  - Direct commit with service account\n  - Pull request for review (recommended)\n\n### 4. GitOps Controller Detection\n- ArgoCD/Flux detects changes in config repository\n- Compares desired state (Git) vs actual state (cluster)\n- Identifies drift and plans synchronization\n\n### 5. Deployment Execution\n- GitOps controller applies changes to Kubernetes cluster\n- Rolling update deploys new image with environment variables\n- Health checks ensure successful deployment\n\n### 6. Monitoring and Rollback\n- Monitor application metrics and logs\n- If issues arise, rollback via Git revert\n- GitOps controller automatically reverts cluster state\n\n### Key Benefits\n- **Separation of concerns**: App code vs infrastructure config\n- **Audit trail**: All changes tracked in Git\n- **Declarative**: Desired state defined in Git\n- **Automated**: Reduces manual deployment errors",
      "diagram": "graph TD\n    A[Developer Commits Code] --> B[CI Pipeline Triggered]\n    B --> C[Build & Test Application]\n    C --> D[Build Container Image]\n    D --> E[Push Image to Registry]\n    E --> F[Update Config Repository]\n    F --> G[Commit New Image Tag + Env Vars]\n    G --> H[ArgoCD Detects Config Changes]\n    H --> I[Compare Desired vs Actual State]\n    I --> J[Apply Changes to Cluster]\n    J --> K[Rolling Update Deployment]\n    K --> L[Health Checks Pass]\n    L --> M[Deployment Complete]\n    \n    N[Config Repository] --> H\n    O[Application Repository] --> A\n    P[Container Registry] --> J\n    \n    style A fill:#e1f5fe\n    style M fill:#c8e6c9\n    style H fill:#fff3e0",
      "difficulty": "intermediate",
      "tags": [
        "gitops",
        "argocd"
      ],
      "lastUpdated": "2025-12-12T09:15:31.620Z"
    },
    "de-137": {
      "id": "de-137",
      "question": "You have a Terraform configuration that creates an AWS S3 bucket. After running 'terraform apply', you realize you need to add versioning to the bucket. What's the safest way to modify your existing infrastructure?",
      "answer": "Add versioning block to existing resource, run terraform plan to review changes, then terraform apply to update the bucket in-place.",
      "explanation": "## Safe Infrastructure Updates with Terraform\n\nWhen modifying existing Terraform resources, follow these steps:\n\n1. **Modify the configuration**: Add the versioning block to your existing `aws_s3_bucket` resource\n2. **Plan first**: Run `terraform plan` to see what changes will be made\n3. **Review carefully**: Ensure Terraform shows an \"update in-place\" operation, not destroy/recreate\n4. **Apply changes**: Run `terraform apply` to update the existing bucket\n\n### Example Configuration:\n```hcl\nresource \"aws_s3_bucket\" \"example\" {\n  bucket = \"my-example-bucket\"\n}\n\nresource \"aws_s3_bucket_versioning\" \"example\" {\n  bucket = aws_s3_bucket.example.id\n  versioning_configuration {\n    status = \"Enabled\"\n  }\n}\n```\n\n### Why This Approach is Safe:\n- **No data loss**: Updates bucket settings without destroying it\n- **Predictable**: `terraform plan` shows exactly what will change\n- **Reversible**: Can disable versioning later if needed\n- **Best practice**: Always plan before applying changes",
      "diagram": "graph TD\n    A[Existing S3 Bucket] --> B[Modify Terraform Config]\n    B --> C[Add Versioning Block]\n    C --> D[terraform plan]\n    D --> E{Review Changes}\n    E -->|Safe Update| F[terraform apply]\n    E -->|Destructive Change| G[Revise Configuration]\n    G --> D\n    F --> H[Updated S3 Bucket with Versioning]\n    \n    style A fill:#e1f5fe\n    style H fill:#c8e6c9\n    style D fill:#fff3e0\n    style F fill:#f3e5f5",
      "difficulty": "beginner",
      "tags": [
        "terraform",
        "iac"
      ],
      "lastUpdated": "2025-12-12T09:15:44.757Z"
    },
    "fe-1": {
      "id": "fe-1",
      "question": "Explain the React Virtual DOM and Reconciliation process.",
      "answer": "Virtual DOM is a lightweight copy of the real DOM. Reconciliation is the sync process.",
      "explanation": "1. **Render**: React creates a Virtual DOM tree from components.\n2. **Diff**: When state changes, create a new VDOM tree and compare with the old one (Diffing Algorithm).\n3. **Patch**: Calculate the minimum number of changes needed and update the **Real DOM** in a batch.\n\n**Keys** help React identify which items have changed, added, or removed in lists, preventing unnecessary re-renders.",
      "diagram": "\nflowchart LR\n    State --> VDOM_New\n    VDOM_Old --> Diff\n    VDOM_New --> Diff{Diffing}\n    Diff --> Patch[Update Real DOM]\n",
      "difficulty": "intermediate",
      "tags": [
        "react",
        "perf",
        "internals"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "fe-2": {
      "id": "fe-2",
      "question": "What is the Event Loop in JavaScript? How do Microtasks differ from Macrotasks?",
      "answer": "The Event Loop coordinates the Call Stack, Web APIs, and Callback Queues.",
      "explanation": "JS is single-threaded.\n\n1. **Call Stack**: Executes sync code.\n2. **Microtask Queue**: Promises, MutationObservers. (Higher Priority - processed immediately after stack clears).\n3. **Macrotask Queue**: setTimeout, setInterval, I/O. (Processed one per loop tick).\n\n**Order**: Sync -> All Microtasks -> Render UI -> One Macrotask -> Repeat.",
      "diagram": "\ngraph TD\n    Stack[Call Stack] -->|Empty?| CheckMicro{Microtasks?}\n    CheckMicro -->|Yes| RunMicro[Run All Microtasks]\n    RunMicro --> CheckMicro\n    CheckMicro -->|No| Render[Render UI]\n    Render --> RunMacro[Run One Macrotask]\n    RunMacro --> Stack\n",
      "difficulty": "beginner",
      "tags": [
        "js",
        "async",
        "core"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "fe-3": {
      "id": "fe-3",
      "question": "Explain 'Closure' in JavaScript with a practical use case.",
      "answer": "A Closure is a function remembering its lexical scope even when executed outside that scope.",
      "explanation": "**Concept**: When a function returns another function, the inner function retains access to the outer function's variables.\n\n**Use Case: Data Privacy / Currying**\n```javascript\nfunction createCounter() {\n  let count = 0; // Private variable\n  return {\n    increment: () => count++,\n    get: () => count\n  };\n}\nconst c = createCounter();\nc.increment(); // 1\n// count is not accessible directly\n```",
      "diagram": "\ngraph TD\n    Outer[Outer Function] --> Var[count = 0]\n    Outer --> Inner[Inner Function]\n    Inner -.->|Remembers| Var\n",
      "difficulty": "intermediate",
      "tags": [
        "js",
        "scope",
        "patterns"
      ],
      "lastUpdated": "2025-12-12T09:07:04.187Z"
    },
    "fr-154": {
      "id": "fr-154",
      "question": "What happens when you load a large image without specifying width and height attributes in the HTML?",
      "answer": "The browser causes layout shift (CLS) as it reflows the page once the image dimensions are known after loading.",
      "explanation": "## Layout Shift Problem\n\nWhen you don't specify `width` and `height` attributes on an `<img>` tag, the browser doesn't know how much space to reserve for the image initially.\n\n### What Happens:\n\n1. **Initial Render**: Browser renders the page with 0px height for the image\n2. **Image Downloads**: Browser fetches the image file\n3. **Reflow**: Once loaded, browser calculates actual dimensions and shifts content below\n4. **Poor UX**: Users experience content jumping as they read\n\n### The Solution:\n\n```html\n<!-- Bad: No dimensions -->\n<img src=\"photo.jpg\" alt=\"Photo\">\n\n<!-- Good: Dimensions specified -->\n<img src=\"photo.jpg\" alt=\"Photo\" width=\"800\" height=\"600\">\n\n<!-- Modern: Aspect ratio with CSS -->\n<img src=\"photo.jpg\" alt=\"Photo\" style=\"aspect-ratio: 4/3; width: 100%;\">\n```\n\n### Performance Impact:\n\n- **CLS (Cumulative Layout Shift)**: Core Web Vital metric that measures visual stability\n- **User Experience**: Content jumping frustrates users and causes misclicks\n- **SEO**: Google uses CLS as a ranking factor\n\n### Best Practices:\n\n1. Always specify dimensions or aspect ratio\n2. Use `loading=\"lazy\"` for below-the-fold images\n3. Consider responsive images with `srcset`\n4. Use CSS `aspect-ratio` for flexible layouts",
      "diagram": "graph TD\n    A[Browser Starts Parsing HTML] --> B[Encounters img Tag]\n    B --> C{Has width/height?}\n    C -->|No| D[Reserves 0px Space]\n    C -->|Yes| E[Reserves Correct Space]\n    D --> F[Renders Page]\n    E --> G[Renders Page]\n    F --> H[Image Downloads]\n    H --> I[Browser Calculates Size]\n    I --> J[Content Shifts Down]\n    J --> K[Poor CLS Score]\n    G --> L[Image Downloads]\n    L --> M[Fills Reserved Space]\n    M --> N[No Layout Shift]\n    N --> O[Good CLS Score]",
      "difficulty": "beginner",
      "tags": [
        "perf",
        "optimization"
      ],
      "lastUpdated": "2025-12-13T01:08:31.827Z"
    },
    "fr-157": {
      "id": "fr-157",
      "question": "What is the difference between `let`, `const`, and `var` in JavaScript?",
      "answer": "var is function-scoped and hoisted; let/const are block-scoped. const cannot be reassigned, let can be.",
      "explanation": "## Variable Declarations in JavaScript\n\n### `var`\n- **Scope**: Function-scoped (or globally-scoped if declared outside a function)\n- **Hoisting**: Hoisted to the top of its scope and initialized with `undefined`\n- **Reassignment**: Can be reassigned\n- **Redeclaration**: Can be redeclared in the same scope\n\n```javascript\nfunction example() {\n  console.log(x); // undefined (hoisted)\n  var x = 5;\n  var x = 10; // OK - can redeclare\n}\n```\n\n### `let`\n- **Scope**: Block-scoped (limited to the block `{}` where it's defined)\n- **Hoisting**: Hoisted but not initialized (temporal dead zone)\n- **Reassignment**: Can be reassigned\n- **Redeclaration**: Cannot be redeclared in the same scope\n\n```javascript\nif (true) {\n  let y = 5;\n  y = 10; // OK - can reassign\n  // let y = 15; // Error - cannot redeclare\n}\n// console.log(y); // Error - y is not defined\n```\n\n### `const`\n- **Scope**: Block-scoped\n- **Hoisting**: Hoisted but not initialized (temporal dead zone)\n- **Reassignment**: Cannot be reassigned\n- **Redeclaration**: Cannot be redeclared in the same scope\n- **Note**: For objects/arrays, the reference cannot change, but properties/elements can be modified\n\n```javascript\nconst z = 5;\n// z = 10; // Error - cannot reassign\n\nconst obj = { name: 'John' };\nobj.name = 'Jane'; // OK - modifying property\n// obj = {}; // Error - cannot reassign reference\n```\n\n### Best Practices\n- Use `const` by default\n- Use `let` when you need to reassign\n- Avoid `var` in modern JavaScript",
      "diagram": "graph TD\n    A[Variable Declaration] --> B[var]\n    A --> C[let]\n    A --> D[const]\n    B --> E[Function Scoped]\n    B --> F[Hoisted with undefined]\n    B --> G[Can Redeclare]\n    C --> H[Block Scoped]\n    C --> I[Temporal Dead Zone]\n    C --> J[Can Reassign]\n    D --> K[Block Scoped]\n    D --> L[Temporal Dead Zone]\n    D --> M[Cannot Reassign]\n    D --> N[Object Properties Mutable]",
      "difficulty": "beginner",
      "tags": [
        "js",
        "core"
      ],
      "lastUpdated": "2025-12-13T01:09:09.317Z"
    },
    "fr-161": {
      "id": "fr-161",
      "question": "How would you implement a React hook that tracks component render count and warns when it exceeds a threshold, while avoiding infinite render loops?",
      "answer": "Use useRef to persist count across renders and useEffect with no deps to increment. Check threshold and warn without triggering re-render.",
      "explanation": "## Implementation Strategy\n\n```javascript\nfunction useRenderCount(threshold = 50) {\n  const renderCount = useRef(0);\n  const hasWarned = useRef(false);\n  \n  renderCount.current += 1;\n  \n  if (renderCount.current > threshold && !hasWarned.current) {\n    console.warn(`Component rendered ${renderCount.current} times`);\n    hasWarned.current = true;\n  }\n  \n  return renderCount.current;\n}\n```\n\n## Key Concepts\n\n**Why useRef instead of useState?**\n- `useRef` doesn't trigger re-renders when updated\n- `useState` would cause infinite loop since updating state triggers render\n- Perfect for tracking values across renders without side effects\n\n**Avoiding Infinite Loops**\n- Never call `setState` directly in render body\n- Use `useRef` for mutable values that don't affect UI\n- `hasWarned` ref prevents repeated console warnings\n\n**When to Use**\n- Performance debugging in development\n- Detecting unnecessary re-renders\n- Monitoring component optimization effectiveness\n\n**Advanced Considerations**\n- Reset count on unmount with cleanup function\n- Add dev-only checks with `process.env.NODE_ENV`\n- Track render reasons with additional metadata",
      "diagram": "graph TD\n    A[Component Renders] --> B[useRenderCount Hook Called]\n    B --> C[Increment renderCount.current]\n    C --> D{Count > Threshold?}\n    D -->|No| E[Return Count]\n    D -->|Yes| F{Already Warned?}\n    F -->|No| G[Log Warning]\n    F -->|Yes| E\n    G --> H[Set hasWarned = true]\n    H --> E\n    E --> I[Component Continues Rendering]\n    I -.->|Next Render| A\n    \n    style B fill:#61dafb\n    style C fill:#ffd700\n    style G fill:#ff6b6b",
      "difficulty": "advanced",
      "tags": [
        "react",
        "perf"
      ],
      "lastUpdated": "2025-12-13T03:02:58.720Z"
    },
    "fr-162": {
      "id": "fr-162",
      "question": "Explain how JavaScript's event loop handles microtasks vs macrotasks. What happens when a Promise resolves inside a setTimeout callback?",
      "answer": "Microtasks (Promises) execute before macrotasks (setTimeout). Promise in setTimeout runs after that setTimeout completes, before next macrotask.",
      "explanation": "## Event Loop: Microtasks vs Macrotasks\n\n### Task Queue Types\n\n**Macrotasks (Task Queue):**\n- `setTimeout`, `setInterval`\n- `setImmediate` (Node.js)\n- I/O operations\n- UI rendering\n\n**Microtasks (Job Queue):**\n- `Promise.then/catch/finally`\n- `queueMicrotask()`\n- `MutationObserver`\n- `process.nextTick` (Node.js - highest priority)\n\n### Execution Order\n\n1. Execute synchronous code\n2. Execute ALL microtasks in queue\n3. Execute ONE macrotask\n4. Execute ALL microtasks again\n5. Render (if needed)\n6. Repeat from step 3\n\n### Example Analysis\n\n```javascript\nconsole.log('1');\n\nsetTimeout(() => {\n  console.log('2');\n  Promise.resolve().then(() => console.log('3'));\n}, 0);\n\nPromise.resolve().then(() => {\n  console.log('4');\n  setTimeout(() => console.log('5'), 0);\n});\n\nconsole.log('6');\n```\n\n**Output:** 1, 6, 4, 2, 3, 5\n\n**Why:**\n- `1`, `6`: Synchronous code runs first\n- `4`: Microtask queue empties before any macrotask\n- `2`: First macrotask (setTimeout) executes\n- `3`: Microtasks from that macrotask run immediately\n- `5`: Next macrotask (setTimeout queued from Promise)\n\n### Key Insight\n\nWhen a Promise resolves inside setTimeout, it creates a microtask that executes **after** the current macrotask completes but **before** the next macrotask. This prevents macrotask starvation and ensures Promise handlers run promptly.",
      "diagram": "graph TD\n    A[Call Stack] --> B{Stack Empty?}\n    B -->|Yes| C[Check Microtask Queue]\n    C -->|Has Microtasks| D[Execute ALL Microtasks]\n    D --> C\n    C -->|Empty| E[Check Macrotask Queue]\n    E -->|Has Macrotasks| F[Execute ONE Macrotask]\n    F --> B\n    E -->|Empty| G[Wait for Tasks]\n    G --> B\n    B -->|No| H[Execute Current Function]\n    H --> B\n    \n    style C fill:#90EE90\n    style D fill:#90EE90\n    style E fill:#FFB6C1\n    style F fill:#FFB6C1",
      "difficulty": "advanced",
      "tags": [
        "js",
        "core"
      ],
      "lastUpdated": "2025-12-13T03:03:14.094Z"
    },
    "fr-163": {
      "id": "fr-163",
      "question": "You have a React app rendering a list of 10,000 items. Each item has a complex component with multiple child components. The list scrolls slowly and feels janky. Describe three different optimization strategies you would apply, explaining when to use each approach and their trade-offs.",
      "answer": "Use virtualization (react-window), memoization (React.memo/useMemo), and code splitting. Each targets different bottlenecks.",
      "explanation": "## Three Core Optimization Strategies\n\n### 1. Virtualization (react-window/react-virtual)\n\n**When to use:** Large lists where only a few items are visible at once\n\n**How it works:**\n- Only renders items currently in viewport + small buffer\n- Dynamically mounts/unmounts components as user scrolls\n- Reduces DOM nodes from 10,000 to ~20-30\n\n```javascript\nimport { FixedSizeList } from 'react-window';\n\nconst VirtualList = () => (\n  <FixedSizeList\n    height={600}\n    itemCount={10000}\n    itemSize={50}\n    width=\"100%\"\n  >\n    {({ index, style }) => (\n      <div style={style}>Item {index}</div>\n    )}\n  </FixedSizeList>\n);\n```\n\n**Trade-offs:**\n- ✅ Massive performance gain for long lists\n- ✅ Constant memory usage regardless of list size\n- ❌ Requires fixed/estimated item heights\n- ❌ Breaks native browser features (find-in-page, accessibility)\n\n### 2. Memoization (React.memo, useMemo, useCallback)\n\n**When to use:** Components re-rendering unnecessarily with same props\n\n**How it works:**\n- `React.memo`: Prevents re-render if props haven't changed\n- `useMemo`: Caches expensive computations\n- `useCallback`: Stabilizes function references\n\n```javascript\nconst ListItem = React.memo(({ item, onDelete }) => {\n  const formattedData = useMemo(\n    () => expensiveFormat(item.data),\n    [item.data]\n  );\n  \n  return <div onClick={onDelete}>{formattedData}</div>;\n});\n\nconst List = () => {\n  const handleDelete = useCallback((id) => {\n    deleteItem(id);\n  }, []);\n  \n  return items.map(item => (\n    <ListItem key={item.id} item={item} onDelete={handleDelete} />\n  ));\n};\n```\n\n**Trade-offs:**\n- ✅ Prevents wasted renders\n- ✅ Easy to implement incrementally\n- ❌ Adds memory overhead for memoization\n- ❌ Shallow comparison can miss deep object changes\n- ❌ Overuse can hurt performance (comparison cost)\n\n### 3. Code Splitting & Lazy Loading\n\n**When to use:** Heavy components not immediately needed\n\n**How it works:**\n- Split bundle into chunks\n- Load components on-demand\n- Reduce initial JavaScript payload\n\n```javascript\nconst HeavyChart = lazy(() => import('./HeavyChart'));\n\nconst ListItem = ({ item }) => (\n  <div>\n    <h3>{item.title}</h3>\n    <Suspense fallback={<Spinner />}>\n      {item.showChart && <HeavyChart data={item.data} />}\n    </Suspense>\n  </div>\n);\n```\n\n**Trade-offs:**\n- ✅ Faster initial load\n- ✅ Reduces main bundle size\n- ❌ Network delay when loading chunks\n- ❌ Requires loading states\n- ❌ Can cause layout shifts\n\n## Decision Matrix\n\n| Scenario | Best Strategy |\n|----------|---------------|\n| 10k+ simple items | Virtualization |\n| Complex items, frequent parent updates | Memoization |\n| Heavy dependencies per item | Code splitting |\n| All of the above | Combine all three |\n\n## Measuring Impact\n\nUse React DevTools Profiler to identify:\n- **Render duration**: How long each component takes\n- **Render frequency**: How often components re-render\n- **Wasted renders**: Components rendering with same props",
      "diagram": "graph TD\n    A[10k Items List Problem] --> B{Identify Bottleneck}\n    B --> C[Too Many DOM Nodes]\n    B --> D[Unnecessary Re-renders]\n    B --> E[Large Bundle Size]\n    \n    C --> F[Virtualization]\n    F --> F1[Render only visible items]\n    F1 --> F2[~30 DOM nodes vs 10k]\n    \n    D --> G[Memoization]\n    G --> G1[React.memo on ListItem]\n    G1 --> G2[useMemo for computations]\n    G2 --> G3[useCallback for handlers]\n    \n    E --> H[Code Splitting]\n    H --> H1[Lazy load heavy components]\n    H1 --> H2[Reduce initial bundle]\n    \n    F2 --> I[Optimized App]\n    G3 --> I\n    H2 --> I\n    \n    style C fill:#ff6b6b\n    style D fill:#ffd93d\n    style E fill:#6bcf7f\n    style I fill:#4ecdc4",
      "difficulty": "advanced",
      "tags": [
        "perf",
        "optimization"
      ],
      "lastUpdated": "2025-12-13T03:04:11.191Z"
    },
    "fr-172": {
      "id": "fr-172",
      "question": "How would you optimize the rendering performance of a React component that displays a large list (10,000+ items) with frequent updates?",
      "answer": "Use virtualization, memoization, and debounced updates to minimize re-renders and DOM nodes.",
      "explanation": "For large lists with frequent updates, implement several performance optimizations:\n\n1. **Virtualization**: Only render visible items using libraries like react-window or react-virtualized. This reduces DOM nodes from 10,000+ to ~20-50 visible items.\n\n2. **Memoization**: Wrap list items in React.memo() to prevent unnecessary re-renders when props haven't changed.\n\n3. **Debouncing**: Debounce rapid updates (like search/filter) to batch changes and reduce render frequency.\n\n4. **Key optimization**: Use stable, unique keys (not array indices) to help React identify which items changed.\n\n5. **State management**: Consider using useReducer for complex list state to minimize state updates.\n\n6. **CSS containment**: Use `contain: strict` on list containers to isolate layout calculations.\n\nThese techniques combined can reduce render time from seconds to milliseconds.",
      "diagram": "graph TD\n    A[Large List Data] --> B[Virtualization Layer]\n    B --> C[Visible Items Only 20-50]\n    C --> D[Memoized Components]\n    D --> E[Optimized DOM]\n    \n    F[User Input] --> G[Debounce 300ms]\n    G --> H[Batched Updates]\n    H --> I[Minimal Re-renders]\n    \n    J[React.memo] --> K[Props Comparison]\n    K --> L[Skip Unnecessary Renders]\n    \n    M[Stable Keys] --> N[Efficient Reconciliation]\n    N --> O[Fast DOM Updates]",
      "difficulty": "intermediate",
      "tags": [
        "perf",
        "optimization"
      ],
      "lastUpdated": "2025-12-14T01:19:05.794Z"
    },
    "fr-173": {
      "id": "fr-173",
      "question": "What is the output of this code and explain the event loop behavior: console.log('A'); setTimeout(() => console.log('B'), 0); Promise.resolve().then(() => console.log('C')); Promise.resolve().then(() => console.log('D')); console.log('E');",
      "answer": "A, E, C, D, B. Microtasks execute before next macrotask.",
      "explanation": "# Event Loop Execution Order\n\n## Synchronous Code First\n- `console.log('A')` and `console.log('E')` execute synchronously\n\n## Microtask Queue Priority\n- `Promise.then()` callbacks enter microtask queue\n- Microtasks have higher priority than macrotasks\n- All microtasks execute before next macrotask\n- Microtasks execute in FIFO order: C then D\n\n## Macrotask Queue\n- `setTimeout` enters macrotask queue\n- Executes after all microtasks are complete\n\n## Key Concepts\n- **Call Stack**: Synchronous execution\n- **Microtask Queue**: Promises, async/await\n- **Macrotask Queue**: setTimeout, DOM events\n- **Event Loop**: Manages task scheduling\n\nThis demonstrates JavaScript's non-blocking nature and task prioritization.",
      "diagram": "graph TD\n    A[Start] --> B[console.log'A']\n    B --> C[console.log'E']\n    C --> D[Microtask Queue Empty?]\n    D -->|Yes| E[Execute Promise.then C]\n    E --> F[Execute Promise.then D]\n    F --> G[Microtask Queue Empty?]\n    G -->|Yes| H[Execute setTimeout B]\n    H --> I[End]\n    D -->|No| J[Wait for Microtasks]\n    J --> D\n    G -->|No| K[Wait for Microtasks]\n    K --> G",
      "difficulty": "advanced",
      "tags": [
        "js",
        "core"
      ],
      "lastUpdated": "2025-12-14T01:19:30.686Z"
    },
    "sre-1": {
      "id": "sre-1",
      "question": "What are SLIs, SLOs, and SLAs? How do they relate?",
      "answer": "Metrics, Goals, and Consequences.",
      "explanation": "1. **SLI (Indicator)**: The actual number. \"My latency is 200ms\".\n2. **SLO (Objective)**: The internal goal. \"Latency should be < 300ms for 99% of requests\".\n3. **SLA (Agreement)**: The legal contract with users. \"If latency > 500ms, we refund you 10%\".\n\n*SREs focus on SLOs. SLAs are for lawyers.*",
      "diagram": "graph TD\n    SLI[\"Indicator<br/>Reality\"] -->|Measured Against| SLO[\"Objective<br/>Goal\"]\n    SLO -->|Buffer| SLA[\"Agreement<br/>Contract\"]",
      "difficulty": "beginner",
      "tags": [
        "metrics",
        "policy",
        "definitions",
        "observability"
      ],
      "lastUpdated": "2025-12-12T09:07:04.188Z"
    },
    "sre-2": {
      "id": "sre-2",
      "question": "What is an Error Budget and how do you use it?",
      "answer": "100% - SLO = Error Budget. It's the allowed amount of unreliability.",
      "explanation": "If SLO is 99.9% uptime, you have 0.1% error budget (approx 43 mins/month).\n\n**Usage**:\n- **Budget Remaining?**: Ship features fast, take risks, do chaos engineering.\n- **Budget Exhausted?**: FREEZE deployments. Focus 100% on reliability until budget recovers.\n\n*It aligns Dev (speed) and Ops (stability) incentives.*",
      "diagram": "\ngraph LR\n    SLO[99.9% SLO] --> EB[0.1% Error Budget]\n    EB -->|Remaining| Ship[Ship Features]\n    EB -->|Exhausted| Freeze[Freeze Deploys]\n",
      "difficulty": "beginner",
      "tags": [
        "management",
        "concept",
        "risk"
      ],
      "lastUpdated": "2025-12-12T09:07:04.188Z"
    },
    "gh-59": {
      "id": "gh-59",
      "question": "What is Site Reliability Engineering?",
      "answer": "Site Reliability Engineering (SRE) is a discipline that incorporates aspects of software engineering and applies them to infrastructure and operations...",
      "explanation": "Site Reliability Engineering (SRE) is a discipline that incorporates aspects of software engineering and applies them to infrastructure and operations problems to create scalable and highly reliable software systems.\n\nKey principles:\n1. **Embrace Risk:**\n- Define acceptable risk levels\n- Use error budgets\n- Balance reliability and innovation\n\n2. **Eliminate Toil:**\n- Automate manual tasks\n- Reduce operational overhead\n- Focus on engineering work",
      "difficulty": "beginner",
      "tags": [
        "sre",
        "reliability"
      ],
      "lastUpdated": "2025-12-12T09:07:04.188Z"
    },
    "gh-60": {
      "id": "gh-60",
      "question": "What are Service Level Objectives (SLOs)?",
      "answer": "Service Level Objectives (SLOs) are specific, measurable targets for service performance that you set and agree to meet.",
      "explanation": "Service Level Objectives (SLOs) are specific, measurable targets for service performance that you set and agree to meet.\n\nExample SLO definition:\n```yaml\nService: User Authentication\nSLO:\nMetric: Availability\nTarget: 99.9%\nWindow: 30 days\nMeasurement:\n- Success rate of authentication requests\n- Latency under 300ms for 99% of requests\n```",
      "difficulty": "intermediate",
      "tags": [
        "sre",
        "reliability"
      ],
      "lastUpdated": "2025-12-12T09:07:04.188Z"
    },
    "gh-61": {
      "id": "gh-61",
      "question": "What are Service Level Indicators (SLIs)?",
      "answer": "Service Level Indicators (SLIs) are quantitative measures of service level aspects such as latency, throughput, availability, and error rate.",
      "explanation": "Service Level Indicators (SLIs) are quantitative measures of service level aspects such as latency, throughput, availability, and error rate.\n\nCommon SLIs:\n1. **Request Latency:**\n- Time to handle a request\n- Distribution of response times\n\n2. **Error Rate:**\n- Failed requests/total requests\n- Error budget consumption\n\n3. **System Throughput:**\n- Requests per second\n- Transactions per second",
      "difficulty": "intermediate",
      "tags": [
        "sre",
        "reliability"
      ],
      "lastUpdated": "2025-12-12T09:07:04.188Z"
    },
    "gh-62": {
      "id": "gh-62",
      "question": "What is Error Budget?",
      "answer": "An Error Budget is the maximum amount of time that a technical system can fail without contractual consequences. It's the difference between the SLO t...",
      "explanation": "An Error Budget is the maximum amount of time that a technical system can fail without contractual consequences. It's the difference between the SLO target and 100% reliability.\n\nExample calculation:\n```\nSLO Target: 99.9% uptime\nError Budget: 100% - 99.9% = 0.1%\nMonthly Error Budget: 43.2 minutes (0.1% of 30 days)\n```\n\nKey concepts:\n1. **Budget Calculation:**\n- Based on SLO targets\n- Measured over time windows\n- Reset periodically\n\n2. **Budget Usage:**\n- Track incidents\n- Monitor consumption\n- Alert on budget burn",
      "difficulty": "beginner",
      "tags": [
        "sre",
        "reliability"
      ],
      "lastUpdated": "2025-12-12T09:07:04.188Z"
    },
    "gh-63": {
      "id": "gh-63",
      "question": "What is Toil in SRE?",
      "answer": "Toil is the kind of work tied to running a production service that tends to be manual, repetitive, automatable, tactical, devoid of enduring value, an...",
      "explanation": "Toil is the kind of work tied to running a production service that tends to be manual, repetitive, automatable, tactical, devoid of enduring value, and that scales linearly as a service grows.\n\nCharacteristics of toil:\n1. **Manual work:**\n- No automation\n- Human intervention required\n- Repetitive tasks\n\n2. **Impact:**\n- Reduces time for project work\n- Increases operational overhead\n- Affects team morale\n\n3. **Solutions:**\n\nAutomation:\n- Script repetitive tasks\n- Implement self-service tools\n- Create automated workflows\n\nProcess Improvement:\n- Identify toil sources\n- Set toil budgets\n- Track toil metrics\n\nEngineering Solutions:\n- Design for automation\n- Build self-healing systems\n- Implement proper monitoring",
      "difficulty": "beginner",
      "tags": [
        "sre",
        "reliability"
      ],
      "lastUpdated": "2025-12-12T09:07:04.188Z"
    },
    "gh-73": {
      "id": "gh-73",
      "question": "What is Incident Management?",
      "answer": "Incident Management is the process of responding to and resolving IT service disruptions.",
      "explanation": "Incident Management is the process of responding to and resolving IT service disruptions.\n\nKey components:\n1. **Detection:**\n- Monitoring alerts\n- User reports\n- Automated detection\n\n2. **Response:**\n```yaml\nInitial Response:\n- Acknowledge incident\n- Assess severity\n- Notify stakeholders\n\nResolution:\n- Investigate root cause\n- Apply fix\n- Verify solution\n```",
      "diagram": "\ngraph LR\n    Detect[Detect] --> Respond[Respond]\n    Respond --> Resolve[Resolve]\n    Resolve --> Review[Post-Mortem]\n",
      "difficulty": "beginner",
      "tags": [
        "incident",
        "on-call"
      ],
      "lastUpdated": "2025-12-12T09:07:04.188Z"
    },
    "gh-76": {
      "id": "gh-76",
      "question": "What is Infrastructure Monitoring?",
      "answer": "Infrastructure Monitoring is the process of collecting and analyzing data from IT infrastructure components to ensure optimal performance and availabi...",
      "explanation": "Infrastructure Monitoring is the process of collecting and analyzing data from IT infrastructure components to ensure optimal performance and availability.\n\nKey components:\n1. **Metrics Collection:**\n- System metrics\n- Network metrics\n- Application metrics\n\n2. **Analysis:**\n```yaml\nMonitoring Areas:\n- Resource utilization\n- Performance metrics\n- Availability\n- Error rates\n- Response times\n```",
      "difficulty": "beginner",
      "tags": [
        "monitoring",
        "infra"
      ],
      "lastUpdated": "2025-12-12T09:07:04.188Z"
    },
    "gh-77": {
      "id": "gh-77",
      "question": "What are Monitoring Tools?",
      "answer": "Common monitoring tools used in DevOps:",
      "explanation": "Common monitoring tools used in DevOps:\n\n1. **Infrastructure Monitoring:**\n- Prometheus\n- Nagios\n- Zabbix\n- Datadog\n\n2. **Application Monitoring:**\n```yaml\nTools:\n- New Relic\n- AppDynamics\n- Dynatrace\nFeatures:\n- Transaction tracing\n- Error tracking\n- Performance analytics\n```",
      "difficulty": "intermediate",
      "tags": [
        "monitoring",
        "infra"
      ],
      "lastUpdated": "2025-12-12T09:07:04.188Z"
    },
    "gh-78": {
      "id": "gh-78",
      "question": "What are Monitoring Best Practices?",
      "answer": "Monitoring Best Practices are proven methods that enhance the effectiveness of monitoring tools and processes.",
      "explanation": "Monitoring Best Practices are proven methods that enhance the effectiveness of monitoring tools and processes.\n\nKey practices:\n```yaml\nTechnical Practices:\n- Infrastructure as Code\n- Continuous Integration\n- Automated Testing\n- Continuous Deployment\n- Monitoring and Logging\n\nCultural Practices:\n- Shared Responsibility\n- Blameless Post-mortems\n- Knowledge Sharing\n- Continuous Learning\n- Cross-functional Teams\n\nProcess Practices:\n- Agile Methodology\n- Version Control\n- Configuration Management\n- Release Management\n- Incident Management\n```",
      "difficulty": "intermediate",
      "tags": [
        "monitoring",
        "infra"
      ],
      "lastUpdated": "2025-12-12T09:07:04.188Z"
    },
    "gh-79": {
      "id": "gh-79",
      "question": "What is Application Performance Monitoring?",
      "answer": "Application Performance Monitoring (APM) is the practice of collecting and analyzing data about the performance and stability of applications to impro...",
      "explanation": "Application Performance Monitoring (APM) is the practice of collecting and analyzing data about the performance and stability of applications to improve their reliability and responsiveness.\n\nKey components:\n1. **Metrics Collection:**\n- Application metrics\n- Transaction tracing\n- Error tracking\n- Performance analytics\n\n2. **Analysis:**\n```yaml\nMonitoring Areas:\n- Application response times\n- Error rates\n- Resource utilization\n- Scalability\n- Reliability\n```",
      "difficulty": "beginner",
      "tags": [
        "monitoring",
        "infra"
      ],
      "lastUpdated": "2025-12-12T09:07:04.188Z"
    },
    "sr-124": {
      "id": "sr-124",
      "question": "What are the four golden signals of monitoring?",
      "answer": "Latency, Traffic, Errors, and Saturation - key metrics for service health.",
      "explanation": "**The Four Golden Signals**:\n\n1. **Latency**: Time to serve a request\n2. **Traffic**: Demand on your system (requests/sec)\n3. **Errors**: Rate of failed requests\n4. **Saturation**: How full your service is (CPU, memory)\n\nThese signals help identify issues before they become outages.",
      "diagram": "graph TD\n    M[Monitoring] --> L[Latency]\n    M --> T[Traffic]\n    M --> E[Errors]\n    M --> S[Saturation]",
      "difficulty": "beginner",
      "tags": [
        "metrics",
        "monitoring"
      ],
      "lastUpdated": "2025-12-12T09:07:04.188Z"
    },
    "sr-126": {
      "id": "sr-126",
      "question": "What makes a good blameless postmortem?",
      "answer": "Focus on systems and processes, not individuals; identify root causes and actionable improvements.",
      "explanation": "**Blameless Postmortem Elements**:\n- Timeline of events\n- Root cause analysis (5 Whys)\n- What went well\n- What could be improved\n- Action items with owners\n\n**Key Principles**:\n- No finger-pointing\n- Assume good intentions\n- Focus on learning\n- Share widely",
      "diagram": "graph TD\n    Incident[Incident] --> Timeline[Timeline]\n    Timeline --> RCA[Root Cause]\n    RCA --> Actions[Action Items]\n    Actions --> Prevention[Prevention]",
      "difficulty": "advanced",
      "tags": [
        "incident",
        "postmortem"
      ],
      "lastUpdated": "2025-12-12T09:07:04.188Z"
    },
    "sr-130": {
      "id": "sr-130",
      "question": "Your web service has an SLO of 99.9% availability over 30 days. You've had 3 outages: 45 minutes, 20 minutes, and 15 minutes. What's your current availability SLI and are you meeting your SLO?",
      "answer": "SLI: 99.81% availability. Not meeting 99.9% SLO - exceeded error budget by 0.09%.",
      "explanation": "## SLI Calculation\n\n**Total time in 30 days:** 30 × 24 × 60 = 43,200 minutes\n\n**Total downtime:** 45 + 20 + 15 = 80 minutes\n\n**Uptime:** 43,200 - 80 = 43,120 minutes\n\n**SLI (Service Level Indicator):** (43,120 ÷ 43,200) × 100 = **99.81%**\n\n## SLO Analysis\n\n**Target SLO:** 99.9% availability\n**Current SLI:** 99.81%\n**Status:** ❌ **Not meeting SLO**\n\n## Error Budget\n\n**Allowed downtime for 99.9% SLO:** 43,200 × 0.001 = 43.2 minutes\n**Actual downtime:** 80 minutes\n**Error budget exceeded by:** 80 - 43.2 = 36.8 minutes (0.09%)\n\n## Key Concepts\n\n- **SLI (Service Level Indicator):** Actual measured performance metric\n- **SLO (Service Level Objective):** Target reliability goal\n- **Error Budget:** Allowed unreliability (100% - SLO%)\n\nWhen SLI < SLO, you've exceeded your error budget and should focus on reliability improvements over new features.",
      "diagram": "graph TD\n    A[30 Days Total Time<br/>43,200 minutes] --> B[Calculate Downtime]\n    B --> C[Outage 1: 45 min<br/>Outage 2: 20 min<br/>Outage 3: 15 min]\n    C --> D[Total Downtime<br/>80 minutes]\n    A --> E[Calculate Uptime<br/>43,200 - 80 = 43,120 min]\n    E --> F[SLI Calculation<br/>43,120 ÷ 43,200 × 100]\n    F --> G[Current SLI<br/>99.81%]\n    H[SLO Target<br/>99.9%] --> I{SLI ≥ SLO?}\n    G --> I\n    I -->|No| J[❌ SLO Breach<br/>Error Budget Exceeded]\n    I -->|Yes| K[✅ SLO Met<br/>Within Error Budget]\n    L[Error Budget<br/>43.2 minutes allowed] --> M[Budget Exceeded<br/>36.8 minutes over]",
      "difficulty": "intermediate",
      "tags": [
        "slo",
        "sli",
        "error-budget"
      ],
      "lastUpdated": "2025-12-12T09:07:04.188Z"
    },
    "sr-131": {
      "id": "sr-131",
      "question": "You're managing a microservices platform with 50 services. Service A has a 95th percentile latency of 200ms and handles 10,000 RPS. It calls Service B (50ms, 5,000 RPS) and Service C (100ms, 3,000 RPS). During Black Friday, you expect 5x traffic. Service A's CPU utilization is currently 60%, memory at 70%. How do you plan capacity to maintain <500ms 95th percentile end-to-end latency?",
      "answer": "Scale Service A to 15 instances, Service B to 8 instances, Service C to 5 instances. Add circuit breakers, implement caching, and use load shedding.",
      "explanation": "## Capacity Planning Analysis\n\n### Current State Assessment\n- **Service A**: 200ms p95, 10K RPS, 60% CPU, 70% memory\n- **Service B**: 50ms p95, 5K RPS (called by A)\n- **Service C**: 100ms p95, 3K RPS (called by A)\n- **Current end-to-end latency**: ~350ms (200+50+100)\n\n### Black Friday Projections (5x traffic)\n- **Service A**: 50K RPS target\n- **Service B**: 25K RPS target  \n- **Service C**: 15K RPS target\n\n### Capacity Planning Strategy\n\n#### 1. **Horizontal Scaling Calculations**\n```\nService A: 50K RPS ÷ (10K RPS × 0.4 headroom) = ~12.5 → 15 instances\nService B: 25K RPS ÷ 5K RPS = 5 → 8 instances (buffer)\nService C: 15K RPS ÷ 3K RPS = 5 instances\n```\n\n#### 2. **Latency Optimization**\n- **Circuit breakers**: Prevent cascade failures\n- **Caching**: Reduce Service B/C calls by 30-40%\n- **Connection pooling**: Reduce connection overhead\n- **Load shedding**: Drop non-critical requests at 80% capacity\n\n#### 3. **Resource Allocation**\n- **CPU**: Target 40-50% utilization under peak load\n- **Memory**: Target 60% utilization with garbage collection headroom\n- **Network**: Ensure bandwidth can handle 5x throughput\n\n#### 4. **Monitoring & Alerting**\n- Set alerts at 70% capacity utilization\n- Monitor queue depths and connection pool exhaustion\n- Track error rates and implement auto-scaling triggers\n\n#### 5. **Fallback Strategies**\n- **Graceful degradation**: Disable non-essential features\n- **CDN offloading**: Cache static content\n- **Database read replicas**: Distribute read load\n\nThis approach ensures <500ms p95 latency while maintaining system reliability during traffic spikes.",
      "diagram": "graph TD\n    A[Load Balancer] --> B[Service A - 15 instances]\n    B --> C[Service B - 8 instances]\n    B --> D[Service C - 5 instances]\n    B --> E[Cache Layer]\n    F[Circuit Breaker] --> B\n    G[Auto Scaler] --> B\n    G --> C\n    G --> D\n    H[Monitoring] --> I[Alerts]\n    I --> G\n    J[Load Shedder] --> A\n    K[CDN] --> A\n    \n    style B fill:#ff9999\n    style C fill:#99ccff\n    style D fill:#99ff99\n    style E fill:#ffcc99\n    style F fill:#ff6666\n    style J fill:#cc99ff",
      "difficulty": "advanced",
      "tags": [
        "capacity",
        "scaling"
      ],
      "lastUpdated": "2025-12-12T09:07:04.188Z"
    },
    "sr-133": {
      "id": "sr-133",
      "question": "What's the difference between logs, metrics, and traces in observability?",
      "answer": "Logs: events, Metrics: numbers over time, Traces: request journeys.",
      "explanation": "**The Three Pillars of Observability**:\n\n1. **Logs**: Timestamped events showing what happened\n   - Structured logs with key-value pairs\n   - Error messages, debug info\n   - \"User login failed at 2:15 PM\"\n\n2. **Metrics**: Numerical data aggregated over time\n   - CPU usage, request count, error rate\n   - Charts and dashboards\n   - \"95th percentile latency: 200ms\"\n\n3. **Traces**: End-to-end request flow across services\n   - Shows path through distributed systems\n   - Identifies bottlenecks and failures\n   - \"API call took 500ms across 3 services\"\n\n**Why all three?** Logs tell you *what* happened, metrics show *how much*, traces reveal *where*.",
      "diagram": "graph TD\n    A[Request] --> B[Frontend]\n    B --> C[API Gateway]\n    C --> D[Service A]\n    C --> E[Service B]\n    D --> F[Database]\n    \n    G[Logs] --> H[\"Error: DB timeout\"]\n    I[Metrics] --> J[\"CPU: 80%\"]\n    K[Traces] --> L[\"500ms total\"]\n    \n    style G fill:#ff9999\n    style I fill:#99ccff\n    style K fill:#99ff99",
      "difficulty": "beginner",
      "tags": [
        "metrics",
        "monitoring"
      ],
      "lastUpdated": "2025-12-12T09:14:39.919Z"
    },
    "sr-142": {
      "id": "sr-142",
      "question": "You receive a PagerDuty alert at 3 AM: 'Production API is returning 500 errors'. What are your first three steps in handling this incident?",
      "answer": "Acknowledge alert, assess impact, and form response team.",
      "explanation": "**Incident Response First Steps**:\n\n1. **Acknowledge Alert (0-2 mins)**\n   - Accept PagerDuty incident\n   - Set status to 'Investigating'\n   - Prevent escalation\n\n2. **Assess Impact (2-5 mins)**\n   - Check monitoring dashboards\n   - Verify error rates and latency\n   - Determine user impact scope\n\n3. **Form Response Team (5-10 mins)**\n   - Notify on-call engineer\n   - Create incident channel (Slack)\n   - Document initial timeline\n\n**Key Principles**:\n- Stay calm under pressure\n- Communicate clearly\n- Document everything\n- Follow runbook if available",
      "diagram": "graph TD\n    A[PagerDuty Alert] --> B[Acknowledge Incident]\n    B --> C[Assess Impact]\n    C --> D[Check Dashboards]\n    C --> E[Verify Error Rates]\n    C --> F[Determine User Impact]\n    D --> G[Form Response Team]\n    E --> G\n    F --> G\n    G --> H[Notify On-Call]\n    G --> I[Create Slack Channel]\n    G --> J[Document Timeline]",
      "difficulty": "beginner",
      "tags": [
        "incident",
        "postmortem"
      ],
      "lastUpdated": "2025-12-12T10:04:51.668Z"
    },
    "sr-143": {
      "id": "sr-143",
      "question": "Your web application currently handles 1000 requests per minute during peak hours. Each request takes an average of 200ms to process. If you expect traffic to double in the next 6 months, how many additional server instances do you need if each server can handle 50 concurrent requests?",
      "answer": "Need 2 more instances. Current: 1000 req/min ÷ 60s = 16.67 req/s × 0.2s = 3.33 concurrent. Double = 6.67. Need 1 more instance minimum.",
      "explanation": "## Capacity Planning Calculation\n\n**Step 1: Calculate current concurrent requests**\n- Current load: 1000 requests/minute = 16.67 requests/second\n- Processing time: 200ms = 0.2 seconds\n- Concurrent requests = 16.67 × 0.2 = 3.33 concurrent requests\n\n**Step 2: Calculate future requirements**\n- Expected traffic: 2000 requests/minute = 33.33 requests/second\n- Future concurrent requests = 33.33 × 0.2 = 6.67 concurrent requests\n\n**Step 3: Determine server capacity**\n- Each server handles 50 concurrent requests\n- Current servers needed: 3.33 ÷ 50 = 0.067 servers (1 server sufficient)\n- Future servers needed: 6.67 ÷ 50 = 0.133 servers (1 server sufficient)\n\n**However**, for safety margin and avoiding saturation:\n- Add buffer capacity (typically 20-30%)\n- Consider peak spikes beyond average\n- Plan for gradual scaling\n\n**Recommendation**: Add 1-2 additional instances to handle growth safely.",
      "diagram": "graph TD\n    A[Current Load<br/>1000 req/min] --> B[16.67 req/sec]\n    B --> C[3.33 concurrent<br/>requests]\n    D[Future Load<br/>2000 req/min] --> E[33.33 req/sec]\n    E --> F[6.67 concurrent<br/>requests]\n    G[Server Capacity<br/>50 concurrent] --> H{Scaling Decision}\n    C --> H\n    F --> H\n    H --> I[Add 1-2 Instances<br/>for safety margin]",
      "difficulty": "beginner",
      "tags": [
        "capacity",
        "scaling"
      ],
      "lastUpdated": "2025-12-12T10:05:04.369Z"
    },
    "sr-146": {
      "id": "sr-146",
      "question": "Design a chaos engineering experiment to test the resilience of a microservices-based e-commerce platform during a database partition event. How would you ensure the experiment doesn't cause customer data loss while still providing meaningful insights?",
      "answer": "Implement controlled partition with circuit breakers, canary deployments, and data consistency checks to isolate failures.",
      "explanation": "## Chaos Engineering Experiment Design\n\n### **Objective**\nTest system resilience during database partition events while preventing customer data loss.\n\n### **Key Components**\n1. **Blast Radius Control**: Limit impact to non-critical services first\n2. **Data Consistency Validation**: Ensure no data corruption or loss\n3. **Gradual Escalation**: Start with read-only operations, then controlled writes\n\n### **Implementation Steps**\n1. **Preparation Phase**\n   - Identify critical data paths (orders, payments, user data)\n   - Set up monitoring and alerting for data consistency\n   - Create rollback procedures\n\n2. **Experiment Execution**\n   - Inject network partition between primary and replica databases\n   - Monitor service behavior and automatic failover mechanisms\n   - Validate circuit breaker patterns and retry logic\n\n3. **Validation Criteria**\n   - No customer order data loss\n   - Payment processing maintains ACID properties\n   - User sessions remain consistent\n   - System recovers within SLA thresholds\n\n### **Safety Mechanisms**\n- **Canary Deployment**: Test on 1% traffic first\n- **Automated Rollback**: Trigger on data inconsistency detection\n- **Read-Only Mode**: Switch to read-only during critical operations\n- **Data Verification**: Post-experiment consistency checks",
      "diagram": "graph TD\n    A[Client Request] --> B[API Gateway]\n    B --> C[Order Service]\n    B --> D[Payment Service]\n    B --> E[User Service]\n    \n    C --> F[Primary DB]\n    C --> G[Replica DB]\n    D --> H[Payment DB]\n    E --> I[User DB]\n    \n    F -.->|Network Partition| G\n    \n    J[Chaos Controller] --> K[Network Partition]\n    J --> L[Monitoring System]\n    J --> M[Circuit Breaker]\n    \n    L --> N[Data Consistency Check]\n    L --> O[Performance Metrics]\n    \n    M --> P[Failover to Replica]\n    M --> Q[Read-Only Mode]\n    \n    N --> R{Data Valid?}\n    R -->|Yes| S[Continue Experiment]\n    R -->|No| T[Auto Rollback]",
      "difficulty": "advanced",
      "tags": [
        "chaos",
        "resilience"
      ],
      "lastUpdated": "2025-12-12T10:14:55.204Z"
    },
    "sr-147": {
      "id": "sr-147",
      "question": "Your distributed system has 5 microservices with the following failure rates: Service A (0.1%), Service B (0.2%), Service C (0.05%), Service D (0.15%), Service E (0.3%). Each request flows through all services sequentially. If your SLO requires 99.5% success rate and you process 1M requests daily, what's your current system reliability and how would you architect fault tolerance to meet the SLO?",
      "answer": "Current reliability: 99.2%. Need circuit breakers, retries, and service mesh to achieve 99.5% SLO.",
      "explanation": "## System Reliability Analysis\n\n### **Current State Calculation**\n\n**Sequential Service Chain Reliability:**\n- Service A: 99.9% (0.1% failure)\n- Service B: 99.8% (0.2% failure) \n- Service C: 99.95% (0.05% failure)\n- Service D: 99.85% (0.15% failure)\n- Service E: 99.7% (0.3% failure)\n\n**Overall System Reliability:**\n```\n0.999 × 0.998 × 0.9995 × 0.9985 × 0.997 = 0.9920 = 99.2%\n```\n\n**Daily Impact:**\n- 1M requests × 0.8% failure rate = 8,000 failed requests/day\n- **Gap to SLO:** 99.5% - 99.2% = 0.3% (3,000 requests)\n\n### **Fault Tolerance Architecture**\n\n#### **1. Circuit Breaker Pattern**\n- Implement per-service circuit breakers\n- Fast-fail on consecutive failures\n- Automatic recovery testing\n\n#### **2. Retry Strategy**\n- Exponential backoff with jitter\n- Maximum 3 retries per service\n- Timeout-based failure detection\n\n#### **3. Service Mesh Implementation**\n- **Istio/Linkerd** for traffic management\n- Automatic load balancing\n- Health check integration\n\n#### **4. Redundancy & Failover**\n- Deploy multiple instances per service\n- Cross-AZ deployment for availability\n- Database read replicas\n\n#### **5. Graceful Degradation**\n- Non-critical service bypass\n- Cached response fallbacks\n- Feature flags for service isolation\n\n**Expected Improvement:** With proper fault tolerance, achieve 99.6-99.8% reliability, exceeding the 99.5% SLO target.",
      "diagram": "graph TD\n    A[Client Request] --> B[API Gateway]\n    B --> C[Circuit Breaker]\n    C --> D[Service A<br/>99.9%]\n    D --> E[Service B<br/>99.8%]\n    E --> F[Service C<br/>99.95%]\n    F --> G[Service D<br/>99.85%]\n    G --> H[Service E<br/>99.7%]\n    \n    I[Retry Logic] --> C\n    J[Service Mesh] --> D\n    J --> E\n    J --> F\n    J --> G\n    J --> H\n    \n    K[Health Checks] --> L[Load Balancer]\n    L --> M[Service Instances]\n    \n    N[Monitoring] --> O[SLO: 99.5%]\n    N --> P[Current: 99.2%]\n    N --> Q[Target: 99.6%+]\n    \n    style D fill:#ffcccc\n    style E fill:#ffcccc\n    style F fill:#ccffcc\n    style G fill:#ffcccc\n    style H fill:#ffcccc\n    style O fill:#ff6666\n    style P fill:#ffaa66\n    style Q fill:#66ff66",
      "difficulty": "advanced",
      "tags": [
        "reliability",
        "incident"
      ],
      "lastUpdated": "2025-12-12T10:15:16.631Z"
    },
    "sr-148": {
      "id": "sr-148",
      "question": "You're designing capacity planning for a microservices platform handling 100M requests/day with 3x traffic spikes during flash sales. Each service has different resource profiles: API gateway (CPU-bound, 2ms avg), user service (memory-bound, 50ms avg), payment service (I/O-bound, 200ms avg), and inventory service (database-bound, 100ms avg). Design a capacity planning strategy that accounts for cascading failures, auto-scaling lag (2-3 minutes), and maintains 99.9% availability during peak loads.",
      "answer": "Use predictive scaling with 40% headroom, circuit breakers, bulkhead isolation, and pre-warming strategies for known events.",
      "explanation": "## Capacity Planning Strategy\n\n### 1. **Baseline Capacity Analysis**\n- **Normal Load**: 100M requests/day = ~1,157 RPS average\n- **Peak Load**: 3x spike = ~3,471 RPS\n- **Service-specific bottlenecks**: Identify each service's limiting resource\n\n### 2. **Predictive Scaling Approach**\n```\nTarget Capacity = (Peak Load × Safety Factor) + Headroom\nSafety Factor = 1.4 (40% buffer for auto-scaling lag)\nHeadroom = 20% for unexpected spikes\n```\n\n### 3. **Service-Specific Strategies**\n- **API Gateway**: Horizontal scaling with load balancing\n- **User Service**: Memory-optimized instances with connection pooling\n- **Payment Service**: Queue-based processing with circuit breakers\n- **Inventory Service**: Read replicas and caching layers\n\n### 4. **Resilience Patterns**\n- **Circuit Breakers**: Prevent cascading failures\n- **Bulkhead Isolation**: Separate resource pools per service\n- **Graceful Degradation**: Non-critical features fail first\n\n### 5. **Pre-emptive Scaling**\n- **Scheduled Scaling**: Pre-warm for known events\n- **Predictive Metrics**: Use historical data and ML models\n- **Multi-zone Distribution**: Spread load across availability zones\n\n### 6. **Monitoring & Alerting**\n- **Leading Indicators**: Queue depth, response times, error rates\n- **Capacity Metrics**: CPU, memory, network, disk utilization\n- **Business Metrics**: Revenue impact, user experience scores",
      "diagram": "graph TD\n    A[Load Balancer] --> B[API Gateway Cluster]\n    B --> C[User Service Pool]\n    B --> D[Payment Service Pool]\n    B --> E[Inventory Service Pool]\n    \n    C --> F[User DB Replicas]\n    D --> G[Payment Queue]\n    E --> H[Inventory Cache]\n    E --> I[Inventory DB]\n    \n    J[Predictive Scaler] --> K[Metrics Collector]\n    K --> L[Historical Data]\n    K --> M[Real-time Metrics]\n    \n    J --> N[Auto-scaling Groups]\n    N --> B\n    N --> C\n    N --> D\n    N --> E\n    \n    O[Circuit Breaker] --> B\n    O --> C\n    O --> D\n    O --> E\n    \n    P[Monitoring Dashboard] --> K\n    Q[Alerting System] --> K",
      "difficulty": "advanced",
      "tags": [
        "capacity",
        "scaling"
      ],
      "lastUpdated": "2025-12-12T10:15:33.442Z"
    },
    "sr-149": {
      "id": "sr-149",
      "question": "You're designing capacity planning for a microservices platform handling 10M daily active users. Each user generates 50 API calls/day with 80% during peak hours (8am-8pm). Your services have these characteristics: Auth service (5ms avg latency, 95th percentile 15ms), User service (12ms avg, 95th percentile 40ms), Payment service (25ms avg, 95th percentile 80ms). Given a target 99.9% availability and P95 latency under 100ms for end-to-end requests, calculate the required capacity considering: 1) Circuit breaker overhead (5% additional load), 2) Auto-scaling lag (30 seconds), 3) Database connection pooling (max 100 connections per instance), 4) Memory constraints (2GB per service instance). How many instances of each service do you need?",
      "answer": "Auth: 45 instances, User: 52 instances, Payment: 78 instances. Factor in 40% headroom for auto-scaling lag and circuit breaker overhead.",
      "explanation": "## Capacity Planning Calculation\n\n### Traffic Analysis\n- **Daily requests**: 10M users × 50 calls = 500M requests/day\n- **Peak traffic**: 80% in 12 hours = 400M requests in 12h\n- **Peak RPS**: 400M ÷ (12 × 3600) = ~9,259 RPS\n\n### Service-Specific Calculations\n\n#### Auth Service\n- **Target**: P95 < 15ms (already meeting SLA)\n- **Capacity per instance**: Assuming 200 RPS per instance at P95\n- **Base instances needed**: 9,259 ÷ 200 = 47 instances\n- **With overhead**: 47 × 1.05 (circuit breaker) = 49 instances\n- **Auto-scaling buffer**: 49 × 0.3 = 15 additional\n- **Total**: 49 + 15 = **64 instances**\n\n#### User Service\n- **Current P95**: 40ms (needs optimization)\n- **Capacity per instance**: ~150 RPS to maintain P95 < 40ms\n- **Base instances**: 9,259 ÷ 150 = 62 instances\n- **With overhead**: 62 × 1.35 = **84 instances**\n\n#### Payment Service\n- **Current P95**: 80ms (critical path)\n- **Capacity per instance**: ~100 RPS due to higher latency\n- **Database constraint**: 100 connections ÷ 10 services = 10 connections per instance\n- **Base instances**: 9,259 ÷ 100 = 93 instances\n- **With overhead**: 93 × 1.35 = **126 instances**\n\n### Key Considerations\n\n1. **Circuit Breaker Overhead**: 5% additional load when services are degraded\n2. **Auto-scaling Lag**: 30-second delay requires 30% buffer capacity\n3. **Database Bottleneck**: Connection pool limits may require horizontal scaling\n4. **Memory Constraints**: 2GB per instance limits concurrent request handling\n5. **Cascading Failures**: Payment service latency affects entire request chain\n\n### Optimization Recommendations\n\n- Implement request queuing for payment service\n- Add read replicas for database scaling\n- Use connection multiplexing to optimize database usage\n- Consider async processing for non-critical payment operations",
      "diagram": "graph TD\n    A[Load Balancer<br/>9,259 RPS] --> B[Auth Service<br/>64 instances]\n    A --> C[User Service<br/>84 instances]\n    A --> D[Payment Service<br/>126 instances]\n    \n    B --> E[Auth DB<br/>Connection Pool: 100]\n    C --> F[User DB<br/>Connection Pool: 100]\n    D --> G[Payment DB<br/>Connection Pool: 100]\n    \n    H[Auto Scaler<br/>30s lag] --> B\n    H --> C\n    H --> D\n    \n    I[Circuit Breaker<br/>5% overhead] --> B\n    I --> C\n    I --> D\n    \n    J[Monitoring<br/>P95 < 100ms<br/>99.9% availability] --> A",
      "difficulty": "advanced",
      "tags": [
        "capacity",
        "scaling"
      ],
      "lastUpdated": "2025-12-12T10:15:55.304Z"
    },
    "sr-150": {
      "id": "sr-150",
      "question": "You're implementing chaos engineering for a distributed payment system processing $10M daily transactions. Design a chaos experiment to test resilience against Byzantine failures where 30% of payment validation nodes provide conflicting consensus results. How would you ensure financial accuracy while testing system behavior under adversarial conditions?",
      "answer": "Use shadow consensus with financial reconciliation, gradual fault injection, and real-time audit trails to test Byzantine fault tolerance.",
      "explanation": "## Byzantine Fault Tolerance Chaos Experiment\n\n### **Experiment Objective**\nTest payment system resilience against Byzantine failures where nodes provide conflicting consensus results while maintaining financial integrity.\n\n### **Safety-First Architecture**\n\n#### **1. Shadow Consensus System**\n- **Parallel Processing**: Run chaos experiment on shadow payment network\n- **Real-time Mirroring**: Copy production traffic to test environment\n- **Financial Isolation**: No real money movement during experiment\n- **Consensus Validation**: Compare results between production and chaos systems\n\n#### **2. Byzantine Fault Injection Strategy**\n```\nPhase 1: Single malicious node (10% of validators)\nPhase 2: Multiple conflicting nodes (20% of validators)\nPhase 3: Coordinated Byzantine attack (30% of validators)\n```\n\n#### **3. Financial Accuracy Safeguards**\n- **Cryptographic Audit Trail**: Immutable transaction logs with digital signatures\n- **Multi-signature Validation**: Require 2/3+ honest nodes for transaction approval\n- **Real-time Reconciliation**: Continuous balance verification across all nodes\n- **Rollback Mechanisms**: Automatic transaction reversal on consensus failure\n\n### **Implementation Steps**\n\n#### **Pre-Experiment Validation**\n- Verify all nodes have identical ledger state\n- Establish baseline performance metrics\n- Configure monitoring for consensus divergence\n- Set up automated circuit breakers\n\n#### **Chaos Injection Patterns**\n- **Double-spending Attempts**: Malicious nodes approve conflicting transactions\n- **Timestamp Manipulation**: Nodes report incorrect transaction ordering\n- **Balance Falsification**: Nodes report incorrect account balances\n- **Network Partitioning**: Isolate honest nodes from Byzantine nodes\n\n#### **Monitoring & Detection**\n- **Consensus Metrics**: Track agreement rates across validator nodes\n- **Financial Integrity**: Monitor for balance inconsistencies\n- **Performance Impact**: Measure transaction throughput degradation\n- **Recovery Time**: Track system restoration after fault injection\n\n### **Success Criteria**\n- System maintains financial accuracy despite 30% Byzantine nodes\n- Transaction throughput degrades gracefully (< 50% reduction)\n- Honest nodes detect and isolate malicious behavior within 30 seconds\n- No double-spending or balance corruption occurs\n- System recovers to normal operation within 2 minutes\n\n### **Risk Mitigation**\n- **Immediate Rollback**: Automated experiment termination on financial anomaly\n- **Isolated Environment**: Complete separation from production systems\n- **Continuous Auditing**: Real-time financial reconciliation\n- **Expert Oversight**: Financial engineers monitor experiment execution",
      "diagram": "graph TD\n    A[Production Traffic] --> B[Traffic Mirror]\n    B --> C[Shadow Payment Network]\n    \n    C --> D[Honest Validator 1]\n    C --> E[Honest Validator 2]\n    C --> F[Honest Validator 3]\n    C --> G[Byzantine Node 1]\n    C --> H[Byzantine Node 2]\n    \n    D --> I[Consensus Engine]\n    E --> I\n    F --> I\n    G --> I\n    H --> I\n    \n    I --> J{Consensus Check}\n    J -->|Valid| K[Transaction Approved]\n    J -->|Invalid| L[Transaction Rejected]\n    \n    M[Chaos Controller] --> N[Fault Injection]\n    N --> G\n    N --> H\n    \n    O[Financial Auditor] --> P[Balance Verification]\n    O --> Q[Transaction Integrity]\n    O --> R[Consensus Monitoring]\n    \n    P --> S{Financial Accuracy?}\n    S -->|Yes| T[Continue Experiment]\n    S -->|No| U[Emergency Rollback]\n    \n    V[Monitoring Dashboard] --> W[Consensus Rate]\n    V --> X[Transaction Throughput]\n    V --> Y[Byzantine Detection Time]\n    \n    style G fill:#ff6666\n    style H fill:#ff6666\n    style U fill:#ff0000\n    style S fill:#ffaa00",
      "difficulty": "advanced",
      "tags": [
        "chaos",
        "resilience"
      ],
      "lastUpdated": "2025-12-12T10:16:21.457Z"
    },
    "sr-153": {
      "id": "sr-153",
      "question": "You're implementing chaos engineering for a microservices architecture. Your payment service has a 99.9% SLA. During a chaos experiment, you inject 500ms latency into 20% of requests to the database. The service starts timing out after 1 second. What's the most critical metric to monitor first, and what would indicate the experiment should be stopped immediately?",
      "answer": "Monitor error rate/SLA compliance. Stop if error rate exceeds error budget (0.1%) or cascading failures detected in dependent services.",
      "explanation": "## Critical Metrics Priority\n\n### Primary Metric: Error Rate & SLA Compliance\nWith a 99.9% SLA, you have an **error budget of 0.1%**. This is your guardrail.\n\n### Why This Matters\n- 20% of requests getting 500ms latency + network overhead = likely >1s total\n- These requests will timeout, directly impacting SLA\n- Expected impact: ~20% of traffic affected initially\n\n### Stop Conditions\n1. **Error rate exceeds error budget** (>0.1% over measurement window)\n2. **Cascading failures detected** - dependent services showing increased errors\n3. **Queue buildup** - thread pool exhaustion or connection pool saturation\n4. **Circuit breakers tripping** in upstream services\n\n### Secondary Metrics to Watch\n- P99 latency (should spike but not affect all requests)\n- Active connections/thread pool utilization\n- Retry storm indicators\n- Customer-facing transaction success rate\n\n### Chaos Engineering Best Practices\n- Start with smaller blast radius (5-10% of traffic)\n- Use automated stop conditions\n- Monitor blast radius expansion\n- Have rollback ready within seconds\n\n## Calculation Example\nIf you serve 1000 req/s:\n- 200 req/s affected by latency injection\n- If all timeout: 20% error rate\n- This **exceeds** your 0.1% error budget by 200x\n- **Immediate stop required**\n\nThe experiment design itself may be too aggressive for the given SLA.",
      "diagram": "graph TD\n    A[Chaos Experiment Start] --> B[Inject 500ms Latency<br/>20% of DB Requests]\n    B --> C{Monitor Error Rate}\n    C -->|< 0.1%| D[Continue Experiment]\n    C -->|> 0.1%| E[STOP: SLA Breach]\n    B --> F{Check Cascading Effects}\n    F -->|Isolated| D\n    F -->|Spreading| G[STOP: Blast Radius Growing]\n    D --> H{Monitor Secondary Metrics}\n    H --> I[Thread Pool Usage]\n    H --> J[Circuit Breaker Status]\n    H --> K[Queue Depth]\n    I -->|Saturated| G\n    J -->|Tripped| G\n    K -->|Growing| G\n    E --> L[Rollback Immediately]\n    G --> L\n    D --> M[Complete Experiment<br/>Analyze Results]",
      "difficulty": "intermediate",
      "tags": [
        "chaos",
        "resilience"
      ],
      "lastUpdated": "2025-12-13T01:08:12.121Z"
    },
    "sr-155": {
      "id": "sr-155",
      "question": "What is the difference between metrics, logs, and traces in observability, and when would you use each?",
      "answer": "Metrics: aggregated numbers over time. Logs: discrete events. Traces: request flow across services. Use all three for complete observability.",
      "explanation": "## The Three Pillars of Observability\n\n### Metrics\n**What**: Numerical measurements aggregated over time (CPU usage, request rate, error count)\n**When to use**: \n- Monitoring system health and performance trends\n- Setting up alerts and SLOs\n- Understanding resource utilization\n**Example**: `http_requests_total`, `memory_usage_bytes`\n\n### Logs\n**What**: Discrete event records with timestamps and context\n**When to use**:\n- Debugging specific issues\n- Auditing and compliance\n- Understanding what happened at a specific point in time\n**Example**: `[2025-12-13 10:30:45] ERROR: Database connection failed`\n\n### Traces\n**What**: End-to-end journey of a request through distributed systems\n**When to use**:\n- Identifying bottlenecks in microservices\n- Understanding service dependencies\n- Debugging latency issues across services\n**Example**: A single API call traced through API Gateway → Auth Service → Database\n\n### Why All Three?\nEach pillar answers different questions:\n- **Metrics**: \"Is there a problem?\"\n- **Logs**: \"What happened?\"\n- **Traces**: \"Where is the problem in the request flow?\"\n\nUsing all three together provides complete visibility into system behavior and enables faster incident resolution.",
      "diagram": "graph TD\n    A[Observability] --> B[Metrics]\n    A --> C[Logs]\n    A --> D[Traces]\n    B --> E[Aggregated Numbers]\n    B --> F[Time Series Data]\n    B --> G[Alerts & Dashboards]\n    C --> H[Discrete Events]\n    C --> I[Structured/Unstructured]\n    C --> J[Debugging Context]\n    D --> K[Request Flow]\n    D --> L[Service Dependencies]\n    D --> M[Latency Analysis]\n    E --> N[Example: CPU 75%]\n    H --> O[Example: Error Log]\n    K --> P[Example: API → DB]",
      "difficulty": "beginner",
      "tags": [
        "metrics",
        "monitoring"
      ],
      "lastUpdated": "2025-12-13T01:08:45.260Z"
    },
    "sr-154": {
      "id": "sr-154",
      "question": "Your API serves 10M requests/day with a 99.9% availability SLO and 30-day error budget. After a 4-hour outage affecting 100% of traffic, how many minutes of downtime remain in your error budget?",
      "answer": "39.6 minutes remaining (4 hours consumed from 43.2-minute monthly budget)",
      "explanation": "## Error Budget Calculation\n\n**Given:**\n- SLO: 99.9% availability\n- Error budget: 0.1% (100% - 99.9%)\n- Time window: 30 days\n- Outage: 4 hours (240 minutes)\n\n**Step 1: Calculate total error budget**\n```\n30 days = 30 × 24 × 60 = 43,200 minutes\nError budget = 43,200 × 0.001 = 43.2 minutes\n```\n\n**Step 2: Calculate remaining budget**\n```\nConsumed = 240 minutes (4 hours)\nRemaining = 43.2 - 240 = -196.8 minutes\n```\n\n**Result:** The error budget is **exhausted**. You've exceeded it by 196.8 minutes (4.55×). This means:\n- No more incidents allowed this month\n- Feature releases should be frozen\n- Focus shifts to reliability improvements\n- Need executive approval for risky changes\n\n**Key SRE Principle:** Error budgets balance innovation velocity with reliability. When exhausted, teams prioritize stability over new features until the budget resets.",
      "diagram": "graph TD\n    A[30-Day Period] --> B[Total Minutes: 43,200]\n    B --> C[SLO: 99.9%]\n    C --> D[Error Budget: 0.1%]\n    D --> E[43.2 minutes allowed downtime]\n    E --> F[Outage: 240 minutes]\n    F --> G{Budget Status}\n    G -->|Exceeded by 196.8 min| H[Freeze Features]\n    G -->|Within Budget| I[Continue Normal Ops]\n    H --> J[Focus on Reliability]",
      "difficulty": "intermediate",
      "tags": [
        "slo",
        "sli",
        "error-budget"
      ],
      "lastUpdated": "2025-12-13T01:09:58.799Z"
    },
    "sr-169": {
      "id": "sr-169",
      "question": "Your API service has an SLO of 99.9% availability. If you have 5 incidents this month with downtimes of 10min, 5min, 15min, 8min, and 12min, did you meet your SLO? (Assume 30-day month)",
      "answer": "No. Total downtime: 50min out of 43,200min = 99.88% availability, which violates 99.9% SLO",
      "explanation": "## Understanding SLO vs SLI\n\n**SLI (Service Level Indicator)**: The actual measured metric (e.g., uptime percentage)\n**SLO (Service Level Objective)**: The target threshold for the SLI\n\n## Calculation\n\n1. **Total minutes in 30 days**: 30 × 24 × 60 = 43,200 minutes\n2. **Total downtime**: 10 + 5 + 15 + 8 + 12 = 50 minutes\n3. **Uptime**: 43,200 - 50 = 43,150 minutes\n4. **Availability (SLI)**: (43,150 / 43,200) × 100 = 99.88%\n\n## Result\n\nSince 99.88% < 99.9%, you **did NOT meet** your SLO. You have an **error budget deficit** of 0.02%.\n\n## Error Budget\n\nWith a 99.9% SLO, you have a 0.1% error budget = 43.2 minutes/month. You used 50 minutes, exceeding the budget by 6.8 minutes.",
      "diagram": "graph TD\n    A[Monthly SLO: 99.9%] --> B[Error Budget: 0.1%]\n    B --> C[43.2 minutes allowed downtime]\n    D[Actual Incidents] --> E[50 minutes total downtime]\n    E --> F{Compare}\n    C --> F\n    F --> G[50min > 43.2min]\n    G --> H[SLO Violated ❌]\n    H --> I[Error Budget Exceeded by 6.8min]",
      "difficulty": "beginner",
      "tags": [
        "slo",
        "sli",
        "error-budget"
      ],
      "lastUpdated": "2025-12-14T01:18:30.948Z"
    },
    "sd-1": {
      "id": "sd-1",
      "question": "Can you explain the Load Balancer strategy? When would you use Layer 4 vs Layer 7 load balancing?",
      "answer": "Load Balancing distributes traffic across multiple servers to ensure reliability and scalability.",
      "explanation": "A Load Balancer (LB) acts as a reverse proxy. \n\n**Layer 4 (Transport Layer)**: Distributes based on IP/Port. Fast, low overhead, but no context of content. Good for simple packet distribution.\n\n**Layer 7 (Application Layer)**: Inspects HTTP headers/content. Can route based on URL/cookies (e.g., /api to Service A, /static to Service B). More expensive but smarter.\n\n**Common Algorithms**:\n- **Round Robin**: Sequential.\n- **Least Connections**: Sends to server with fewest active connections.\n- **IP Hash**: Ensures a user always goes to the same server (sticky sessions).",
      "diagram": "graph LR\n    User --> LB[Load Balancer]\n    LB -->|Layer 4| S1[\"Server 1<br/>IP:Port\"]\n    LB -->|Layer 7| S2[\"Server 2<br/>/api\"]\n    style LB fill:#fff,stroke:#000,color:#000",
      "difficulty": "advanced",
      "tags": [
        "infra",
        "scale",
        "networking"
      ],
      "lastUpdated": "2025-12-12T09:07:04.188Z"
    },
    "sd-2": {
      "id": "sd-2",
      "question": "What is Consistent Hashing and why is it critical for distributed caches?",
      "answer": "Consistent Hashing maps keys to a ring of nodes to minimize data movement when scaling.",
      "explanation": "In standard `hash(key) % N`, adding a node changes `N`, causing nearly ALL keys to remap (cache stampede).\n\n**Consistent Hashing** maps both servers and keys to a circle (0-360°). Keys map to the next server clockwise.\n\n**Benefit**: Adding/removing a node only affects the immediate neighbors (k/N keys move), not the whole cluster.\n\nUsed in: DynamoDB, Cassandra, Discord Ringpop.",
      "diagram": "\ngraph TD\n    subgraph Hash Ring\n    N1((Node 1)) --- N2((Node 2))\n    N2 --- N3((Node 3))\n    N3 --- N1\n    end\n    Key[Key K] -.->|Clockwise| N2\n    style N2 fill:#f00,stroke:#fff,color:#fff\n",
      "difficulty": "advanced",
      "tags": [
        "hashing",
        "dist-sys",
        "caching"
      ],
      "lastUpdated": "2025-12-12T09:07:04.188Z"
    },
    "sd-3": {
      "id": "sd-3",
      "question": "Explain the CAP Theorem. Can you really 'choose two'?",
      "answer": "CAP states a distributed store can only provide 2 of 3: Consistency, Availability, Partition Tolerance.",
      "explanation": "**Partition Tolerance (P)** is NOT optional in distributed systems (networks fail). \n\nSo the real choice is **CP vs AP** during a partition:\n\n- **CP (Consistency)**: Return error/timeout if data can't be synced. (e.g., Banking - better to fail than show wrong balance).\n- **AP (Availability)**: Return stale data but keep running. (e.g., Facebook Feed - better to show old posts than nothing).\n\n**PACELC Theorem** extends this: Else (when no partition), choose Latency (L) vs Consistency (C).",
      "diagram": "graph TD\n    CAP[CAP Theorem]\n    CAP --> C[Consistency]\n    CAP --> A[Availability]\n    CAP --> P[Partition Tolerance]\n    Note[Pick 2 of 3]\n    style Note fill:#f59e0b,stroke:#fff,color:#000",
      "difficulty": "advanced",
      "tags": [
        "theory",
        "dist-sys",
        "database"
      ],
      "lastUpdated": "2025-12-12T09:07:04.188Z"
    },
    "sd-4": {
      "id": "sd-4",
      "question": "How do you handle Database Sharding? What are the downsides?",
      "answer": "Sharding splits a large database into smaller, faster, easily managed parts called data shards.",
      "explanation": "**Horizontal Partitioning**: Splitting rows based on a Shard Key (e.g., UserID).\n\n**Downsides/Challenges**:\n1. **Resharding**: Hard to move data when a shard fills up.\n2. **Hotspot Key**: If Justin Bieber is on Shard 1, Shard 1 melts down.\n3. **Joins**: Cross-shard joins are expensive/impossible.\n\n**Mitigation**: Consistent Hashing, Virtual Nodes.",
      "diagram": "\ngraph TD\n    App --> Router\n    Router -->|ID < 100| S1[(Shard 1)]\n    Router -->|ID > 100| S2[(Shard 2)]\n",
      "difficulty": "advanced",
      "tags": [
        "db",
        "scale",
        "architecture"
      ],
      "lastUpdated": "2025-12-12T09:07:04.188Z"
    },
    "sd-5": {
      "id": "sd-5",
      "question": "Design a Rate Limiter. What algorithms would you consider?",
      "answer": "Rate Limiting controls the amount of traffic sent or received by a network interface controller.",
      "explanation": "Prevents DoS attacks and resource starvation.\n\n**Algorithms**:\n1. **Token Bucket**: Tokens added at rate `r`. Request consumes token. Allows bursts.\n2. **Leaky Bucket**: Requests enter queue, processed at constant rate. Smooths traffic.\n3. **Fixed Window**: Count requests in 1s window. Edge case: 2x traffic at window boundary.\n4. **Sliding Window Log**: Precise but expensive (stores timestamps).\n\n**Implementation**: Redis (Lua scripts for atomicity).",
      "diagram": "\ngraph LR\n    Req[Request] --> Check{Buckets Full?}\n    Check -->|No| Process[Process]\n    Check -->|Yes| Drop[429 Too Many Requests]\n",
      "difficulty": "advanced",
      "tags": [
        "security",
        "api",
        "algorithms"
      ],
      "lastUpdated": "2025-12-12T09:07:04.188Z"
    },
    "gh-31": {
      "id": "gh-31",
      "question": "What is Scalability in DevOps?",
      "answer": "Scalability is the capability of a system to handle a growing amount of work by adding resources to the system. There are two types of scaling:",
      "explanation": "Scalability is the capability of a system to handle a growing amount of work by adding resources to the system. There are two types of scaling:\n\n1. **Vertical Scaling (Scale Up):**\n- Adding more power to existing resources\n- Example: Upgrading CPU/RAM\n\n2. **Horizontal Scaling (Scale Out):**\n- Adding more resources\n- Example: Adding more servers",
      "diagram": "\ngraph TD\n    subgraph Vertical\n    S1[Small] --> S2[Large]\n    end\n    subgraph Horizontal\n    H1[Server] --- H2[Server] --- H3[Server]\n    end\n",
      "difficulty": "advanced",
      "tags": [
        "scale",
        "ha"
      ],
      "lastUpdated": "2025-12-12T09:07:04.188Z"
    },
    "gh-32": {
      "id": "gh-32",
      "question": "What is High Availability?",
      "answer": "High Availability (HA) is a characteristic of a system that aims to ensure an agreed level of operational performance, usually uptime, for a higher th...",
      "explanation": "High Availability (HA) is a characteristic of a system that aims to ensure an agreed level of operational performance, usually uptime, for a higher than normal period.\n\nKey components:\n1. **Redundancy:**\n- Multiple instances\n- No single point of failure\n\n2. **Monitoring:**\n- Health checks\n- Automated failover\n\n3. **Load Balancing:**\n- Traffic distribution\n- Resource optimization",
      "diagram": "\ngraph TD\n    LB[Load Balancer] --> S1[Server 1]\n    LB --> S2[Server 2]\n    S1 -.->|Failover| S2\n",
      "difficulty": "advanced",
      "tags": [
        "scale",
        "ha"
      ],
      "lastUpdated": "2025-12-12T09:07:04.188Z"
    },
    "gh-33": {
      "id": "gh-33",
      "question": "What is Load Balancing?",
      "answer": "Load Balancing is the process of distributing network traffic across multiple servers to ensure no single server bears too much demand.",
      "explanation": "Load Balancing is the process of distributing network traffic across multiple servers to ensure no single server bears too much demand.\n\nCommon Load Balancing algorithms:\n1. **Round Robin**\n2. **Least Connections**\n3. **IP Hash**\n4. **Weighted Round Robin**\n5. **Resource-Based**\n\nExample of Nginx Load Balancer configuration:\n```nginx\nhttp {\nupstream backend {\nserver backend1.example.com;\nserver backend2.example.com;\nserver backend3.example.com;\n}\n\nserver {\nlisten 80;\nlocation / {\nproxy_pass http://backend;\n}\n}\n}\n```",
      "difficulty": "advanced",
      "tags": [
        "scale",
        "ha"
      ],
      "lastUpdated": "2025-12-12T09:07:04.188Z"
    },
    "gh-34": {
      "id": "gh-34",
      "question": "What is Auto Scaling?",
      "answer": "Auto Scaling is a feature that automatically adjusts the number of compute resources based on the current demand.",
      "explanation": "Auto Scaling is a feature that automatically adjusts the number of compute resources based on the current demand.\n\nKey concepts:\n1. **Scaling Policies:**\n- Target tracking\n- Step scaling\n- Simple scaling\n\n2. **Metrics:**\n- CPU utilization\n- Memory usage\n- Request count\n- Custom metrics\n\nExample of AWS Auto Scaling configuration:\n```yaml\nAutoScalingGroup:\nMinSize: 1\nMaxSize: 10\nDesiredCapacity: 2\nHealthCheckType: ELB\nHealthCheckGracePeriod: 300\nLaunchTemplate:\nLaunchTemplateId: !Ref LaunchTemplate\nVersion: !GetAtt LaunchTemplate.LatestVersionNumber\n```",
      "diagram": "\ngraph LR\n    Metrics[Metrics] --> ASG[Auto Scaling]\n    ASG -->|Scale Out| Add[Add Instances]\n    ASG -->|Scale In| Remove[Remove Instances]\n",
      "difficulty": "advanced",
      "tags": [
        "scale",
        "ha"
      ],
      "lastUpdated": "2025-12-12T09:07:04.188Z"
    },
    "sy-132": {
      "id": "sy-132",
      "question": "Design a distributed rate limiting system that can handle 1M+ requests per second across multiple data centers while maintaining consistency and low latency. How would you handle burst traffic, different rate limiting algorithms (token bucket, sliding window), and ensure fair distribution across users?",
      "answer": "Use distributed token bucket with Redis Cluster, consistent hashing for user distribution, and local caching with periodic sync for low latency.",
      "explanation": "## Distributed Rate Limiting System Design\n\n### Core Components\n\n**1. Rate Limiting Algorithms**\n- **Token Bucket**: Best for burst handling, allows temporary spikes\n- **Sliding Window**: More accurate but computationally expensive\n- **Fixed Window**: Simple but can cause boundary issues\n\n**2. Architecture Overview**\n- **API Gateway Layer**: First line of defense with local rate limiting\n- **Distributed Cache**: Redis Cluster for shared state across regions\n- **Rate Limit Service**: Dedicated microservice for complex logic\n- **Configuration Service**: Dynamic rule updates without deployment\n\n### Implementation Strategy\n\n**Local + Distributed Hybrid Approach:**\n```\n1. Local cache (99% of requests) - sub-millisecond latency\n2. Periodic sync with distributed store (every 100ms)\n3. Fallback to distributed check for edge cases\n```\n\n**Data Distribution:**\n- Consistent hashing for user → shard mapping\n- Replication factor of 3 for high availability\n- Cross-region replication with eventual consistency\n\n**Handling Scale:**\n- Partition by user ID hash\n- Use Lua scripts in Redis for atomic operations\n- Implement circuit breakers for Redis failures\n- Local rate limiting as fallback\n\n### Advanced Features\n\n**Burst Handling:**\n- Token bucket with configurable burst capacity\n- Adaptive rate limiting based on system load\n- Priority queues for different user tiers\n\n**Fairness & Anti-Gaming:**\n- Per-user quotas with spillover pools\n- Detect and penalize abusive patterns\n- Implement jitter to prevent thundering herd\n\n**Monitoring & Observability:**\n- Real-time metrics on rate limit hits\n- Distributed tracing for debugging\n- Alerting on unusual traffic patterns",
      "diagram": "graph TD\n    A[Client Requests] --> B[Load Balancer]\n    B --> C[API Gateway Cluster]\n    C --> D[Local Rate Limiter]\n    D --> E{Within Local Limit?}\n    E -->|Yes| F[Process Request]\n    E -->|No| G[Check Distributed Store]\n    G --> H[Redis Cluster]\n    H --> I[Rate Limit Service]\n    I --> J{Within Global Limit?}\n    J -->|Yes| K[Update Counters]\n    J -->|No| L[Reject Request]\n    K --> F\n    L --> M[Return 429]\n    \n    N[Config Service] --> O[Rate Limit Rules]\n    O --> C\n    O --> I\n    \n    P[Monitoring] --> Q[Metrics Collection]\n    Q --> R[Alerting]\n    \n    H --> S[Cross-Region Sync]\n    S --> T[Other Data Centers]",
      "difficulty": "advanced",
      "tags": [
        "api",
        "rest"
      ],
      "lastUpdated": "2025-12-12T09:07:04.188Z"
    },
    "sy-137": {
      "id": "sy-137",
      "question": "Design a distributed system that provides exactly-once processing guarantees for event streams with out-of-order delivery and network partitions. How would you handle idempotency, deduplication, and causal consistency across multiple processing nodes?",
      "answer": "Use vector clocks for causal ordering, deterministic IDs for deduplication, and idempotent processors with write-ahead logs.",
      "explanation": "This is an advanced distributed systems design problem that combines several complex concepts:\n\n## Core Challenges\n1. **Exactly-once semantics**: Prevent duplicate processing while ensuring no events are lost\n2. **Out-of-order handling**: Events may arrive in different orders than sent\n3. **Network partitions**: System must remain consistent during partial failures\n4. **Causal consistency**: Maintain logical relationships between related events\n\n## Architecture Components\n\n### 1. Event Ingestion Layer\n- **Vector Clocks**: Attach to each event to track causal relationships\n- **Deterministic Event IDs**: Use content-based hashing + timestamp for deduplication\n- **Partitioning Strategy**: Hash-based sharding with consistent hashing\n\n### 2. Processing Layer\n- **Idempotent Processors**: Design state changes to be repeatable\n- **Write-Ahead Logs**: Record intent before execution for recovery\n- **Checkpointing**: Periodic state snapshots for fault tolerance\n\n### 3. Coordination Layer\n- **Raft Consensus**: For metadata and configuration management\n- **Gossip Protocol**: Disseminate vector clock updates\n- **Anti-entropy Mechanisms**: Detect and repair inconsistencies\n\n### 4. Storage Layer\n- **Multi-version Concurrency Control (MVCC)**: Handle concurrent access\n- **Compaction**: Remove obsolete versions while preserving causality\n- **Replication**: Quorum-based writes with read repair\n\n## Key Algorithms\n\n### Deduplication Strategy\n```python\ndef is_duplicate(event_id, processed_events):\n    if event_id in processed_events:\n        return True\n    # Check bloom filter for quick negative lookup\n    if bloom_filter.might_contain(event_id):\n        # Verify in persistent storage\n        return storage.contains(event_id)\n    return False\n```\n\n### Causal Ordering\n- Compare vector clocks to determine event ordering\n- Buffer events until causal dependencies are satisfied\n- Use topological sorting for dependency resolution\n\n### Failure Recovery\n- Replay from last checkpoint using write-ahead logs\n- Rebuild vector clock state from persistent storage\n- Coordinate with other nodes for consistency verification\n\n## Trade-offs\n- **Latency vs Consistency**: Vector clocks add overhead but ensure correctness\n- **Storage vs Performance**: MVCC increases storage but enables concurrency\n- **Complexity vs Reliability**: Sophisticated coordination improves fault tolerance\n\nThis design demonstrates mastery of distributed systems concepts including consensus algorithms, causal consistency, fault tolerance, and exactly-once processing semantics.",
      "diagram": "graph TD\n    A[Client] --> B[Event Ingestion]\n    B --> C[Vector Clock Attachment]\n    C --> D[Deterministic ID Generation]\n    D --> E[Partition Router]\n    E --> F[Processing Node 1]\n    E --> G[Processing Node 2]\n    E --> H[Processing Node N]\n    F --> I[Idempotent Processor]\n    G --> J[Idempotent Processor]\n    H --> K[Idempotent Processor]\n    I --> L[Write-Ahead Log]\n    J --> M[Write-Ahead Log]\n    K --> N[Write-Ahead Log]\n    L --> O[MVCC Storage]\n    M --> O\n    N --> O\n    O --> P[Replication Layer]\n    P --> Q[Raft Consensus Group]\n    F --> R[Gossip Protocol]\n    G --> R\n    H --> R\n    R --> S[Vector Clock Sync]\n    Q --> T[Configuration Manager]\n    S --> U[Anti-entropy Repair]",
      "difficulty": "advanced",
      "tags": [
        "dist-sys",
        "architecture"
      ],
      "lastUpdated": "2025-12-12T09:36:23.632Z"
    },
    "sy-138": {
      "id": "sy-138",
      "question": "Design a distributed rate limiting system that can handle 10M requests per minute across 100+ microservices with different rate limit policies per service and API key.",
      "answer": "Use token bucket algorithm with Redis cluster, local caching, and hierarchical rate limiting (global + per-service + per-key).",
      "explanation": "# Distributed Rate Limiting System Design\n\n## Core Requirements\n- 10M requests/minute throughput\n- Multiple rate limit policies per service\n- Per-API key limits\n- 99.9% availability\n- Sub-10ms latency\n\n## Architecture Components\n\n### 1. Rate Limiting Engine\n- **Token Bucket Algorithm**: Flexible burst handling\n- **Sliding Window**: Time-based accuracy\n- **Policy Engine**: Dynamic rule evaluation\n\n### 2. Storage Layer\n- **Redis Cluster**: Primary counter storage\n- **Local Cache**: LRU for frequently accessed keys\n- **Persistent Storage**: PostgreSQL for policy configuration\n\n### 3. Distribution Strategy\n- **Consistent Hashing**: Even key distribution\n- **Replication**: Multi-master Redis setup\n- **Sharding**: Key-based partitioning\n\n## Key Design Patterns\n\n### Hierarchical Rate Limiting\n1. **Global Limits**: Platform-wide protection\n2. **Service Limits**: Per-microservice constraints\n3. **API Key Limits**: User-specific restrictions\n\n### Performance Optimizations\n- **Batch Processing**: Redis MGET/MSET operations\n- **Async Updates**: Fire-and-forget counter increments\n- **Pre-aggregation**: Local batching before sync\n\n### Failure Handling\n- **Graceful Degradation**: Fallback to local-only limiting\n- **Circuit Breaker**: Fail-fast for Redis outages\n- **Rate Limit Escalation**: Progressive restriction\n\n## Implementation Considerations\n\n### Synchronization\n- **Atomic Operations**: Redis INCR with expiration\n- **Clock Drift**: NTP synchronization\n- **Consistent Window**: Aligned time boundaries\n\n### Scalability\n- **Horizontal Scaling**: Add Redis nodes\n- **Geographic Distribution**: Edge caching\n- **Load Distribution**: Smart client routing\n\n### Monitoring & Observability\n- **Real-time Metrics**: Prometheus integration\n- **Alerting**: Rate limit breach detection\n- **Audit Trail**: Policy change tracking",
      "diagram": "graph TD\n    A[Client Request] --> B[API Gateway]\n    B --> C[Rate Limiter Service]\n    C --> D{Local Cache Check}\n    D -->|Hit| E[Return Decision]\n    D -->|Miss| F[Redis Cluster]\n    F --> G[Policy Engine]\n    G --> H[Rate Limit Algorithm]\n    H --> I[Update Local Cache]\n    I --> E\n    E --> J{Allow?}\n    J -->|Yes| K[Forward to Service]\n    J -->|No| L[Return 429]\n    \n    subgraph \"Redis Cluster\"\n        F1[Shard 1]\n        F2[Shard 2]\n        F3[Shard N]\n    end\n    \n    subgraph \"Policy Store\"\n        G1[Global Policies]\n        G2[Service Policies]\n        G3[API Key Policies]\n    end\n    \n    G --> G1\n    G --> G2\n    G --> G3",
      "difficulty": "advanced",
      "tags": [
        "api",
        "rest"
      ],
      "lastUpdated": "2025-12-12T09:36:34.140Z"
    },
    "sy-139": {
      "id": "sy-139",
      "question": "Design a rate limiting system for a multi-tenant API that supports burst capacity with token bucket algorithm, distributed coordination, and dynamic tenant-specific policies",
      "answer": "Distributed token bucket with Redis-backed state, tenant-specific rate configs, and local caches for performance",
      "explanation": "This system needs to handle multiple tenants with different rate limits, support burst capacity through token buckets, ensure coordination across multiple API instances, and provide dynamic policy updates.\n\nKey components:\n- **Token Bucket Algorithm**: Each tenant has a bucket with capacity (burst) and refill rate. Tokens are added at configured rate, requests consume tokens.\n- **Distributed State**: Redis stores current token counts and last refill timestamps for each tenant, ensuring consistency across instances.\n- **Local Caching**: Each API instance maintains a local cache of token counts with TTL to reduce Redis calls, using optimistic updates.\n- **Policy Management**: Dynamic rate limit configuration stored in database, propagated through Redis pub/sub.\n- **Fallback**: When Redis unavailable, fall back to local bucket with reduced capacity to prevent service interruption.\n\nPerformance considerations:\n- Pipeline Redis operations for batch updates\n- Use Lua scripts for atomic token operations\n- Implement write-back caching for burst scenarios\n- Separate hot path (rate check) from policy updates\n\nChallenges addressed:\n- Race conditions in distributed environment\n- Clock synchronization issues\n- Graceful degradation during failures\n- Tenant isolation and fairness",
      "diagram": "graph TD\n    A[Client Request] --> B{Rate Limiter Service}\n    B --> C[Local Cache Check]\n    C -->|Hit| D[Consume Local Tokens]\n    C -->|Miss| E[Redis Token Check]\n    E -->|Available| F[Consume Redis Tokens]\n    E -->|Limited| G[Reject - 429]\n    D --> H[Update Local Cache Async]\n    F --> I[Update Local Cache]\n    H --> J[Forward to API]\n    I --> J\n    G --> K[Return Rate Limit Headers]\n    J --> L[Response]\n    M[Policy Manager] --> N[Redis Pub/Sub]\n    N --> O[Update Local Config]\n    O --> B",
      "difficulty": "advanced",
      "tags": [
        "api",
        "rest"
      ],
      "lastUpdated": "2025-12-12T09:36:49.367Z"
    },
    "sy-140": {
      "id": "sy-140",
      "question": "Design a rate limiting service that can handle 10 million requests per second with distributed consistency across multiple data centers. The service should support multiple rate limiting strategies (token bucket, sliding window, fixed window) and provide sub-millisecond latency. How would you architect this to handle bursts, prevent thundering herd problems, and ensure accurate global rate limits?",
      "answer": "Use Redis Cluster with Consistent Hashing + Local Caching + Adaptive Rate Limiting with Hierarchical Rate Limiting (user → API → global).",
      "explanation": "## Architecture Overview\n\n**Core Components:**\n1. **Rate Limiting Engine** - Pluggable strategy pattern supporting token bucket, sliding window, and fixed window algorithms\n2. **Distributed Cache Layer** - Redis Cluster with consistent hashing for horizontal scaling\n3. **Local Cache Tier** - L1 cache with write-through to reduce Redis load\n4. **Configuration Service** - Dynamic rule management with hot-reloading\n5. **Metrics & Analytics** - Real-time monitoring and alerting\n\n**Key Design Decisions:**\n\n### 1. Hierarchical Rate Limiting\n- **User Level**: Per-user quotas (e.g., 1000 req/min)\n- **API Level**: Per-endpoint limits (e.g., 100 req/min)\n- **Global Level**: System-wide protection (e.g., 10M req/s)\n\n### 2. Multi-Level Caching Strategy\n- **L1 Cache**: In-memory with 1-second TTL for 90% of requests\n- **L2 Cache**: Redis Cluster with consistent hashing\n- **Write-through**: Updates propagate to both levels\n\n### 3. Burst Handling\n- **Token Bucket**: Allows controlled bursts\n- **Credit System**: Accumulates unused capacity\n- **Priority Queues**: VIP users get preferential treatment\n\n### 4. Thundering Herd Prevention\n- **Request Coalescing**: Batch requests for same key\n- **Exponential Backoff**: Adaptive retry with jitter\n- **Circuit Breakers**: Fail-fast during overload\n\n### 5. Global Consistency\n- **Vector Clocks**: Resolve conflicts across data centers\n- **Gossip Protocol**: Sync rate limit state\n- **Eventual Consistency**: Acceptable for rate limiting\n\n### 6. Performance Optimizations\n- **Connection Pooling**: Reuse Redis connections\n- **Pipelining**: Batch Redis operations\n- **Compression**: Reduce network overhead\n- **Async Processing**: Non-blocking I/O\n\n### 7. Monitoring & Alerting\n- **Real-time Dashboards**: Rate limit utilization\n- **Anomaly Detection**: Unusual traffic patterns\n- **Auto-scaling**: Dynamic cluster sizing\n\n## Implementation Considerations\n\n**Data Model:**\n- Key: `rate_limit:{user_id}:{api_id}:{window}`\n- Value: `{count, last_reset, credits}`\n- TTL: Window duration + safety margin\n\n**Failure Modes:**\n- Redis unavailable: Fall back to local limits\n- Network partition: Permissive mode with logging\n- Cache stampede: Request deduplication\n\n**Scalability:**\n- Horizontal scaling with Redis Cluster\n- Geographic distribution with edge caching\n- Load balancing with consistent hashing",
      "diagram": "graph TD\n    A[Client Request] --> B[Load Balancer]\n    B --> C[Rate Limiting Service]\n    \n    C --> D{Local Cache Check}\n    D -->|Hit| E[Allow/Deny]\n    D -->|Miss| F[Distributed Cache]\n    \n    F --> G{Redis Cluster}\n    G --> H[Shard 1]\n    G --> I[Shard 2]\n    G --> J[Shard N]\n    \n    C --> K[Rate Limiting Engine]\n    K --> L[Token Bucket]\n    K --> M[Sliding Window]\n    K --> N[Fixed Window]\n    \n    C --> O[Configuration Service]\n    O --> P[Rate Limit Rules]\n    O --> Q[User Quotas]\n    \n    C --> R[Analytics Engine]\n    R --> S[Metrics Dashboard]\n    R --> T[Alert System]\n    \n    U[Data Center 1] --> G\n    V[Data Center 2] --> G\n    W[Data Center N] --> G\n    \n    G --> X[Gossip Protocol]\n    X --> Y[State Synchronization]\n    \n    style C fill:#e1f5fe\n    style G fill:#f3e5f5\n    style K fill:#e8f5e8",
      "difficulty": "advanced",
      "tags": [
        "api",
        "rest"
      ],
      "lastUpdated": "2025-12-12T09:37:42.396Z"
    },
    "sy-141": {
      "id": "sy-141",
      "question": "Design a globally distributed serverless platform for real-time collaborative document editing with offline support and conflict resolution. How would you handle data consistency, versioning, and low-latency synchronization across AWS regions while maintaining sub-50ms response times?",
      "answer": "Use CRDTs for conflict resolution, WebSocket for real-time sync, edge locations for caching, DynamoDB Global Tables with multi-region replication.",
      "explanation": "## Architecture Overview\n\n### Data Model & Consistency\n- **CRDT-based Operational Transformation**: Each client tracks operations using Conflict-Free Replicated Data Types\n- **Document State Partitioning**: Split documents into chunks by section/paragraph for parallel processing\n- **Version Vectors**: Lamport timestamps for causality tracking across regions\n\n### Multi-Region Strategy\n- **Active-Active Regions**: Deploy Lambda functions in us-east-1, eu-west-1, ap-southeast-1\n- **DynamoDB Global Tables**: Multi-master replication with conflict-free write patterns\n- **CloudFront Edge Locations**: Cache hot documents with WebSocket support\n\n### Real-time Synchronization\n- **WebSocket Connections**: API Gateway with WebSocket protocol for bidirectional communication\n- **EventBridge**: Cross-region event propagation for coordination\n- **Local Caching**: ElastiCache Redis in each region for hot document state\n\n### Offline Support\n- **Service Worker**: IndexedDB for local storage and operation queuing\n- **Delta Synchronization**: Only transmit changes, not full document state\n- **Conflict Resolution**: CRDT automatically resolves merge conflicts when reconnecting\n\n### Performance Optimizations\n- **Edge Computing**: CloudFront Functions for document diff calculation at edge\n- **Connection Pooling**: WebSocket multiplexing to reduce connection overhead\n- **Smart Routing**: Route 53 latency-based routing to nearest region\n\n### Monitoring & Scaling\n- **Auto Scaling**: Lambda provisioned concurrency for predictable performance\n- **Real-time Metrics**: CloudWatch custom metrics for collaboration metrics\n- **Circuit Breakers**: Regional isolation to prevent cascade failures",
      "diagram": "graph TD\n    A[Client Browser] -->|WebSocket| B[CloudFront Edge]\n    B -->|WebSocket| C[API Gateway WebSocket]\n    C --> D[Lambda Auth]\n    C --> E[Lambda Router]\n    E --> F[Lambda Document Handler]\n    E --> G[Lambda Sync Handler]\n    F --> H[DynamoDB Global Table]\n    G --> I[EventBridge Bus]\n    I --> J[Cross-Region EventBridge]\n    J --> K[Other Region Lambda]\n    F --> L[ElastiCache Redis]\n    K --> M[DynamoDB Replica]\n    A --> N[Service Worker]\n    N --> O[IndexedDB Storage]\n    P[Route 53] -->|Latency Routing| B\n    Q[CloudWatch] -->|Metrics| E",
      "difficulty": "advanced",
      "tags": [
        "infra",
        "scale"
      ],
      "lastUpdated": "2025-12-12T09:38:09.154Z"
    },
    "sy-144": {
      "id": "sy-144",
      "question": "Design a distributed rate limiting system that can handle 1M+ requests per second across multiple data centers while maintaining consistency and preventing thundering herd problems during cache misses.",
      "answer": "Use sliding window counters with Redis Cluster, consistent hashing, and circuit breakers with jittered backoff for cache miss protection.",
      "explanation": "## Architecture Components\n\n### 1. Distributed Counter Storage\n- **Redis Cluster** with consistent hashing for horizontal scaling\n- **Sliding window counters** using sorted sets with timestamps\n- **Multi-level caching**: L1 (local), L2 (regional), L3 (global)\n\n### 2. Rate Limiting Algorithm\n```\nkey = user_id:window_start_time\ncount = ZCOUNT key (now-window_size) now\nif count < limit:\n  ZADD key now unique_request_id\n  EXPIRE key window_size\n  return ALLOW\nelse:\n  return DENY\n```\n\n### 3. Consistency Strategy\n- **Eventually consistent** across regions (acceptable for rate limiting)\n- **Strong consistency** within data center using Redis transactions\n- **Conflict resolution**: Last-writer-wins with vector clocks\n\n### 4. Thundering Herd Prevention\n- **Circuit breaker pattern** with exponential backoff\n- **Jittered cache refresh** (random 10-30% of TTL)\n- **Probabilistic early expiration** to spread load\n- **Request coalescing** for identical cache misses\n\n### 5. High Availability\n- **Multi-master Redis setup** with cross-region replication\n- **Graceful degradation**: Allow requests when cache unavailable\n- **Health checks** and automatic failover\n- **Rate limit approximation** during partial failures",
      "diagram": "graph TD\n    A[Client Request] --> B[Load Balancer]\n    B --> C[Rate Limiter Service]\n    C --> D{Local Cache Hit?}\n    D -->|Yes| E[Check Counter]\n    D -->|No| F[Circuit Breaker]\n    F -->|Open| G[Allow Request]\n    F -->|Closed| H[Redis Cluster]\n    H --> I[Sliding Window Counter]\n    I --> J{Under Limit?}\n    J -->|Yes| K[Increment Counter]\n    J -->|No| L[Reject Request]\n    K --> M[Update Local Cache]\n    M --> N[Forward Request]\n    E --> J\n    \n    subgraph Redis Cluster\n        H1[Redis Master 1]\n        H2[Redis Master 2]\n        H3[Redis Master 3]\n        H1 -.-> H2\n        H2 -.-> H3\n        H3 -.-> H1\n    end\n    \n    subgraph Multi-DC\n        DC1[Data Center 1]\n        DC2[Data Center 2]\n        DC1 -.->|Async Replication| DC2\n    end",
      "difficulty": "advanced",
      "tags": [
        "dist-sys",
        "architecture"
      ],
      "lastUpdated": "2025-12-12T10:05:21.929Z"
    },
    "sy-151": {
      "id": "sy-151",
      "question": "Design a rate limiting API for a multi-tenant SaaS platform where different customers have different rate limits (free: 100 req/hour, premium: 1000 req/hour, enterprise: custom). How would you design the API endpoints and data structures to efficiently track and enforce these limits?",
      "answer": "Use token bucket algorithm with Redis, API key middleware, and tiered limit configs stored in DB with in-memory cache for fast lookups.",
      "explanation": "## Rate Limiting API Design\n\n### Core Components\n\n**1. API Structure**\n```\nPOST /api/v1/ratelimit/check\nGET /api/v1/ratelimit/status/:apiKey\nPOST /api/v1/ratelimit/reset/:apiKey (admin)\n```\n\n**2. Data Structures**\n- **Redis**: Store token buckets with TTL\n  - Key: `ratelimit:{tenant_id}:{window}`\n  - Value: `{tokens_remaining, last_refill_time}`\n- **Database**: Tenant configurations\n  - `tenants` table: `{id, tier, custom_limit, window_seconds}`\n- **In-Memory Cache**: Hot tenant limits (LRU cache)\n\n**3. Token Bucket Algorithm**\n- Each request consumes 1 token\n- Tokens refill at configured rate\n- Bucket capacity = tier limit\n- Use Redis INCR/DECR for atomic operations\n\n**4. Implementation Flow**\n1. Extract API key from request header\n2. Check in-memory cache for tenant tier\n3. If miss, query DB and cache result\n4. Check Redis for current token count\n5. If tokens available: decrement and allow\n6. If depleted: return 429 with Retry-After header\n\n**5. Response Headers**\n```\nX-RateLimit-Limit: 1000\nX-RateLimit-Remaining: 847\nX-RateLimit-Reset: 1640000000\nRetry-After: 3600 (if rate limited)\n```\n\n**6. Scalability Considerations**\n- Use Redis Cluster for horizontal scaling\n- Implement sliding window for smoother limits\n- Add circuit breaker for Redis failures (fail open)\n- Use distributed rate limiting for multi-region\n\n**7. Edge Cases**\n- Burst allowance for enterprise customers\n- Grace period for tier upgrades\n- Rate limit exemptions for health checks",
      "diagram": "graph TD\n    A[Client Request] --> B[API Gateway]\n    B --> C{Extract API Key}\n    C --> D[Check Memory Cache]\n    D --> E{Cache Hit?}\n    E -->|No| F[Query DB for Tier]\n    F --> G[Cache Tier Config]\n    G --> H[Check Redis Token Bucket]\n    E -->|Yes| H\n    H --> I{Tokens Available?}\n    I -->|Yes| J[Decrement Token]\n    J --> K[Allow Request]\n    K --> L[Add Rate Limit Headers]\n    I -->|No| M[Return 429 Too Many Requests]\n    M --> N[Add Retry-After Header]\n    L --> O[Forward to Backend]\n    \n    subgraph Redis\n    H\n    J\n    end\n    \n    subgraph Database\n    F\n    end\n    \n    subgraph Memory Cache\n    D\n    G\n    end",
      "difficulty": "intermediate",
      "tags": [
        "api",
        "rest"
      ],
      "lastUpdated": "2025-12-13T01:07:37.875Z"
    },
    "sy-158": {
      "id": "sy-158",
      "question": "Design a distributed rate limiter that can handle 1M requests/second across 100 data centers with <10ms latency. How do you ensure accurate rate limiting while avoiding coordination overhead?",
      "answer": "Use local counters with async gossip protocol, sliding window algorithm, and token bucket with eventual consistency guarantees.",
      "explanation": "## Solution Architecture\n\n### Key Components\n\n1. **Local Rate Limiters**: Each node maintains local counters using sliding window or token bucket algorithms\n2. **Gossip Protocol**: Nodes periodically exchange counter information to achieve eventual consistency\n3. **Hybrid Approach**: Combine local decisions with periodic synchronization\n\n### Design Choices\n\n**Local Token Buckets**\n- Each node gets quota allocation (total_limit / num_nodes)\n- Tokens refill at configured rate\n- Fast local decisions (<1ms)\n- Trade-off: May allow brief bursts above global limit\n\n**Sliding Window Counters**\n- Track requests in time windows (e.g., 1-second buckets)\n- Use Redis sorted sets or in-memory structures\n- Weighted counting for window boundaries\n\n**Gossip Synchronization**\n- Nodes exchange counter deltas every 100-500ms\n- Epidemic broadcast ensures eventual consistency\n- Adjust local quotas based on cluster-wide usage\n\n### Handling Edge Cases\n\n**Hot Partitions**: Use consistent hashing with virtual nodes to distribute load\n\n**Network Partitions**: Implement conservative limits during splits (fail-safe mode)\n\n**Burst Traffic**: Pre-allocate burst capacity (e.g., 120% of steady-state limit)\n\n### Accuracy vs Performance Trade-offs\n\n- **Strict Accuracy**: Use centralized Redis with Lua scripts (higher latency ~50ms)\n- **High Performance**: Local-only decisions (may exceed limit by 5-10%)\n- **Balanced**: Gossip-based with 1-2% overage tolerance\n\n### Implementation Details\n\n```\nAlgorithm: Hybrid Rate Limiter\n1. Check local token bucket\n2. If tokens available, allow immediately\n3. If near limit, check gossip state\n4. Background: sync counters every 200ms\n5. Adjust local quota based on cluster load\n```\n\n### Scalability Considerations\n\n- **Horizontal Scaling**: Add nodes dynamically, redistribute quotas\n- **Geographic Distribution**: Regional rate limiters with cross-region aggregation\n- **Storage**: Use in-memory stores (Redis, Memcached) with TTL-based cleanup",
      "diagram": "graph TD\n    A[Client Request] --> B[Load Balancer]\n    B --> C[Node 1: Local Rate Limiter]\n    B --> D[Node 2: Local Rate Limiter]\n    B --> E[Node 3: Local Rate Limiter]\n    \n    C --> F[Local Token Bucket]\n    D --> G[Local Token Bucket]\n    E --> H[Local Token Bucket]\n    \n    F -.Gossip Sync.-> G\n    G -.Gossip Sync.-> H\n    H -.Gossip Sync.-> F\n    \n    C --> I[Redis Cache]\n    D --> I\n    E --> I\n    \n    I --> J[Sliding Window Counters]\n    \n    F --> K{Tokens Available?}\n    K -->|Yes| L[Allow Request]\n    K -->|No| M[Reject: 429]\n    \n    N[Gossip Manager] --> F\n    N --> G\n    N --> H\n    \n    O[Quota Adjuster] --> N\n    I -.Periodic Sync.-> O",
      "difficulty": "advanced",
      "tags": [
        "dist-sys",
        "architecture"
      ],
      "lastUpdated": "2025-12-13T01:09:32.003Z"
    },
    "sy-169": {
      "id": "sy-169",
      "question": "Design a simple URL shortening service like bit.ly. What components would you need and how would they work together?",
      "answer": "Database for mappings, API server for redirects, web interface for users, cache for performance.",
      "explanation": "# URL Shortening Service Design\n\n## Core Components\n\n1. **API Server**: Handles HTTP requests for creating short URLs and redirects\n2. **Database**: Stores mapping between short codes and original URLs\n3. **Cache Layer**: Redis for frequently accessed URLs to reduce database load\n4. **Web Interface**: Frontend for users to create and manage short URLs\n\n## Data Flow\n\n1. User submits long URL via web interface\n2. API generates unique short code (base62 encoding)\n3. Store mapping in database\n4. Return short URL to user\n5. When short URL accessed, check cache first, then database\n6. Redirect to original URL\n\n## Key Considerations\n\n- Use base62 encoding for compact, readable short codes\n- Implement rate limiting to prevent abuse\n- Add analytics tracking for click counts\n- Consider CDN for global performance",
      "diagram": "graph TD\n    A[User] --> B[Web Interface]\n    B --> C[API Server]\n    C --> D[Database]\n    C --> E[Cache Layer]\n    F[Short URL] --> C\n    C --> G[Redirect to Original URL]\n    E --> C",
      "difficulty": "beginner",
      "tags": [
        "infra",
        "scale"
      ],
      "lastUpdated": "2025-12-14T01:17:10.517Z"
    },
    "sy-171": {
      "id": "sy-171",
      "question": "Design a globally distributed, multi-region database system that provides strong consistency with 99.999% availability while handling 10M QPS and supporting automatic failover within 50ms.",
      "answer": "Use consensus-based replication with quorum writes, geo-distributed nodes, and intelligent routing with local read caches.",
      "explanation": "## Architecture Overview\n\nThis system requires a sophisticated approach to achieve both strong consistency and high availability across multiple geographic regions.\n\n### Core Components\n\n**1. Consensus Layer (Raft/PBFT)**\n- Implement a consensus protocol for leader election and log replication\n- Use majority quorum (n/2 + 1) for strong consistency guarantees\n- Deploy consensus nodes across all regions for fault tolerance\n\n**2. Storage Layer**\n- Partition data using consistent hashing for even distribution\n- Replicate each partition across multiple regions (typically 3-5)\n- Use write-ahead logging (WAL) for durability\n- Implement compaction and garbage collection for log management\n\n**3. Routing Layer**\n- Global load balancer with health checks\n- Intelligent request routing based on proximity and data locality\n- Connection pooling and multiplexing for performance\n\n**4. Caching Layer**\n- Multi-level caching: edge, regional, and global\n- Cache invalidation through pub/sub mechanisms\n- Read-through/write-through patterns for consistency\n\n### Performance Optimizations\n\n**Write Path Optimization**\n- Batch writes to reduce network round trips\n- Parallel replication to multiple regions\n- Optimistic concurrency control with conflict resolution\n\n**Read Path Optimization**\n- Local read replicas for low latency\n- Read-after-write consistency through version vectors\n- Adaptive caching based on access patterns\n\n### Failure Handling\n\n**Automatic Failover Mechanisms**\n- Continuous health monitoring with heartbeat detection\n- Leader election within 50ms using consensus protocol\n- Seamless client redirection during failover\n- Data reconciliation after partition healing\n\n**Disaster Recovery**\n- Point-in-time recovery through periodic snapshots\n- Cross-region backup replication\n- Automated restoration procedures\n\n### Scalability Considerations\n\n**Horizontal Scaling**\n- Dynamic node addition/removal without downtime\n- Automatic data rebalancing using consistent hashing\n- Elastic resource allocation based on load\n\n**Vertical Scaling**\n- Memory optimization for hot data\n- SSD/NVVM storage for performance-critical operations\n- CPU-intensive operations offloaded to specialized nodes\n\n### Monitoring and Observability\n\n**Metrics Collection**\n- Real-time performance monitoring (latency, throughput, error rates)\n- Distributed tracing for request flow analysis\n- Resource utilization tracking across regions\n\n**Alerting and Automation**\n- Proactive alerting for performance degradation\n- Automated scaling based on predefined thresholds\n- Self-healing capabilities for common failure modes",
      "diagram": "graph TD\n    A[Client Request] --> B[Global Load Balancer]\n    B --> C[Regional Router]\n    C --> D[Consensus Leader]\n    D --> E[Write-Ahead Log]\n    E --> F[Replication Manager]\n    F --> G[Region 1 Primary]\n    F --> H[Region 2 Primary]\n    F --> I[Region 3 Primary]\n    G --> J[Region 1 Replicas]\n    H --> K[Region 2 Replicas]\n    I --> L[Region 3 Replicas]\n    C --> M[Read Cache Layer]\n    M --> N[Edge Cache]\n    M --> O[Regional Cache]\n    D --> P[Health Monitor]\n    P --> Q[Failover Controller]\n    Q --> R[Automatic Leader Election]\n    G --> S[Storage Engine]\n    H --> T[Storage Engine]\n    I --> U[Storage Engine]\n    S --> V[Compaction Service]\n    T --> W[Compaction Service]\n    U --> X[Compaction Service]",
      "difficulty": "advanced",
      "tags": [
        "infra",
        "scale"
      ],
      "lastUpdated": "2025-12-14T01:17:45.908Z"
    }
  },
  "lastUpdated": "2025-12-14T10:08:54.380Z"
}