[
  {
    "id": "sre-1",
    "question": "What are SLIs, SLOs, and SLAs? How do they relate?",
    "answer": "Metrics, Goals, and Consequences.",
    "explanation": "1. **SLI (Indicator)**: The actual number. \"My latency is 200ms\".\n2. **SLO (Objective)**: The internal goal. \"Latency should be < 300ms for 99% of requests\".\n3. **SLA (Agreement)**: The legal contract with users. \"If latency > 500ms, we refund you 10%\".\n\n*SREs focus on SLOs. SLAs are for lawyers.*",
    "tags": [
      "metrics",
      "policy",
      "definitions",
      "observability"
    ],
    "difficulty": "beginner",
    "channel": "sre",
    "subChannel": "observability",
    "diagram": "graph TD\n    SLI[\"Indicator<br/>Reality\"] -->|Measured Against| SLO[\"Objective<br/>Goal\"]\n    SLO -->|Buffer| SLA[\"Agreement<br/>Contract\"]",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "sre-2",
    "question": "What is an Error Budget and how do you use it?",
    "answer": "100% - SLO = Error Budget. It's the allowed amount of unreliability.",
    "explanation": "If SLO is 99.9% uptime, you have 0.1% error budget (approx 43 mins/month).\n\n**Usage**:\n- **Budget Remaining?**: Ship features fast, take risks, do chaos engineering.\n- **Budget Exhausted?**: FREEZE deployments. Focus 100% on reliability until budget recovers.\n\n*It aligns Dev (speed) and Ops (stability) incentives.*",
    "tags": [
      "management",
      "concept",
      "risk"
    ],
    "difficulty": "beginner",
    "channel": "sre",
    "subChannel": "reliability",
    "diagram": "\ngraph LR\n    SLO[99.9% SLO] --> EB[0.1% Error Budget]\n    EB -->|Remaining| Ship[Ship Features]\n    EB -->|Exhausted| Freeze[Freeze Deploys]\n",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "gh-59",
    "question": "What is Site Reliability Engineering?",
    "answer": "Site Reliability Engineering (SRE) is a discipline that incorporates aspects of software engineering and applies them to infrastructure and operations...",
    "explanation": "Site Reliability Engineering (SRE) is a discipline that incorporates aspects of software engineering and applies them to infrastructure and operations problems to create scalable and highly reliable software systems.\n\nKey principles:\n1. **Embrace Risk:**\n- Define acceptable risk levels\n- Use error budgets\n- Balance reliability and innovation\n\n2. **Eliminate Toil:**\n- Automate manual tasks\n- Reduce operational overhead\n- Focus on engineering work",
    "tags": [
      "sre",
      "reliability"
    ],
    "difficulty": "beginner",
    "channel": "sre",
    "subChannel": "reliability",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "gh-60",
    "question": "What are Service Level Objectives (SLOs)?",
    "answer": "Service Level Objectives (SLOs) are specific, measurable targets for service performance that you set and agree to meet.",
    "explanation": "Service Level Objectives (SLOs) are specific, measurable targets for service performance that you set and agree to meet.\n\nExample SLO definition:\n```yaml\nService: User Authentication\nSLO:\nMetric: Availability\nTarget: 99.9%\nWindow: 30 days\nMeasurement:\n- Success rate of authentication requests\n- Latency under 300ms for 99% of requests\n```",
    "tags": [
      "sre",
      "reliability"
    ],
    "difficulty": "intermediate",
    "channel": "sre",
    "subChannel": "reliability",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "gh-61",
    "question": "What are Service Level Indicators (SLIs)?",
    "answer": "Service Level Indicators (SLIs) are quantitative measures of service level aspects such as latency, throughput, availability, and error rate.",
    "explanation": "Service Level Indicators (SLIs) are quantitative measures of service level aspects such as latency, throughput, availability, and error rate.\n\nCommon SLIs:\n1. **Request Latency:**\n- Time to handle a request\n- Distribution of response times\n\n2. **Error Rate:**\n- Failed requests/total requests\n- Error budget consumption\n\n3. **System Throughput:**\n- Requests per second\n- Transactions per second",
    "tags": [
      "sre",
      "reliability"
    ],
    "difficulty": "intermediate",
    "channel": "sre",
    "subChannel": "reliability",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "gh-62",
    "question": "What is Error Budget?",
    "answer": "An Error Budget is the maximum amount of time that a technical system can fail without contractual consequences. It's the difference between the SLO t...",
    "explanation": "An Error Budget is the maximum amount of time that a technical system can fail without contractual consequences. It's the difference between the SLO target and 100% reliability.\n\nExample calculation:\n```\nSLO Target: 99.9% uptime\nError Budget: 100% - 99.9% = 0.1%\nMonthly Error Budget: 43.2 minutes (0.1% of 30 days)\n```\n\nKey concepts:\n1. **Budget Calculation:**\n- Based on SLO targets\n- Measured over time windows\n- Reset periodically\n\n2. **Budget Usage:**\n- Track incidents\n- Monitor consumption\n- Alert on budget burn",
    "tags": [
      "sre",
      "reliability"
    ],
    "difficulty": "beginner",
    "channel": "sre",
    "subChannel": "reliability",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "gh-63",
    "question": "What is Toil in SRE?",
    "answer": "Toil is the kind of work tied to running a production service that tends to be manual, repetitive, automatable, tactical, devoid of enduring value, an...",
    "explanation": "Toil is the kind of work tied to running a production service that tends to be manual, repetitive, automatable, tactical, devoid of enduring value, and that scales linearly as a service grows.\n\nCharacteristics of toil:\n1. **Manual work:**\n- No automation\n- Human intervention required\n- Repetitive tasks\n\n2. **Impact:**\n- Reduces time for project work\n- Increases operational overhead\n- Affects team morale\n\n3. **Solutions:**\n\nAutomation:\n- Script repetitive tasks\n- Implement self-service tools\n- Create automated workflows\n\nProcess Improvement:\n- Identify toil sources\n- Set toil budgets\n- Track toil metrics\n\nEngineering Solutions:\n- Design for automation\n- Build self-healing systems\n- Implement proper monitoring",
    "tags": [
      "sre",
      "reliability"
    ],
    "difficulty": "beginner",
    "channel": "sre",
    "subChannel": "reliability",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "gh-73",
    "question": "What is Incident Management?",
    "answer": "Incident Management is the process of responding to and resolving IT service disruptions.",
    "explanation": "Incident Management is the process of responding to and resolving IT service disruptions.\n\nKey components:\n1. **Detection:**\n- Monitoring alerts\n- User reports\n- Automated detection\n\n2. **Response:**\n```yaml\nInitial Response:\n- Acknowledge incident\n- Assess severity\n- Notify stakeholders\n\nResolution:\n- Investigate root cause\n- Apply fix\n- Verify solution\n```",
    "tags": [
      "incident",
      "on-call"
    ],
    "difficulty": "beginner",
    "channel": "sre",
    "subChannel": "incident-management",
    "diagram": "\ngraph LR\n    Detect[Detect] --> Respond[Respond]\n    Respond --> Resolve[Resolve]\n    Resolve --> Review[Post-Mortem]\n",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "gh-76",
    "question": "What is Infrastructure Monitoring?",
    "answer": "Infrastructure Monitoring is the process of collecting and analyzing data from IT infrastructure components to ensure optimal performance and availabi...",
    "explanation": "Infrastructure Monitoring is the process of collecting and analyzing data from IT infrastructure components to ensure optimal performance and availability.\n\nKey components:\n1. **Metrics Collection:**\n- System metrics\n- Network metrics\n- Application metrics\n\n2. **Analysis:**\n```yaml\nMonitoring Areas:\n- Resource utilization\n- Performance metrics\n- Availability\n- Error rates\n- Response times\n```",
    "tags": [
      "monitoring",
      "infra"
    ],
    "difficulty": "beginner",
    "channel": "sre",
    "subChannel": "observability",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "gh-77",
    "question": "What are Monitoring Tools?",
    "answer": "Common monitoring tools used in DevOps:",
    "explanation": "Common monitoring tools used in DevOps:\n\n1. **Infrastructure Monitoring:**\n- Prometheus\n- Nagios\n- Zabbix\n- Datadog\n\n2. **Application Monitoring:**\n```yaml\nTools:\n- New Relic\n- AppDynamics\n- Dynatrace\nFeatures:\n- Transaction tracing\n- Error tracking\n- Performance analytics\n```",
    "tags": [
      "monitoring",
      "infra"
    ],
    "difficulty": "intermediate",
    "channel": "sre",
    "subChannel": "observability",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "gh-78",
    "question": "What are Monitoring Best Practices?",
    "answer": "Monitoring Best Practices are proven methods that enhance the effectiveness of monitoring tools and processes.",
    "explanation": "Monitoring Best Practices are proven methods that enhance the effectiveness of monitoring tools and processes.\n\nKey practices:\n```yaml\nTechnical Practices:\n- Infrastructure as Code\n- Continuous Integration\n- Automated Testing\n- Continuous Deployment\n- Monitoring and Logging\n\nCultural Practices:\n- Shared Responsibility\n- Blameless Post-mortems\n- Knowledge Sharing\n- Continuous Learning\n- Cross-functional Teams\n\nProcess Practices:\n- Agile Methodology\n- Version Control\n- Configuration Management\n- Release Management\n- Incident Management\n```",
    "tags": [
      "monitoring",
      "infra"
    ],
    "difficulty": "intermediate",
    "channel": "sre",
    "subChannel": "observability",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "gh-79",
    "question": "What is Application Performance Monitoring?",
    "answer": "Application Performance Monitoring (APM) is the practice of collecting and analyzing data about the performance and stability of applications to impro...",
    "explanation": "Application Performance Monitoring (APM) is the practice of collecting and analyzing data about the performance and stability of applications to improve their reliability and responsiveness.\n\nKey components:\n1. **Metrics Collection:**\n- Application metrics\n- Transaction tracing\n- Error tracking\n- Performance analytics\n\n2. **Analysis:**\n```yaml\nMonitoring Areas:\n- Application response times\n- Error rates\n- Resource utilization\n- Scalability\n- Reliability\n```",
    "tags": [
      "monitoring",
      "infra"
    ],
    "difficulty": "beginner",
    "channel": "sre",
    "subChannel": "observability",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "sr-124",
    "question": "What are the four golden signals of monitoring?",
    "answer": "Latency, Traffic, Errors, and Saturation - key metrics for service health.",
    "explanation": "**The Four Golden Signals**:\n\n1. **Latency**: Time to serve a request\n2. **Traffic**: Demand on your system (requests/sec)\n3. **Errors**: Rate of failed requests\n4. **Saturation**: How full your service is (CPU, memory)\n\nThese signals help identify issues before they become outages.",
    "tags": [
      "metrics",
      "monitoring"
    ],
    "difficulty": "beginner",
    "channel": "sre",
    "subChannel": "observability",
    "diagram": "graph TD\n    M[Monitoring] --> L[Latency]\n    M --> T[Traffic]\n    M --> E[Errors]\n    M --> S[Saturation]",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "sr-126",
    "question": "What makes a good blameless postmortem?",
    "answer": "Focus on systems and processes, not individuals; identify root causes and actionable improvements.",
    "explanation": "**Blameless Postmortem Elements**:\n- Timeline of events\n- Root cause analysis (5 Whys)\n- What went well\n- What could be improved\n- Action items with owners\n\n**Key Principles**:\n- No finger-pointing\n- Assume good intentions\n- Focus on learning\n- Share widely",
    "tags": [
      "incident",
      "postmortem"
    ],
    "difficulty": "advanced",
    "channel": "sre",
    "subChannel": "incident-management",
    "diagram": "graph TD\n    Incident[Incident] --> Timeline[Timeline]\n    Timeline --> RCA[Root Cause]\n    RCA --> Actions[Action Items]\n    Actions --> Prevention[Prevention]",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "sr-130",
    "question": "Your web service has an SLO of 99.9% availability over 30 days. You've had 3 outages: 45 minutes, 20 minutes, and 15 minutes. What's your current availability SLI and are you meeting your SLO?",
    "answer": "SLI: 99.81% availability. Not meeting 99.9% SLO - exceeded error budget by 0.09%.",
    "explanation": "## SLI Calculation\n\n**Total time in 30 days:** 30 × 24 × 60 = 43,200 minutes\n\n**Total downtime:** 45 + 20 + 15 = 80 minutes\n\n**Uptime:** 43,200 - 80 = 43,120 minutes\n\n**SLI (Service Level Indicator):** (43,120 ÷ 43,200) × 100 = **99.81%**\n\n## SLO Analysis\n\n**Target SLO:** 99.9% availability\n**Current SLI:** 99.81%\n**Status:** ❌ **Not meeting SLO**\n\n## Error Budget\n\n**Allowed downtime for 99.9% SLO:** 43,200 × 0.001 = 43.2 minutes\n**Actual downtime:** 80 minutes\n**Error budget exceeded by:** 80 - 43.2 = 36.8 minutes (0.09%)\n\n## Key Concepts\n\n- **SLI (Service Level Indicator):** Actual measured performance metric\n- **SLO (Service Level Objective):** Target reliability goal\n- **Error Budget:** Allowed unreliability (100% - SLO%)\n\nWhen SLI < SLO, you've exceeded your error budget and should focus on reliability improvements over new features.",
    "tags": [
      "slo",
      "sli",
      "error-budget"
    ],
    "difficulty": "intermediate",
    "channel": "sre",
    "subChannel": "slo-sli",
    "diagram": "graph TD\n    A[30 Days Total Time<br/>43,200 minutes] --> B[Calculate Downtime]\n    B --> C[Outage 1: 45 min<br/>Outage 2: 20 min<br/>Outage 3: 15 min]\n    C --> D[Total Downtime<br/>80 minutes]\n    A --> E[Calculate Uptime<br/>43,200 - 80 = 43,120 min]\n    E --> F[SLI Calculation<br/>43,120 ÷ 43,200 × 100]\n    F --> G[Current SLI<br/>99.81%]\n    H[SLO Target<br/>99.9%] --> I{SLI ≥ SLO?}\n    G --> I\n    I -->|No| J[❌ SLO Breach<br/>Error Budget Exceeded]\n    I -->|Yes| K[✅ SLO Met<br/>Within Error Budget]\n    L[Error Budget<br/>43.2 minutes allowed] --> M[Budget Exceeded<br/>36.8 minutes over]",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "sr-131",
    "question": "You're managing a microservices platform with 50 services. Service A has a 95th percentile latency of 200ms and handles 10,000 RPS. It calls Service B (50ms, 5,000 RPS) and Service C (100ms, 3,000 RPS). During Black Friday, you expect 5x traffic. Service A's CPU utilization is currently 60%, memory at 70%. How do you plan capacity to maintain <500ms 95th percentile end-to-end latency?",
    "answer": "Scale Service A to 15 instances, Service B to 8 instances, Service C to 5 instances. Add circuit breakers, implement caching, and use load shedding.",
    "explanation": "## Capacity Planning Analysis\n\n### Current State Assessment\n- **Service A**: 200ms p95, 10K RPS, 60% CPU, 70% memory\n- **Service B**: 50ms p95, 5K RPS (called by A)\n- **Service C**: 100ms p95, 3K RPS (called by A)\n- **Current end-to-end latency**: ~350ms (200+50+100)\n\n### Black Friday Projections (5x traffic)\n- **Service A**: 50K RPS target\n- **Service B**: 25K RPS target  \n- **Service C**: 15K RPS target\n\n### Capacity Planning Strategy\n\n#### 1. **Horizontal Scaling Calculations**\n```\nService A: 50K RPS ÷ (10K RPS × 0.4 headroom) = ~12.5 → 15 instances\nService B: 25K RPS ÷ 5K RPS = 5 → 8 instances (buffer)\nService C: 15K RPS ÷ 3K RPS = 5 instances\n```\n\n#### 2. **Latency Optimization**\n- **Circuit breakers**: Prevent cascade failures\n- **Caching**: Reduce Service B/C calls by 30-40%\n- **Connection pooling**: Reduce connection overhead\n- **Load shedding**: Drop non-critical requests at 80% capacity\n\n#### 3. **Resource Allocation**\n- **CPU**: Target 40-50% utilization under peak load\n- **Memory**: Target 60% utilization with garbage collection headroom\n- **Network**: Ensure bandwidth can handle 5x throughput\n\n#### 4. **Monitoring & Alerting**\n- Set alerts at 70% capacity utilization\n- Monitor queue depths and connection pool exhaustion\n- Track error rates and implement auto-scaling triggers\n\n#### 5. **Fallback Strategies**\n- **Graceful degradation**: Disable non-essential features\n- **CDN offloading**: Cache static content\n- **Database read replicas**: Distribute read load\n\nThis approach ensures <500ms p95 latency while maintaining system reliability during traffic spikes.",
    "tags": [
      "capacity",
      "scaling"
    ],
    "difficulty": "advanced",
    "channel": "sre",
    "subChannel": "capacity-planning",
    "diagram": "graph TD\n    A[Load Balancer] --> B[Service A - 15 instances]\n    B --> C[Service B - 8 instances]\n    B --> D[Service C - 5 instances]\n    B --> E[Cache Layer]\n    F[Circuit Breaker] --> B\n    G[Auto Scaler] --> B\n    G --> C\n    G --> D\n    H[Monitoring] --> I[Alerts]\n    I --> G\n    J[Load Shedder] --> A\n    K[CDN] --> A\n    \n    style B fill:#ff9999\n    style C fill:#99ccff\n    style D fill:#99ff99\n    style E fill:#ffcc99\n    style F fill:#ff6666\n    style J fill:#cc99ff",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "sr-133",
    "question": "What's the difference between logs, metrics, and traces in observability?",
    "answer": "Logs: events, Metrics: numbers over time, Traces: request journeys.",
    "explanation": "**The Three Pillars of Observability**:\n\n1. **Logs**: Timestamped events showing what happened\n   - Structured logs with key-value pairs\n   - Error messages, debug info\n   - \"User login failed at 2:15 PM\"\n\n2. **Metrics**: Numerical data aggregated over time\n   - CPU usage, request count, error rate\n   - Charts and dashboards\n   - \"95th percentile latency: 200ms\"\n\n3. **Traces**: End-to-end request flow across services\n   - Shows path through distributed systems\n   - Identifies bottlenecks and failures\n   - \"API call took 500ms across 3 services\"\n\n**Why all three?** Logs tell you *what* happened, metrics show *how much*, traces reveal *where*.",
    "tags": [
      "metrics",
      "monitoring"
    ],
    "difficulty": "beginner",
    "channel": "sre",
    "subChannel": "observability",
    "diagram": "graph TD\n    A[Request] --> B[Frontend]\n    B --> C[API Gateway]\n    C --> D[Service A]\n    C --> E[Service B]\n    D --> F[Database]\n    \n    G[Logs] --> H[\"Error: DB timeout\"]\n    I[Metrics] --> J[\"CPU: 80%\"]\n    K[Traces] --> L[\"500ms total\"]\n    \n    style G fill:#ff9999\n    style I fill:#99ccff\n    style K fill:#99ff99",
    "lastUpdated": "2025-12-12T09:14:39.919Z"
  },
  {
    "id": "sr-142",
    "question": "You receive a PagerDuty alert at 3 AM: 'Production API is returning 500 errors'. What are your first three steps in handling this incident?",
    "answer": "Acknowledge alert, assess impact, and form response team.",
    "explanation": "**Incident Response First Steps**:\n\n1. **Acknowledge Alert (0-2 mins)**\n   - Accept PagerDuty incident\n   - Set status to 'Investigating'\n   - Prevent escalation\n\n2. **Assess Impact (2-5 mins)**\n   - Check monitoring dashboards\n   - Verify error rates and latency\n   - Determine user impact scope\n\n3. **Form Response Team (5-10 mins)**\n   - Notify on-call engineer\n   - Create incident channel (Slack)\n   - Document initial timeline\n\n**Key Principles**:\n- Stay calm under pressure\n- Communicate clearly\n- Document everything\n- Follow runbook if available",
    "tags": [
      "incident",
      "postmortem"
    ],
    "difficulty": "beginner",
    "channel": "sre",
    "subChannel": "incident-management",
    "diagram": "graph TD\n    A[PagerDuty Alert] --> B[Acknowledge Incident]\n    B --> C[Assess Impact]\n    C --> D[Check Dashboards]\n    C --> E[Verify Error Rates]\n    C --> F[Determine User Impact]\n    D --> G[Form Response Team]\n    E --> G\n    F --> G\n    G --> H[Notify On-Call]\n    G --> I[Create Slack Channel]\n    G --> J[Document Timeline]",
    "lastUpdated": "2025-12-12T10:04:51.668Z"
  },
  {
    "id": "sr-143",
    "question": "Your web application currently handles 1000 requests per minute during peak hours. Each request takes an average of 200ms to process. If you expect traffic to double in the next 6 months, how many additional server instances do you need if each server can handle 50 concurrent requests?",
    "answer": "Need 2 more instances. Current: 1000 req/min ÷ 60s = 16.67 req/s × 0.2s = 3.33 concurrent. Double = 6.67. Need 1 more instance minimum.",
    "explanation": "## Capacity Planning Calculation\n\n**Step 1: Calculate current concurrent requests**\n- Current load: 1000 requests/minute = 16.67 requests/second\n- Processing time: 200ms = 0.2 seconds\n- Concurrent requests = 16.67 × 0.2 = 3.33 concurrent requests\n\n**Step 2: Calculate future requirements**\n- Expected traffic: 2000 requests/minute = 33.33 requests/second\n- Future concurrent requests = 33.33 × 0.2 = 6.67 concurrent requests\n\n**Step 3: Determine server capacity**\n- Each server handles 50 concurrent requests\n- Current servers needed: 3.33 ÷ 50 = 0.067 servers (1 server sufficient)\n- Future servers needed: 6.67 ÷ 50 = 0.133 servers (1 server sufficient)\n\n**However**, for safety margin and avoiding saturation:\n- Add buffer capacity (typically 20-30%)\n- Consider peak spikes beyond average\n- Plan for gradual scaling\n\n**Recommendation**: Add 1-2 additional instances to handle growth safely.",
    "tags": [
      "capacity",
      "scaling"
    ],
    "difficulty": "beginner",
    "channel": "sre",
    "subChannel": "capacity-planning",
    "diagram": "graph TD\n    A[Current Load<br/>1000 req/min] --> B[16.67 req/sec]\n    B --> C[3.33 concurrent<br/>requests]\n    D[Future Load<br/>2000 req/min] --> E[33.33 req/sec]\n    E --> F[6.67 concurrent<br/>requests]\n    G[Server Capacity<br/>50 concurrent] --> H{Scaling Decision}\n    C --> H\n    F --> H\n    H --> I[Add 1-2 Instances<br/>for safety margin]",
    "lastUpdated": "2025-12-12T10:05:04.369Z"
  },
  {
    "id": "sr-146",
    "question": "Design a chaos engineering experiment to test the resilience of a microservices-based e-commerce platform during a database partition event. How would you ensure the experiment doesn't cause customer data loss while still providing meaningful insights?",
    "answer": "Implement controlled partition with circuit breakers, canary deployments, and data consistency checks to isolate failures.",
    "explanation": "## Chaos Engineering Experiment Design\n\n### **Objective**\nTest system resilience during database partition events while preventing customer data loss.\n\n### **Key Components**\n1. **Blast Radius Control**: Limit impact to non-critical services first\n2. **Data Consistency Validation**: Ensure no data corruption or loss\n3. **Gradual Escalation**: Start with read-only operations, then controlled writes\n\n### **Implementation Steps**\n1. **Preparation Phase**\n   - Identify critical data paths (orders, payments, user data)\n   - Set up monitoring and alerting for data consistency\n   - Create rollback procedures\n\n2. **Experiment Execution**\n   - Inject network partition between primary and replica databases\n   - Monitor service behavior and automatic failover mechanisms\n   - Validate circuit breaker patterns and retry logic\n\n3. **Validation Criteria**\n   - No customer order data loss\n   - Payment processing maintains ACID properties\n   - User sessions remain consistent\n   - System recovers within SLA thresholds\n\n### **Safety Mechanisms**\n- **Canary Deployment**: Test on 1% traffic first\n- **Automated Rollback**: Trigger on data inconsistency detection\n- **Read-Only Mode**: Switch to read-only during critical operations\n- **Data Verification**: Post-experiment consistency checks",
    "tags": [
      "chaos",
      "resilience"
    ],
    "difficulty": "advanced",
    "channel": "sre",
    "subChannel": "chaos-engineering",
    "diagram": "graph TD\n    A[Client Request] --> B[API Gateway]\n    B --> C[Order Service]\n    B --> D[Payment Service]\n    B --> E[User Service]\n    \n    C --> F[Primary DB]\n    C --> G[Replica DB]\n    D --> H[Payment DB]\n    E --> I[User DB]\n    \n    F -.->|Network Partition| G\n    \n    J[Chaos Controller] --> K[Network Partition]\n    J --> L[Monitoring System]\n    J --> M[Circuit Breaker]\n    \n    L --> N[Data Consistency Check]\n    L --> O[Performance Metrics]\n    \n    M --> P[Failover to Replica]\n    M --> Q[Read-Only Mode]\n    \n    N --> R{Data Valid?}\n    R -->|Yes| S[Continue Experiment]\n    R -->|No| T[Auto Rollback]",
    "lastUpdated": "2025-12-12T10:14:55.204Z"
  },
  {
    "id": "sr-147",
    "question": "Your distributed system has 5 microservices with the following failure rates: Service A (0.1%), Service B (0.2%), Service C (0.05%), Service D (0.15%), Service E (0.3%). Each request flows through all services sequentially. If your SLO requires 99.5% success rate and you process 1M requests daily, what's your current system reliability and how would you architect fault tolerance to meet the SLO?",
    "answer": "Current reliability: 99.2%. Need circuit breakers, retries, and service mesh to achieve 99.5% SLO.",
    "explanation": "## System Reliability Analysis\n\n### **Current State Calculation**\n\n**Sequential Service Chain Reliability:**\n- Service A: 99.9% (0.1% failure)\n- Service B: 99.8% (0.2% failure) \n- Service C: 99.95% (0.05% failure)\n- Service D: 99.85% (0.15% failure)\n- Service E: 99.7% (0.3% failure)\n\n**Overall System Reliability:**\n```\n0.999 × 0.998 × 0.9995 × 0.9985 × 0.997 = 0.9920 = 99.2%\n```\n\n**Daily Impact:**\n- 1M requests × 0.8% failure rate = 8,000 failed requests/day\n- **Gap to SLO:** 99.5% - 99.2% = 0.3% (3,000 requests)\n\n### **Fault Tolerance Architecture**\n\n#### **1. Circuit Breaker Pattern**\n- Implement per-service circuit breakers\n- Fast-fail on consecutive failures\n- Automatic recovery testing\n\n#### **2. Retry Strategy**\n- Exponential backoff with jitter\n- Maximum 3 retries per service\n- Timeout-based failure detection\n\n#### **3. Service Mesh Implementation**\n- **Istio/Linkerd** for traffic management\n- Automatic load balancing\n- Health check integration\n\n#### **4. Redundancy & Failover**\n- Deploy multiple instances per service\n- Cross-AZ deployment for availability\n- Database read replicas\n\n#### **5. Graceful Degradation**\n- Non-critical service bypass\n- Cached response fallbacks\n- Feature flags for service isolation\n\n**Expected Improvement:** With proper fault tolerance, achieve 99.6-99.8% reliability, exceeding the 99.5% SLO target.",
    "tags": [
      "reliability",
      "incident"
    ],
    "difficulty": "advanced",
    "channel": "sre",
    "subChannel": "reliability",
    "diagram": "graph TD\n    A[Client Request] --> B[API Gateway]\n    B --> C[Circuit Breaker]\n    C --> D[Service A<br/>99.9%]\n    D --> E[Service B<br/>99.8%]\n    E --> F[Service C<br/>99.95%]\n    F --> G[Service D<br/>99.85%]\n    G --> H[Service E<br/>99.7%]\n    \n    I[Retry Logic] --> C\n    J[Service Mesh] --> D\n    J --> E\n    J --> F\n    J --> G\n    J --> H\n    \n    K[Health Checks] --> L[Load Balancer]\n    L --> M[Service Instances]\n    \n    N[Monitoring] --> O[SLO: 99.5%]\n    N --> P[Current: 99.2%]\n    N --> Q[Target: 99.6%+]\n    \n    style D fill:#ffcccc\n    style E fill:#ffcccc\n    style F fill:#ccffcc\n    style G fill:#ffcccc\n    style H fill:#ffcccc\n    style O fill:#ff6666\n    style P fill:#ffaa66\n    style Q fill:#66ff66",
    "lastUpdated": "2025-12-12T10:15:16.631Z"
  },
  {
    "id": "sr-148",
    "question": "You're designing capacity planning for a microservices platform handling 100M requests/day with 3x traffic spikes during flash sales. Each service has different resource profiles: API gateway (CPU-bound, 2ms avg), user service (memory-bound, 50ms avg), payment service (I/O-bound, 200ms avg), and inventory service (database-bound, 100ms avg). Design a capacity planning strategy that accounts for cascading failures, auto-scaling lag (2-3 minutes), and maintains 99.9% availability during peak loads.",
    "answer": "Use predictive scaling with 40% headroom, circuit breakers, bulkhead isolation, and pre-warming strategies for known events.",
    "explanation": "## Capacity Planning Strategy\n\n### 1. **Baseline Capacity Analysis**\n- **Normal Load**: 100M requests/day = ~1,157 RPS average\n- **Peak Load**: 3x spike = ~3,471 RPS\n- **Service-specific bottlenecks**: Identify each service's limiting resource\n\n### 2. **Predictive Scaling Approach**\n```\nTarget Capacity = (Peak Load × Safety Factor) + Headroom\nSafety Factor = 1.4 (40% buffer for auto-scaling lag)\nHeadroom = 20% for unexpected spikes\n```\n\n### 3. **Service-Specific Strategies**\n- **API Gateway**: Horizontal scaling with load balancing\n- **User Service**: Memory-optimized instances with connection pooling\n- **Payment Service**: Queue-based processing with circuit breakers\n- **Inventory Service**: Read replicas and caching layers\n\n### 4. **Resilience Patterns**\n- **Circuit Breakers**: Prevent cascading failures\n- **Bulkhead Isolation**: Separate resource pools per service\n- **Graceful Degradation**: Non-critical features fail first\n\n### 5. **Pre-emptive Scaling**\n- **Scheduled Scaling**: Pre-warm for known events\n- **Predictive Metrics**: Use historical data and ML models\n- **Multi-zone Distribution**: Spread load across availability zones\n\n### 6. **Monitoring & Alerting**\n- **Leading Indicators**: Queue depth, response times, error rates\n- **Capacity Metrics**: CPU, memory, network, disk utilization\n- **Business Metrics**: Revenue impact, user experience scores",
    "tags": [
      "capacity",
      "scaling"
    ],
    "difficulty": "advanced",
    "channel": "sre",
    "subChannel": "capacity-planning",
    "diagram": "graph TD\n    A[Load Balancer] --> B[API Gateway Cluster]\n    B --> C[User Service Pool]\n    B --> D[Payment Service Pool]\n    B --> E[Inventory Service Pool]\n    \n    C --> F[User DB Replicas]\n    D --> G[Payment Queue]\n    E --> H[Inventory Cache]\n    E --> I[Inventory DB]\n    \n    J[Predictive Scaler] --> K[Metrics Collector]\n    K --> L[Historical Data]\n    K --> M[Real-time Metrics]\n    \n    J --> N[Auto-scaling Groups]\n    N --> B\n    N --> C\n    N --> D\n    N --> E\n    \n    O[Circuit Breaker] --> B\n    O --> C\n    O --> D\n    O --> E\n    \n    P[Monitoring Dashboard] --> K\n    Q[Alerting System] --> K",
    "lastUpdated": "2025-12-12T10:15:33.442Z"
  },
  {
    "id": "sr-149",
    "question": "You're designing capacity planning for a microservices platform handling 10M daily active users. Each user generates 50 API calls/day with 80% during peak hours (8am-8pm). Your services have these characteristics: Auth service (5ms avg latency, 95th percentile 15ms), User service (12ms avg, 95th percentile 40ms), Payment service (25ms avg, 95th percentile 80ms). Given a target 99.9% availability and P95 latency under 100ms for end-to-end requests, calculate the required capacity considering: 1) Circuit breaker overhead (5% additional load), 2) Auto-scaling lag (30 seconds), 3) Database connection pooling (max 100 connections per instance), 4) Memory constraints (2GB per service instance). How many instances of each service do you need?",
    "answer": "Auth: 45 instances, User: 52 instances, Payment: 78 instances. Factor in 40% headroom for auto-scaling lag and circuit breaker overhead.",
    "explanation": "## Capacity Planning Calculation\n\n### Traffic Analysis\n- **Daily requests**: 10M users × 50 calls = 500M requests/day\n- **Peak traffic**: 80% in 12 hours = 400M requests in 12h\n- **Peak RPS**: 400M ÷ (12 × 3600) = ~9,259 RPS\n\n### Service-Specific Calculations\n\n#### Auth Service\n- **Target**: P95 < 15ms (already meeting SLA)\n- **Capacity per instance**: Assuming 200 RPS per instance at P95\n- **Base instances needed**: 9,259 ÷ 200 = 47 instances\n- **With overhead**: 47 × 1.05 (circuit breaker) = 49 instances\n- **Auto-scaling buffer**: 49 × 0.3 = 15 additional\n- **Total**: 49 + 15 = **64 instances**\n\n#### User Service\n- **Current P95**: 40ms (needs optimization)\n- **Capacity per instance**: ~150 RPS to maintain P95 < 40ms\n- **Base instances**: 9,259 ÷ 150 = 62 instances\n- **With overhead**: 62 × 1.35 = **84 instances**\n\n#### Payment Service\n- **Current P95**: 80ms (critical path)\n- **Capacity per instance**: ~100 RPS due to higher latency\n- **Database constraint**: 100 connections ÷ 10 services = 10 connections per instance\n- **Base instances**: 9,259 ÷ 100 = 93 instances\n- **With overhead**: 93 × 1.35 = **126 instances**\n\n### Key Considerations\n\n1. **Circuit Breaker Overhead**: 5% additional load when services are degraded\n2. **Auto-scaling Lag**: 30-second delay requires 30% buffer capacity\n3. **Database Bottleneck**: Connection pool limits may require horizontal scaling\n4. **Memory Constraints**: 2GB per instance limits concurrent request handling\n5. **Cascading Failures**: Payment service latency affects entire request chain\n\n### Optimization Recommendations\n\n- Implement request queuing for payment service\n- Add read replicas for database scaling\n- Use connection multiplexing to optimize database usage\n- Consider async processing for non-critical payment operations",
    "tags": [
      "capacity",
      "scaling"
    ],
    "difficulty": "advanced",
    "channel": "sre",
    "subChannel": "capacity-planning",
    "diagram": "graph TD\n    A[Load Balancer<br/>9,259 RPS] --> B[Auth Service<br/>64 instances]\n    A --> C[User Service<br/>84 instances]\n    A --> D[Payment Service<br/>126 instances]\n    \n    B --> E[Auth DB<br/>Connection Pool: 100]\n    C --> F[User DB<br/>Connection Pool: 100]\n    D --> G[Payment DB<br/>Connection Pool: 100]\n    \n    H[Auto Scaler<br/>30s lag] --> B\n    H --> C\n    H --> D\n    \n    I[Circuit Breaker<br/>5% overhead] --> B\n    I --> C\n    I --> D\n    \n    J[Monitoring<br/>P95 < 100ms<br/>99.9% availability] --> A",
    "lastUpdated": "2025-12-12T10:15:55.304Z"
  },
  {
    "id": "sr-150",
    "question": "You're implementing chaos engineering for a distributed payment system processing $10M daily transactions. Design a chaos experiment to test resilience against Byzantine failures where 30% of payment validation nodes provide conflicting consensus results. How would you ensure financial accuracy while testing system behavior under adversarial conditions?",
    "answer": "Use shadow consensus with financial reconciliation, gradual fault injection, and real-time audit trails to test Byzantine fault tolerance.",
    "explanation": "## Byzantine Fault Tolerance Chaos Experiment\n\n### **Experiment Objective**\nTest payment system resilience against Byzantine failures where nodes provide conflicting consensus results while maintaining financial integrity.\n\n### **Safety-First Architecture**\n\n#### **1. Shadow Consensus System**\n- **Parallel Processing**: Run chaos experiment on shadow payment network\n- **Real-time Mirroring**: Copy production traffic to test environment\n- **Financial Isolation**: No real money movement during experiment\n- **Consensus Validation**: Compare results between production and chaos systems\n\n#### **2. Byzantine Fault Injection Strategy**\n```\nPhase 1: Single malicious node (10% of validators)\nPhase 2: Multiple conflicting nodes (20% of validators)\nPhase 3: Coordinated Byzantine attack (30% of validators)\n```\n\n#### **3. Financial Accuracy Safeguards**\n- **Cryptographic Audit Trail**: Immutable transaction logs with digital signatures\n- **Multi-signature Validation**: Require 2/3+ honest nodes for transaction approval\n- **Real-time Reconciliation**: Continuous balance verification across all nodes\n- **Rollback Mechanisms**: Automatic transaction reversal on consensus failure\n\n### **Implementation Steps**\n\n#### **Pre-Experiment Validation**\n- Verify all nodes have identical ledger state\n- Establish baseline performance metrics\n- Configure monitoring for consensus divergence\n- Set up automated circuit breakers\n\n#### **Chaos Injection Patterns**\n- **Double-spending Attempts**: Malicious nodes approve conflicting transactions\n- **Timestamp Manipulation**: Nodes report incorrect transaction ordering\n- **Balance Falsification**: Nodes report incorrect account balances\n- **Network Partitioning**: Isolate honest nodes from Byzantine nodes\n\n#### **Monitoring & Detection**\n- **Consensus Metrics**: Track agreement rates across validator nodes\n- **Financial Integrity**: Monitor for balance inconsistencies\n- **Performance Impact**: Measure transaction throughput degradation\n- **Recovery Time**: Track system restoration after fault injection\n\n### **Success Criteria**\n- System maintains financial accuracy despite 30% Byzantine nodes\n- Transaction throughput degrades gracefully (< 50% reduction)\n- Honest nodes detect and isolate malicious behavior within 30 seconds\n- No double-spending or balance corruption occurs\n- System recovers to normal operation within 2 minutes\n\n### **Risk Mitigation**\n- **Immediate Rollback**: Automated experiment termination on financial anomaly\n- **Isolated Environment**: Complete separation from production systems\n- **Continuous Auditing**: Real-time financial reconciliation\n- **Expert Oversight**: Financial engineers monitor experiment execution",
    "tags": [
      "chaos",
      "resilience"
    ],
    "difficulty": "advanced",
    "channel": "sre",
    "subChannel": "chaos-engineering",
    "diagram": "graph TD\n    A[Production Traffic] --> B[Traffic Mirror]\n    B --> C[Shadow Payment Network]\n    \n    C --> D[Honest Validator 1]\n    C --> E[Honest Validator 2]\n    C --> F[Honest Validator 3]\n    C --> G[Byzantine Node 1]\n    C --> H[Byzantine Node 2]\n    \n    D --> I[Consensus Engine]\n    E --> I\n    F --> I\n    G --> I\n    H --> I\n    \n    I --> J{Consensus Check}\n    J -->|Valid| K[Transaction Approved]\n    J -->|Invalid| L[Transaction Rejected]\n    \n    M[Chaos Controller] --> N[Fault Injection]\n    N --> G\n    N --> H\n    \n    O[Financial Auditor] --> P[Balance Verification]\n    O --> Q[Transaction Integrity]\n    O --> R[Consensus Monitoring]\n    \n    P --> S{Financial Accuracy?}\n    S -->|Yes| T[Continue Experiment]\n    S -->|No| U[Emergency Rollback]\n    \n    V[Monitoring Dashboard] --> W[Consensus Rate]\n    V --> X[Transaction Throughput]\n    V --> Y[Byzantine Detection Time]\n    \n    style G fill:#ff6666\n    style H fill:#ff6666\n    style U fill:#ff0000\n    style S fill:#ffaa00",
    "lastUpdated": "2025-12-12T10:16:21.457Z"
  },
  {
    "id": "sr-153",
    "question": "You're implementing chaos engineering for a microservices architecture. Your payment service has a 99.9% SLA. During a chaos experiment, you inject 500ms latency into 20% of requests to the database. The service starts timing out after 1 second. What's the most critical metric to monitor first, and what would indicate the experiment should be stopped immediately?",
    "answer": "Monitor error rate/SLA compliance. Stop if error rate exceeds error budget (0.1%) or cascading failures detected in dependent services.",
    "explanation": "## Critical Metrics Priority\n\n### Primary Metric: Error Rate & SLA Compliance\nWith a 99.9% SLA, you have an **error budget of 0.1%**. This is your guardrail.\n\n### Why This Matters\n- 20% of requests getting 500ms latency + network overhead = likely >1s total\n- These requests will timeout, directly impacting SLA\n- Expected impact: ~20% of traffic affected initially\n\n### Stop Conditions\n1. **Error rate exceeds error budget** (>0.1% over measurement window)\n2. **Cascading failures detected** - dependent services showing increased errors\n3. **Queue buildup** - thread pool exhaustion or connection pool saturation\n4. **Circuit breakers tripping** in upstream services\n\n### Secondary Metrics to Watch\n- P99 latency (should spike but not affect all requests)\n- Active connections/thread pool utilization\n- Retry storm indicators\n- Customer-facing transaction success rate\n\n### Chaos Engineering Best Practices\n- Start with smaller blast radius (5-10% of traffic)\n- Use automated stop conditions\n- Monitor blast radius expansion\n- Have rollback ready within seconds\n\n## Calculation Example\nIf you serve 1000 req/s:\n- 200 req/s affected by latency injection\n- If all timeout: 20% error rate\n- This **exceeds** your 0.1% error budget by 200x\n- **Immediate stop required**\n\nThe experiment design itself may be too aggressive for the given SLA.",
    "tags": [
      "chaos",
      "resilience"
    ],
    "difficulty": "intermediate",
    "channel": "sre",
    "subChannel": "chaos-engineering",
    "diagram": "graph TD\n    A[Chaos Experiment Start] --> B[Inject 500ms Latency<br/>20% of DB Requests]\n    B --> C{Monitor Error Rate}\n    C -->|< 0.1%| D[Continue Experiment]\n    C -->|> 0.1%| E[STOP: SLA Breach]\n    B --> F{Check Cascading Effects}\n    F -->|Isolated| D\n    F -->|Spreading| G[STOP: Blast Radius Growing]\n    D --> H{Monitor Secondary Metrics}\n    H --> I[Thread Pool Usage]\n    H --> J[Circuit Breaker Status]\n    H --> K[Queue Depth]\n    I -->|Saturated| G\n    J -->|Tripped| G\n    K -->|Growing| G\n    E --> L[Rollback Immediately]\n    G --> L\n    D --> M[Complete Experiment<br/>Analyze Results]",
    "lastUpdated": "2025-12-13T01:08:12.121Z"
  },
  {
    "id": "sr-155",
    "question": "What is the difference between metrics, logs, and traces in observability, and when would you use each?",
    "answer": "Metrics: aggregated numbers over time. Logs: discrete events. Traces: request flow across services. Use all three for complete observability.",
    "explanation": "## The Three Pillars of Observability\n\n### Metrics\n**What**: Numerical measurements aggregated over time (CPU usage, request rate, error count)\n**When to use**: \n- Monitoring system health and performance trends\n- Setting up alerts and SLOs\n- Understanding resource utilization\n**Example**: `http_requests_total`, `memory_usage_bytes`\n\n### Logs\n**What**: Discrete event records with timestamps and context\n**When to use**:\n- Debugging specific issues\n- Auditing and compliance\n- Understanding what happened at a specific point in time\n**Example**: `[2025-12-13 10:30:45] ERROR: Database connection failed`\n\n### Traces\n**What**: End-to-end journey of a request through distributed systems\n**When to use**:\n- Identifying bottlenecks in microservices\n- Understanding service dependencies\n- Debugging latency issues across services\n**Example**: A single API call traced through API Gateway → Auth Service → Database\n\n### Why All Three?\nEach pillar answers different questions:\n- **Metrics**: \"Is there a problem?\"\n- **Logs**: \"What happened?\"\n- **Traces**: \"Where is the problem in the request flow?\"\n\nUsing all three together provides complete visibility into system behavior and enables faster incident resolution.",
    "tags": [
      "metrics",
      "monitoring"
    ],
    "difficulty": "beginner",
    "channel": "sre",
    "subChannel": "observability",
    "diagram": "graph TD\n    A[Observability] --> B[Metrics]\n    A --> C[Logs]\n    A --> D[Traces]\n    B --> E[Aggregated Numbers]\n    B --> F[Time Series Data]\n    B --> G[Alerts & Dashboards]\n    C --> H[Discrete Events]\n    C --> I[Structured/Unstructured]\n    C --> J[Debugging Context]\n    D --> K[Request Flow]\n    D --> L[Service Dependencies]\n    D --> M[Latency Analysis]\n    E --> N[Example: CPU 75%]\n    H --> O[Example: Error Log]\n    K --> P[Example: API → DB]",
    "lastUpdated": "2025-12-13T01:08:45.260Z"
  },
  {
    "id": "sr-154",
    "question": "Your API serves 10M requests/day with a 99.9% availability SLO and 30-day error budget. After a 4-hour outage affecting 100% of traffic, how many minutes of downtime remain in your error budget?",
    "answer": "39.6 minutes remaining (4 hours consumed from 43.2-minute monthly budget)",
    "explanation": "## Error Budget Calculation\n\n**Given:**\n- SLO: 99.9% availability\n- Error budget: 0.1% (100% - 99.9%)\n- Time window: 30 days\n- Outage: 4 hours (240 minutes)\n\n**Step 1: Calculate total error budget**\n```\n30 days = 30 × 24 × 60 = 43,200 minutes\nError budget = 43,200 × 0.001 = 43.2 minutes\n```\n\n**Step 2: Calculate remaining budget**\n```\nConsumed = 240 minutes (4 hours)\nRemaining = 43.2 - 240 = -196.8 minutes\n```\n\n**Result:** The error budget is **exhausted**. You've exceeded it by 196.8 minutes (4.55×). This means:\n- No more incidents allowed this month\n- Feature releases should be frozen\n- Focus shifts to reliability improvements\n- Need executive approval for risky changes\n\n**Key SRE Principle:** Error budgets balance innovation velocity with reliability. When exhausted, teams prioritize stability over new features until the budget resets.",
    "tags": [
      "slo",
      "sli",
      "error-budget"
    ],
    "difficulty": "intermediate",
    "channel": "sre",
    "subChannel": "slo-sli",
    "diagram": "graph TD\n    A[30-Day Period] --> B[Total Minutes: 43,200]\n    B --> C[SLO: 99.9%]\n    C --> D[Error Budget: 0.1%]\n    D --> E[43.2 minutes allowed downtime]\n    E --> F[Outage: 240 minutes]\n    F --> G{Budget Status}\n    G -->|Exceeded by 196.8 min| H[Freeze Features]\n    G -->|Within Budget| I[Continue Normal Ops]\n    H --> J[Focus on Reliability]",
    "lastUpdated": "2025-12-13T01:09:58.799Z"
  },
  {
    "id": "sr-155",
    "question": "What is the difference between metrics, logs, and traces in observability, and when would you use each?",
    "answer": "Metrics: aggregated numbers over time. Logs: discrete events. Traces: request flow across services. Use all three for complete observability.",
    "explanation": "## The Three Pillars of Observability\n\n### Metrics\n**What**: Numerical measurements aggregated over time (CPU usage, request rate, error count)\n**When to use**: \n- Monitoring system health and performance trends\n- Setting up alerts and SLOs\n- Understanding resource utilization\n**Example**: `http_requests_total`, `memory_usage_bytes`\n\n### Logs\n**What**: Discrete event records with timestamps and context\n**When to use**:\n- Debugging specific issues\n- Auditing and compliance\n- Understanding what happened at a specific point in time\n**Example**: `[2025-12-13 10:30:45] ERROR: Database connection failed`\n\n### Traces\n**What**: End-to-end journey of a request through distributed systems\n**When to use**:\n- Identifying bottlenecks in microservices\n- Understanding service dependencies\n- Debugging latency issues across services\n**Example**: A single API call traced through API Gateway → Auth Service → Database\n\n### Why All Three?\nEach pillar answers different questions:\n- **Metrics**: \"Is there a problem?\"\n- **Logs**: \"What happened?\"\n- **Traces**: \"Where is the problem in the request flow?\"\n\nUsing all three together provides complete visibility into system behavior and enables faster incident resolution.",
    "tags": [
      "metrics",
      "monitoring"
    ],
    "difficulty": "beginner",
    "channel": "sre",
    "subChannel": "observability",
    "diagram": "graph TD\n    A[Observability] --> B[Metrics]\n    A --> C[Logs]\n    A --> D[Traces]\n    B --> E[Aggregated Numbers]\n    B --> F[Time Series Data]\n    B --> G[Alerts & Dashboards]\n    C --> H[Discrete Events]\n    C --> I[Structured/Unstructured]\n    C --> J[Debugging Context]\n    D --> K[Request Flow]\n    D --> L[Service Dependencies]\n    D --> M[Latency Analysis]\n    E --> N[Example: CPU 75%]\n    H --> O[Example: Error Log]\n    K --> P[Example: API → DB]",
    "lastUpdated": "2025-12-13T01:10:12.282Z"
  },
  {
    "id": "sr-169",
    "question": "Your API service has an SLO of 99.9% availability. If you have 5 incidents this month with downtimes of 10min, 5min, 15min, 8min, and 12min, did you meet your SLO? (Assume 30-day month)",
    "answer": "No. Total downtime: 50min out of 43,200min = 99.88% availability, which violates 99.9% SLO",
    "explanation": "## Understanding SLO vs SLI\n\n**SLI (Service Level Indicator)**: The actual measured metric (e.g., uptime percentage)\n**SLO (Service Level Objective)**: The target threshold for the SLI\n\n## Calculation\n\n1. **Total minutes in 30 days**: 30 × 24 × 60 = 43,200 minutes\n2. **Total downtime**: 10 + 5 + 15 + 8 + 12 = 50 minutes\n3. **Uptime**: 43,200 - 50 = 43,150 minutes\n4. **Availability (SLI)**: (43,150 / 43,200) × 100 = 99.88%\n\n## Result\n\nSince 99.88% < 99.9%, you **did NOT meet** your SLO. You have an **error budget deficit** of 0.02%.\n\n## Error Budget\n\nWith a 99.9% SLO, you have a 0.1% error budget = 43.2 minutes/month. You used 50 minutes, exceeding the budget by 6.8 minutes.",
    "tags": [
      "slo",
      "sli",
      "error-budget"
    ],
    "difficulty": "beginner",
    "channel": "sre",
    "subChannel": "slo-sli",
    "diagram": "graph TD\n    A[Monthly SLO: 99.9%] --> B[Error Budget: 0.1%]\n    B --> C[43.2 minutes allowed downtime]\n    D[Actual Incidents] --> E[50 minutes total downtime]\n    E --> F{Compare}\n    C --> F\n    F --> G[50min > 43.2min]\n    G --> H[SLO Violated ❌]\n    H --> I[Error Budget Exceeded by 6.8min]",
    "lastUpdated": "2025-12-14T01:18:30.948Z"
  }
]