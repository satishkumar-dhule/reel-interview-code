{
  "id": "sy-140",
  "question": "Design a rate limiting service that can handle 10 million requests per second with distributed consistency across multiple data centers. The service should support multiple rate limiting strategies (token bucket, sliding window, fixed window) and provide sub-millisecond latency. How would you architect this to handle bursts, prevent thundering herd problems, and ensure accurate global rate limits?",
  "answer": "Use Redis Cluster with Consistent Hashing + Local Caching + Adaptive Rate Limiting with Hierarchical Rate Limiting (user → API → global).",
  "explanation": "## Architecture Overview\n\n**Core Components:**\n1. **Rate Limiting Engine** - Pluggable strategy pattern supporting token bucket, sliding window, and fixed window algorithms\n2. **Distributed Cache Layer** - Redis Cluster with consistent hashing for horizontal scaling\n3. **Local Cache Tier** - L1 cache with write-through to reduce Redis load\n4. **Configuration Service** - Dynamic rule management with hot-reloading\n5. **Metrics & Analytics** - Real-time monitoring and alerting\n\n**Key Design Decisions:**\n\n### 1. Hierarchical Rate Limiting\n- **User Level**: Per-user quotas (e.g., 1000 req/min)\n- **API Level**: Per-endpoint limits (e.g., 100 req/min)\n- **Global Level**: System-wide protection (e.g., 10M req/s)\n\n### 2. Multi-Level Caching Strategy\n- **L1 Cache**: In-memory with 1-second TTL for 90% of requests\n- **L2 Cache**: Redis Cluster with consistent hashing\n- **Write-through**: Updates propagate to both levels\n\n### 3. Burst Handling\n- **Token Bucket**: Allows controlled bursts\n- **Credit System**: Accumulates unused capacity\n- **Priority Queues**: VIP users get preferential treatment\n\n### 4. Thundering Herd Prevention\n- **Request Coalescing**: Batch requests for same key\n- **Exponential Backoff**: Adaptive retry with jitter\n- **Circuit Breakers**: Fail-fast during overload\n\n### 5. Global Consistency\n- **Vector Clocks**: Resolve conflicts across data centers\n- **Gossip Protocol**: Sync rate limit state\n- **Eventual Consistency**: Acceptable for rate limiting\n\n### 6. Performance Optimizations\n- **Connection Pooling**: Reuse Redis connections\n- **Pipelining**: Batch Redis operations\n- **Compression**: Reduce network overhead\n- **Async Processing**: Non-blocking I/O\n\n### 7. Monitoring & Alerting\n- **Real-time Dashboards**: Rate limit utilization\n- **Anomaly Detection**: Unusual traffic patterns\n- **Auto-scaling**: Dynamic cluster sizing\n\n## Implementation Considerations\n\n**Data Model:**\n- Key: `rate_limit:{user_id}:{api_id}:{window}`\n- Value: `{count, last_reset, credits}`\n- TTL: Window duration + safety margin\n\n**Failure Modes:**\n- Redis unavailable: Fall back to local limits\n- Network partition: Permissive mode with logging\n- Cache stampede: Request deduplication\n\n**Scalability:**\n- Horizontal scaling with Redis Cluster\n- Geographic distribution with edge caching\n- Load balancing with consistent hashing",
  "diagram": "graph TD\n    A[Client Request] --> B[Load Balancer]\n    B --> C[Rate Limiting Service]\n    \n    C --> D{Local Cache Check}\n    D -->|Hit| E[Allow/Deny]\n    D -->|Miss| F[Distributed Cache]\n    \n    F --> G{Redis Cluster}\n    G --> H[Shard 1]\n    G --> I[Shard 2]\n    G --> J[Shard N]\n    \n    C --> K[Rate Limiting Engine]\n    K --> L[Token Bucket]\n    K --> M[Sliding Window]\n    K --> N[Fixed Window]\n    \n    C --> O[Configuration Service]\n    O --> P[Rate Limit Rules]\n    O --> Q[User Quotas]\n    \n    C --> R[Analytics Engine]\n    R --> S[Metrics Dashboard]\n    R --> T[Alert System]\n    \n    U[Data Center 1] --> G\n    V[Data Center 2] --> G\n    W[Data Center N] --> G\n    \n    G --> X[Gossip Protocol]\n    X --> Y[State Synchronization]\n    \n    style C fill:#e1f5fe\n    style G fill:#f3e5f5\n    style K fill:#e8f5e8",
  "difficulty": "advanced",
  "tags": [
    "api",
    "rest"
  ],
  "lastUpdated": "2025-12-12T09:37:42.396Z"
}