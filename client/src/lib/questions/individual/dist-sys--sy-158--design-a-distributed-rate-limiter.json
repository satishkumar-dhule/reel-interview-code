{
  "id": "sy-158",
  "question": "Design a distributed rate limiter that can handle 1M requests/second across 100 data centers with <10ms latency. How do you ensure accurate rate limiting while avoiding coordination overhead?",
  "answer": "Use local counters with async gossip protocol, sliding window algorithm, and token bucket with eventual consistency guarantees.",
  "explanation": "## Solution Architecture\n\n### Key Components\n\n1. **Local Rate Limiters**: Each node maintains local counters using sliding window or token bucket algorithms\n2. **Gossip Protocol**: Nodes periodically exchange counter information to achieve eventual consistency\n3. **Hybrid Approach**: Combine local decisions with periodic synchronization\n\n### Design Choices\n\n**Local Token Buckets**\n- Each node gets quota allocation (total_limit / num_nodes)\n- Tokens refill at configured rate\n- Fast local decisions (<1ms)\n- Trade-off: May allow brief bursts above global limit\n\n**Sliding Window Counters**\n- Track requests in time windows (e.g., 1-second buckets)\n- Use Redis sorted sets or in-memory structures\n- Weighted counting for window boundaries\n\n**Gossip Synchronization**\n- Nodes exchange counter deltas every 100-500ms\n- Epidemic broadcast ensures eventual consistency\n- Adjust local quotas based on cluster-wide usage\n\n### Handling Edge Cases\n\n**Hot Partitions**: Use consistent hashing with virtual nodes to distribute load\n\n**Network Partitions**: Implement conservative limits during splits (fail-safe mode)\n\n**Burst Traffic**: Pre-allocate burst capacity (e.g., 120% of steady-state limit)\n\n### Accuracy vs Performance Trade-offs\n\n- **Strict Accuracy**: Use centralized Redis with Lua scripts (higher latency ~50ms)\n- **High Performance**: Local-only decisions (may exceed limit by 5-10%)\n- **Balanced**: Gossip-based with 1-2% overage tolerance\n\n### Implementation Details\n\n```\nAlgorithm: Hybrid Rate Limiter\n1. Check local token bucket\n2. If tokens available, allow immediately\n3. If near limit, check gossip state\n4. Background: sync counters every 200ms\n5. Adjust local quota based on cluster load\n```\n\n### Scalability Considerations\n\n- **Horizontal Scaling**: Add nodes dynamically, redistribute quotas\n- **Geographic Distribution**: Regional rate limiters with cross-region aggregation\n- **Storage**: Use in-memory stores (Redis, Memcached) with TTL-based cleanup",
  "diagram": "graph TD\n    A[Client Request] --> B[Load Balancer]\n    B --> C[Node 1: Local Rate Limiter]\n    B --> D[Node 2: Local Rate Limiter]\n    B --> E[Node 3: Local Rate Limiter]\n    \n    C --> F[Local Token Bucket]\n    D --> G[Local Token Bucket]\n    E --> H[Local Token Bucket]\n    \n    F -.Gossip Sync.-> G\n    G -.Gossip Sync.-> H\n    H -.Gossip Sync.-> F\n    \n    C --> I[Redis Cache]\n    D --> I\n    E --> I\n    \n    I --> J[Sliding Window Counters]\n    \n    F --> K{Tokens Available?}\n    K -->|Yes| L[Allow Request]\n    K -->|No| M[Reject: 429]\n    \n    N[Gossip Manager] --> F\n    N --> G\n    N --> H\n    \n    O[Quota Adjuster] --> N\n    I -.Periodic Sync.-> O",
  "difficulty": "advanced",
  "tags": [
    "dist-sys",
    "architecture"
  ],
  "lastUpdated": "2025-12-13T01:09:32.003Z"
}