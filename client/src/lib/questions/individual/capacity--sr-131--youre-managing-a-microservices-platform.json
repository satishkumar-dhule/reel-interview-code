{
  "id": "sr-131",
  "question": "You're managing a microservices platform with 50 services. Service A has a 95th percentile latency of 200ms and handles 10,000 RPS. It calls Service B (50ms, 5,000 RPS) and Service C (100ms, 3,000 RPS). During Black Friday, you expect 5x traffic. Service A's CPU utilization is currently 60%, memory at 70%. How do you plan capacity to maintain <500ms 95th percentile end-to-end latency?",
  "answer": "Scale Service A to 15 instances, Service B to 8 instances, Service C to 5 instances. Add circuit breakers, implement caching, and use load shedding.",
  "explanation": "## Capacity Planning Analysis\n\n### Current State Assessment\n- **Service A**: 200ms p95, 10K RPS, 60% CPU, 70% memory\n- **Service B**: 50ms p95, 5K RPS (called by A)\n- **Service C**: 100ms p95, 3K RPS (called by A)\n- **Current end-to-end latency**: ~350ms (200+50+100)\n\n### Black Friday Projections (5x traffic)\n- **Service A**: 50K RPS target\n- **Service B**: 25K RPS target  \n- **Service C**: 15K RPS target\n\n### Capacity Planning Strategy\n\n#### 1. **Horizontal Scaling Calculations**\n```\nService A: 50K RPS ÷ (10K RPS × 0.4 headroom) = ~12.5 → 15 instances\nService B: 25K RPS ÷ 5K RPS = 5 → 8 instances (buffer)\nService C: 15K RPS ÷ 3K RPS = 5 instances\n```\n\n#### 2. **Latency Optimization**\n- **Circuit breakers**: Prevent cascade failures\n- **Caching**: Reduce Service B/C calls by 30-40%\n- **Connection pooling**: Reduce connection overhead\n- **Load shedding**: Drop non-critical requests at 80% capacity\n\n#### 3. **Resource Allocation**\n- **CPU**: Target 40-50% utilization under peak load\n- **Memory**: Target 60% utilization with garbage collection headroom\n- **Network**: Ensure bandwidth can handle 5x throughput\n\n#### 4. **Monitoring & Alerting**\n- Set alerts at 70% capacity utilization\n- Monitor queue depths and connection pool exhaustion\n- Track error rates and implement auto-scaling triggers\n\n#### 5. **Fallback Strategies**\n- **Graceful degradation**: Disable non-essential features\n- **CDN offloading**: Cache static content\n- **Database read replicas**: Distribute read load\n\nThis approach ensures <500ms p95 latency while maintaining system reliability during traffic spikes.",
  "diagram": "graph TD\n    A[Load Balancer] --> B[Service A - 15 instances]\n    B --> C[Service B - 8 instances]\n    B --> D[Service C - 5 instances]\n    B --> E[Cache Layer]\n    F[Circuit Breaker] --> B\n    G[Auto Scaler] --> B\n    G --> C\n    G --> D\n    H[Monitoring] --> I[Alerts]\n    I --> G\n    J[Load Shedder] --> A\n    K[CDN] --> A\n    \n    style B fill:#ff9999\n    style C fill:#99ccff\n    style D fill:#99ff99\n    style E fill:#ffcc99\n    style F fill:#ff6666\n    style J fill:#cc99ff",
  "difficulty": "advanced",
  "tags": [
    "capacity",
    "scaling"
  ],
  "lastUpdated": "2025-12-12T09:07:04.188Z"
}