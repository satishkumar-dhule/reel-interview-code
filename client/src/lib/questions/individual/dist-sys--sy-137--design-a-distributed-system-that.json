{
  "id": "sy-137",
  "question": "Design a distributed system that provides exactly-once processing guarantees for event streams with out-of-order delivery and network partitions. How would you handle idempotency, deduplication, and causal consistency across multiple processing nodes?",
  "answer": "Use vector clocks for causal ordering, deterministic IDs for deduplication, and idempotent processors with write-ahead logs.",
  "explanation": "This is an advanced distributed systems design problem that combines several complex concepts:\n\n## Core Challenges\n1. **Exactly-once semantics**: Prevent duplicate processing while ensuring no events are lost\n2. **Out-of-order handling**: Events may arrive in different orders than sent\n3. **Network partitions**: System must remain consistent during partial failures\n4. **Causal consistency**: Maintain logical relationships between related events\n\n## Architecture Components\n\n### 1. Event Ingestion Layer\n- **Vector Clocks**: Attach to each event to track causal relationships\n- **Deterministic Event IDs**: Use content-based hashing + timestamp for deduplication\n- **Partitioning Strategy**: Hash-based sharding with consistent hashing\n\n### 2. Processing Layer\n- **Idempotent Processors**: Design state changes to be repeatable\n- **Write-Ahead Logs**: Record intent before execution for recovery\n- **Checkpointing**: Periodic state snapshots for fault tolerance\n\n### 3. Coordination Layer\n- **Raft Consensus**: For metadata and configuration management\n- **Gossip Protocol**: Disseminate vector clock updates\n- **Anti-entropy Mechanisms**: Detect and repair inconsistencies\n\n### 4. Storage Layer\n- **Multi-version Concurrency Control (MVCC)**: Handle concurrent access\n- **Compaction**: Remove obsolete versions while preserving causality\n- **Replication**: Quorum-based writes with read repair\n\n## Key Algorithms\n\n### Deduplication Strategy\n```python\ndef is_duplicate(event_id, processed_events):\n    if event_id in processed_events:\n        return True\n    # Check bloom filter for quick negative lookup\n    if bloom_filter.might_contain(event_id):\n        # Verify in persistent storage\n        return storage.contains(event_id)\n    return False\n```\n\n### Causal Ordering\n- Compare vector clocks to determine event ordering\n- Buffer events until causal dependencies are satisfied\n- Use topological sorting for dependency resolution\n\n### Failure Recovery\n- Replay from last checkpoint using write-ahead logs\n- Rebuild vector clock state from persistent storage\n- Coordinate with other nodes for consistency verification\n\n## Trade-offs\n- **Latency vs Consistency**: Vector clocks add overhead but ensure correctness\n- **Storage vs Performance**: MVCC increases storage but enables concurrency\n- **Complexity vs Reliability**: Sophisticated coordination improves fault tolerance\n\nThis design demonstrates mastery of distributed systems concepts including consensus algorithms, causal consistency, fault tolerance, and exactly-once processing semantics.",
  "diagram": "graph TD\n    A[Client] --> B[Event Ingestion]\n    B --> C[Vector Clock Attachment]\n    C --> D[Deterministic ID Generation]\n    D --> E[Partition Router]\n    E --> F[Processing Node 1]\n    E --> G[Processing Node 2]\n    E --> H[Processing Node N]\n    F --> I[Idempotent Processor]\n    G --> J[Idempotent Processor]\n    H --> K[Idempotent Processor]\n    I --> L[Write-Ahead Log]\n    J --> M[Write-Ahead Log]\n    K --> N[Write-Ahead Log]\n    L --> O[MVCC Storage]\n    M --> O\n    N --> O\n    O --> P[Replication Layer]\n    P --> Q[Raft Consensus Group]\n    F --> R[Gossip Protocol]\n    G --> R\n    H --> R\n    R --> S[Vector Clock Sync]\n    Q --> T[Configuration Manager]\n    S --> U[Anti-entropy Repair]",
  "difficulty": "advanced",
  "tags": [
    "dist-sys",
    "architecture"
  ],
  "lastUpdated": "2025-12-12T09:36:23.632Z"
}