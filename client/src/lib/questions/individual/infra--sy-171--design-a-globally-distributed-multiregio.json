{
  "id": "sy-171",
  "question": "Design a globally distributed, multi-region database system that provides strong consistency with 99.999% availability while handling 10M QPS and supporting automatic failover within 50ms.",
  "answer": "Use consensus-based replication with quorum writes, geo-distributed nodes, and intelligent routing with local read caches.",
  "explanation": "## Architecture Overview\n\nThis system requires a sophisticated approach to achieve both strong consistency and high availability across multiple geographic regions.\n\n### Core Components\n\n**1. Consensus Layer (Raft/PBFT)**\n- Implement a consensus protocol for leader election and log replication\n- Use majority quorum (n/2 + 1) for strong consistency guarantees\n- Deploy consensus nodes across all regions for fault tolerance\n\n**2. Storage Layer**\n- Partition data using consistent hashing for even distribution\n- Replicate each partition across multiple regions (typically 3-5)\n- Use write-ahead logging (WAL) for durability\n- Implement compaction and garbage collection for log management\n\n**3. Routing Layer**\n- Global load balancer with health checks\n- Intelligent request routing based on proximity and data locality\n- Connection pooling and multiplexing for performance\n\n**4. Caching Layer**\n- Multi-level caching: edge, regional, and global\n- Cache invalidation through pub/sub mechanisms\n- Read-through/write-through patterns for consistency\n\n### Performance Optimizations\n\n**Write Path Optimization**\n- Batch writes to reduce network round trips\n- Parallel replication to multiple regions\n- Optimistic concurrency control with conflict resolution\n\n**Read Path Optimization**\n- Local read replicas for low latency\n- Read-after-write consistency through version vectors\n- Adaptive caching based on access patterns\n\n### Failure Handling\n\n**Automatic Failover Mechanisms**\n- Continuous health monitoring with heartbeat detection\n- Leader election within 50ms using consensus protocol\n- Seamless client redirection during failover\n- Data reconciliation after partition healing\n\n**Disaster Recovery**\n- Point-in-time recovery through periodic snapshots\n- Cross-region backup replication\n- Automated restoration procedures\n\n### Scalability Considerations\n\n**Horizontal Scaling**\n- Dynamic node addition/removal without downtime\n- Automatic data rebalancing using consistent hashing\n- Elastic resource allocation based on load\n\n**Vertical Scaling**\n- Memory optimization for hot data\n- SSD/NVVM storage for performance-critical operations\n- CPU-intensive operations offloaded to specialized nodes\n\n### Monitoring and Observability\n\n**Metrics Collection**\n- Real-time performance monitoring (latency, throughput, error rates)\n- Distributed tracing for request flow analysis\n- Resource utilization tracking across regions\n\n**Alerting and Automation**\n- Proactive alerting for performance degradation\n- Automated scaling based on predefined thresholds\n- Self-healing capabilities for common failure modes",
  "diagram": "graph TD\n    A[Client Request] --> B[Global Load Balancer]\n    B --> C[Regional Router]\n    C --> D[Consensus Leader]\n    D --> E[Write-Ahead Log]\n    E --> F[Replication Manager]\n    F --> G[Region 1 Primary]\n    F --> H[Region 2 Primary]\n    F --> I[Region 3 Primary]\n    G --> J[Region 1 Replicas]\n    H --> K[Region 2 Replicas]\n    I --> L[Region 3 Replicas]\n    C --> M[Read Cache Layer]\n    M --> N[Edge Cache]\n    M --> O[Regional Cache]\n    D --> P[Health Monitor]\n    P --> Q[Failover Controller]\n    Q --> R[Automatic Leader Election]\n    G --> S[Storage Engine]\n    H --> T[Storage Engine]\n    I --> U[Storage Engine]\n    S --> V[Compaction Service]\n    T --> W[Compaction Service]\n    U --> X[Compaction Service]",
  "difficulty": "advanced",
  "tags": [
    "infra",
    "scale"
  ],
  "lastUpdated": "2025-12-14T01:17:45.908Z"
}