[
  {
    "id": "sd-1",
    "question": "Can you explain the Load Balancer strategy? When would you use Layer 4 vs Layer 7 load balancing?",
    "answer": "Load Balancing distributes traffic across multiple servers to ensure reliability and scalability.",
    "explanation": "A Load Balancer (LB) acts as a reverse proxy. \n\n**Layer 4 (Transport Layer)**: Distributes based on IP/Port. Fast, low overhead, but no context of content. Good for simple packet distribution.\n\n**Layer 7 (Application Layer)**: Inspects HTTP headers/content. Can route based on URL/cookies (e.g., /api to Service A, /static to Service B). More expensive but smarter.\n\n**Common Algorithms**:\n- **Round Robin**: Sequential.\n- **Least Connections**: Sends to server with fewest active connections.\n- **IP Hash**: Ensures a user always goes to the same server (sticky sessions).",
    "tags": [
      "infra",
      "scale",
      "networking"
    ],
    "difficulty": "advanced",
    "channel": "system-design",
    "subChannel": "infrastructure",
    "diagram": "graph LR\n    User --> LB[Load Balancer]\n    LB -->|Layer 4| S1[\"Server 1<br/>IP:Port\"]\n    LB -->|Layer 7| S2[\"Server 2<br/>/api\"]\n    style LB fill:#fff,stroke:#000,color:#000",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "sd-2",
    "question": "What is Consistent Hashing and why is it critical for distributed caches?",
    "answer": "Consistent Hashing maps keys to a ring of nodes to minimize data movement when scaling.",
    "explanation": "In standard `hash(key) % N`, adding a node changes `N`, causing nearly ALL keys to remap (cache stampede).\n\n**Consistent Hashing** maps both servers and keys to a circle (0-360°). Keys map to the next server clockwise.\n\n**Benefit**: Adding/removing a node only affects the immediate neighbors (k/N keys move), not the whole cluster.\n\nUsed in: DynamoDB, Cassandra, Discord Ringpop.",
    "tags": [
      "hashing",
      "dist-sys",
      "caching"
    ],
    "difficulty": "advanced",
    "channel": "system-design",
    "subChannel": "distributed-systems",
    "diagram": "\ngraph TD\n    subgraph Hash Ring\n    N1((Node 1)) --- N2((Node 2))\n    N2 --- N3((Node 3))\n    N3 --- N1\n    end\n    Key[Key K] -.->|Clockwise| N2\n    style N2 fill:#f00,stroke:#fff,color:#fff\n",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "sd-3",
    "question": "Explain the CAP Theorem. Can you really 'choose two'?",
    "answer": "CAP states a distributed store can only provide 2 of 3: Consistency, Availability, Partition Tolerance.",
    "explanation": "**Partition Tolerance (P)** is NOT optional in distributed systems (networks fail). \n\nSo the real choice is **CP vs AP** during a partition:\n\n- **CP (Consistency)**: Return error/timeout if data can't be synced. (e.g., Banking - better to fail than show wrong balance).\n- **AP (Availability)**: Return stale data but keep running. (e.g., Facebook Feed - better to show old posts than nothing).\n\n**PACELC Theorem** extends this: Else (when no partition), choose Latency (L) vs Consistency (C).",
    "tags": [
      "theory",
      "dist-sys",
      "database"
    ],
    "difficulty": "advanced",
    "channel": "system-design",
    "subChannel": "distributed-systems",
    "diagram": "graph TD\n    CAP[CAP Theorem]\n    CAP --> C[Consistency]\n    CAP --> A[Availability]\n    CAP --> P[Partition Tolerance]\n    Note[Pick 2 of 3]\n    style Note fill:#f59e0b,stroke:#fff,color:#000",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "sd-4",
    "question": "How do you handle Database Sharding? What are the downsides?",
    "answer": "Sharding splits a large database into smaller, faster, easily managed parts called data shards.",
    "explanation": "**Horizontal Partitioning**: Splitting rows based on a Shard Key (e.g., UserID).\n\n**Downsides/Challenges**:\n1. **Resharding**: Hard to move data when a shard fills up.\n2. **Hotspot Key**: If Justin Bieber is on Shard 1, Shard 1 melts down.\n3. **Joins**: Cross-shard joins are expensive/impossible.\n\n**Mitigation**: Consistent Hashing, Virtual Nodes.",
    "tags": [
      "db",
      "scale",
      "architecture"
    ],
    "difficulty": "advanced",
    "channel": "system-design",
    "subChannel": "database",
    "diagram": "\ngraph TD\n    App --> Router\n    Router -->|ID < 100| S1[(Shard 1)]\n    Router -->|ID > 100| S2[(Shard 2)]\n",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "sd-5",
    "question": "Design a Rate Limiter. What algorithms would you consider?",
    "answer": "Rate Limiting controls the amount of traffic sent or received by a network interface controller.",
    "explanation": "Prevents DoS attacks and resource starvation.\n\n**Algorithms**:\n1. **Token Bucket**: Tokens added at rate `r`. Request consumes token. Allows bursts.\n2. **Leaky Bucket**: Requests enter queue, processed at constant rate. Smooths traffic.\n3. **Fixed Window**: Count requests in 1s window. Edge case: 2x traffic at window boundary.\n4. **Sliding Window Log**: Precise but expensive (stores timestamps).\n\n**Implementation**: Redis (Lua scripts for atomicity).",
    "tags": [
      "security",
      "api",
      "algorithms"
    ],
    "difficulty": "advanced",
    "channel": "system-design",
    "subChannel": "api-design",
    "diagram": "\ngraph LR\n    Req[Request] --> Check{Buckets Full?}\n    Check -->|No| Process[Process]\n    Check -->|Yes| Drop[429 Too Many Requests]\n",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "gh-31",
    "question": "What is Scalability in DevOps?",
    "answer": "Scalability is the capability of a system to handle a growing amount of work by adding resources to the system. There are two types of scaling:",
    "explanation": "Scalability is the capability of a system to handle a growing amount of work by adding resources to the system. There are two types of scaling:\n\n1. **Vertical Scaling (Scale Up):**\n- Adding more power to existing resources\n- Example: Upgrading CPU/RAM\n\n2. **Horizontal Scaling (Scale Out):**\n- Adding more resources\n- Example: Adding more servers",
    "tags": [
      "scale",
      "ha"
    ],
    "difficulty": "advanced",
    "channel": "system-design",
    "subChannel": "infrastructure",
    "diagram": "\ngraph TD\n    subgraph Vertical\n    S1[Small] --> S2[Large]\n    end\n    subgraph Horizontal\n    H1[Server] --- H2[Server] --- H3[Server]\n    end\n",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "gh-32",
    "question": "What is High Availability?",
    "answer": "High Availability (HA) is a characteristic of a system that aims to ensure an agreed level of operational performance, usually uptime, for a higher th...",
    "explanation": "High Availability (HA) is a characteristic of a system that aims to ensure an agreed level of operational performance, usually uptime, for a higher than normal period.\n\nKey components:\n1. **Redundancy:**\n- Multiple instances\n- No single point of failure\n\n2. **Monitoring:**\n- Health checks\n- Automated failover\n\n3. **Load Balancing:**\n- Traffic distribution\n- Resource optimization",
    "tags": [
      "scale",
      "ha"
    ],
    "difficulty": "advanced",
    "channel": "system-design",
    "subChannel": "infrastructure",
    "diagram": "\ngraph TD\n    LB[Load Balancer] --> S1[Server 1]\n    LB --> S2[Server 2]\n    S1 -.->|Failover| S2\n",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "gh-33",
    "question": "What is Load Balancing?",
    "answer": "Load Balancing is the process of distributing network traffic across multiple servers to ensure no single server bears too much demand.",
    "explanation": "Load Balancing is the process of distributing network traffic across multiple servers to ensure no single server bears too much demand.\n\nCommon Load Balancing algorithms:\n1. **Round Robin**\n2. **Least Connections**\n3. **IP Hash**\n4. **Weighted Round Robin**\n5. **Resource-Based**\n\nExample of Nginx Load Balancer configuration:\n```nginx\nhttp {\nupstream backend {\nserver backend1.example.com;\nserver backend2.example.com;\nserver backend3.example.com;\n}\n\nserver {\nlisten 80;\nlocation / {\nproxy_pass http://backend;\n}\n}\n}\n```",
    "tags": [
      "scale",
      "ha"
    ],
    "difficulty": "advanced",
    "channel": "system-design",
    "subChannel": "infrastructure",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "gh-34",
    "question": "What is Auto Scaling?",
    "answer": "Auto Scaling is a feature that automatically adjusts the number of compute resources based on the current demand.",
    "explanation": "Auto Scaling is a feature that automatically adjusts the number of compute resources based on the current demand.\n\nKey concepts:\n1. **Scaling Policies:**\n- Target tracking\n- Step scaling\n- Simple scaling\n\n2. **Metrics:**\n- CPU utilization\n- Memory usage\n- Request count\n- Custom metrics\n\nExample of AWS Auto Scaling configuration:\n```yaml\nAutoScalingGroup:\nMinSize: 1\nMaxSize: 10\nDesiredCapacity: 2\nHealthCheckType: ELB\nHealthCheckGracePeriod: 300\nLaunchTemplate:\nLaunchTemplateId: !Ref LaunchTemplate\nVersion: !GetAtt LaunchTemplate.LatestVersionNumber\n```",
    "tags": [
      "scale",
      "ha"
    ],
    "difficulty": "advanced",
    "channel": "system-design",
    "subChannel": "infrastructure",
    "diagram": "\ngraph LR\n    Metrics[Metrics] --> ASG[Auto Scaling]\n    ASG -->|Scale Out| Add[Add Instances]\n    ASG -->|Scale In| Remove[Remove Instances]\n",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "sy-132",
    "question": "Design a distributed rate limiting system that can handle 1M+ requests per second across multiple data centers while maintaining consistency and low latency. How would you handle burst traffic, different rate limiting algorithms (token bucket, sliding window), and ensure fair distribution across users?",
    "answer": "Use distributed token bucket with Redis Cluster, consistent hashing for user distribution, and local caching with periodic sync for low latency.",
    "explanation": "## Distributed Rate Limiting System Design\n\n### Core Components\n\n**1. Rate Limiting Algorithms**\n- **Token Bucket**: Best for burst handling, allows temporary spikes\n- **Sliding Window**: More accurate but computationally expensive\n- **Fixed Window**: Simple but can cause boundary issues\n\n**2. Architecture Overview**\n- **API Gateway Layer**: First line of defense with local rate limiting\n- **Distributed Cache**: Redis Cluster for shared state across regions\n- **Rate Limit Service**: Dedicated microservice for complex logic\n- **Configuration Service**: Dynamic rule updates without deployment\n\n### Implementation Strategy\n\n**Local + Distributed Hybrid Approach:**\n```\n1. Local cache (99% of requests) - sub-millisecond latency\n2. Periodic sync with distributed store (every 100ms)\n3. Fallback to distributed check for edge cases\n```\n\n**Data Distribution:**\n- Consistent hashing for user → shard mapping\n- Replication factor of 3 for high availability\n- Cross-region replication with eventual consistency\n\n**Handling Scale:**\n- Partition by user ID hash\n- Use Lua scripts in Redis for atomic operations\n- Implement circuit breakers for Redis failures\n- Local rate limiting as fallback\n\n### Advanced Features\n\n**Burst Handling:**\n- Token bucket with configurable burst capacity\n- Adaptive rate limiting based on system load\n- Priority queues for different user tiers\n\n**Fairness & Anti-Gaming:**\n- Per-user quotas with spillover pools\n- Detect and penalize abusive patterns\n- Implement jitter to prevent thundering herd\n\n**Monitoring & Observability:**\n- Real-time metrics on rate limit hits\n- Distributed tracing for debugging\n- Alerting on unusual traffic patterns",
    "tags": [
      "api",
      "rest"
    ],
    "difficulty": "advanced",
    "channel": "system-design",
    "subChannel": "api-design",
    "diagram": "graph TD\n    A[Client Requests] --> B[Load Balancer]\n    B --> C[API Gateway Cluster]\n    C --> D[Local Rate Limiter]\n    D --> E{Within Local Limit?}\n    E -->|Yes| F[Process Request]\n    E -->|No| G[Check Distributed Store]\n    G --> H[Redis Cluster]\n    H --> I[Rate Limit Service]\n    I --> J{Within Global Limit?}\n    J -->|Yes| K[Update Counters]\n    J -->|No| L[Reject Request]\n    K --> F\n    L --> M[Return 429]\n    \n    N[Config Service] --> O[Rate Limit Rules]\n    O --> C\n    O --> I\n    \n    P[Monitoring] --> Q[Metrics Collection]\n    Q --> R[Alerting]\n    \n    H --> S[Cross-Region Sync]\n    S --> T[Other Data Centers]",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "sy-137",
    "question": "Design a distributed system that provides exactly-once processing guarantees for event streams with out-of-order delivery and network partitions. How would you handle idempotency, deduplication, and causal consistency across multiple processing nodes?",
    "answer": "Use vector clocks for causal ordering, deterministic IDs for deduplication, and idempotent processors with write-ahead logs.",
    "explanation": "This is an advanced distributed systems design problem that combines several complex concepts:\n\n## Core Challenges\n1. **Exactly-once semantics**: Prevent duplicate processing while ensuring no events are lost\n2. **Out-of-order handling**: Events may arrive in different orders than sent\n3. **Network partitions**: System must remain consistent during partial failures\n4. **Causal consistency**: Maintain logical relationships between related events\n\n## Architecture Components\n\n### 1. Event Ingestion Layer\n- **Vector Clocks**: Attach to each event to track causal relationships\n- **Deterministic Event IDs**: Use content-based hashing + timestamp for deduplication\n- **Partitioning Strategy**: Hash-based sharding with consistent hashing\n\n### 2. Processing Layer\n- **Idempotent Processors**: Design state changes to be repeatable\n- **Write-Ahead Logs**: Record intent before execution for recovery\n- **Checkpointing**: Periodic state snapshots for fault tolerance\n\n### 3. Coordination Layer\n- **Raft Consensus**: For metadata and configuration management\n- **Gossip Protocol**: Disseminate vector clock updates\n- **Anti-entropy Mechanisms**: Detect and repair inconsistencies\n\n### 4. Storage Layer\n- **Multi-version Concurrency Control (MVCC)**: Handle concurrent access\n- **Compaction**: Remove obsolete versions while preserving causality\n- **Replication**: Quorum-based writes with read repair\n\n## Key Algorithms\n\n### Deduplication Strategy\n```python\ndef is_duplicate(event_id, processed_events):\n    if event_id in processed_events:\n        return True\n    # Check bloom filter for quick negative lookup\n    if bloom_filter.might_contain(event_id):\n        # Verify in persistent storage\n        return storage.contains(event_id)\n    return False\n```\n\n### Causal Ordering\n- Compare vector clocks to determine event ordering\n- Buffer events until causal dependencies are satisfied\n- Use topological sorting for dependency resolution\n\n### Failure Recovery\n- Replay from last checkpoint using write-ahead logs\n- Rebuild vector clock state from persistent storage\n- Coordinate with other nodes for consistency verification\n\n## Trade-offs\n- **Latency vs Consistency**: Vector clocks add overhead but ensure correctness\n- **Storage vs Performance**: MVCC increases storage but enables concurrency\n- **Complexity vs Reliability**: Sophisticated coordination improves fault tolerance\n\nThis design demonstrates mastery of distributed systems concepts including consensus algorithms, causal consistency, fault tolerance, and exactly-once processing semantics.",
    "tags": [
      "dist-sys",
      "architecture"
    ],
    "difficulty": "advanced",
    "channel": "system-design",
    "subChannel": "distributed-systems",
    "diagram": "graph TD\n    A[Client] --> B[Event Ingestion]\n    B --> C[Vector Clock Attachment]\n    C --> D[Deterministic ID Generation]\n    D --> E[Partition Router]\n    E --> F[Processing Node 1]\n    E --> G[Processing Node 2]\n    E --> H[Processing Node N]\n    F --> I[Idempotent Processor]\n    G --> J[Idempotent Processor]\n    H --> K[Idempotent Processor]\n    I --> L[Write-Ahead Log]\n    J --> M[Write-Ahead Log]\n    K --> N[Write-Ahead Log]\n    L --> O[MVCC Storage]\n    M --> O\n    N --> O\n    O --> P[Replication Layer]\n    P --> Q[Raft Consensus Group]\n    F --> R[Gossip Protocol]\n    G --> R\n    H --> R\n    R --> S[Vector Clock Sync]\n    Q --> T[Configuration Manager]\n    S --> U[Anti-entropy Repair]",
    "lastUpdated": "2025-12-12T09:36:23.632Z"
  },
  {
    "id": "sy-138",
    "question": "Design a distributed rate limiting system that can handle 10M requests per minute across 100+ microservices with different rate limit policies per service and API key.",
    "answer": "Use token bucket algorithm with Redis cluster, local caching, and hierarchical rate limiting (global + per-service + per-key).",
    "explanation": "# Distributed Rate Limiting System Design\n\n## Core Requirements\n- 10M requests/minute throughput\n- Multiple rate limit policies per service\n- Per-API key limits\n- 99.9% availability\n- Sub-10ms latency\n\n## Architecture Components\n\n### 1. Rate Limiting Engine\n- **Token Bucket Algorithm**: Flexible burst handling\n- **Sliding Window**: Time-based accuracy\n- **Policy Engine**: Dynamic rule evaluation\n\n### 2. Storage Layer\n- **Redis Cluster**: Primary counter storage\n- **Local Cache**: LRU for frequently accessed keys\n- **Persistent Storage**: PostgreSQL for policy configuration\n\n### 3. Distribution Strategy\n- **Consistent Hashing**: Even key distribution\n- **Replication**: Multi-master Redis setup\n- **Sharding**: Key-based partitioning\n\n## Key Design Patterns\n\n### Hierarchical Rate Limiting\n1. **Global Limits**: Platform-wide protection\n2. **Service Limits**: Per-microservice constraints\n3. **API Key Limits**: User-specific restrictions\n\n### Performance Optimizations\n- **Batch Processing**: Redis MGET/MSET operations\n- **Async Updates**: Fire-and-forget counter increments\n- **Pre-aggregation**: Local batching before sync\n\n### Failure Handling\n- **Graceful Degradation**: Fallback to local-only limiting\n- **Circuit Breaker**: Fail-fast for Redis outages\n- **Rate Limit Escalation**: Progressive restriction\n\n## Implementation Considerations\n\n### Synchronization\n- **Atomic Operations**: Redis INCR with expiration\n- **Clock Drift**: NTP synchronization\n- **Consistent Window**: Aligned time boundaries\n\n### Scalability\n- **Horizontal Scaling**: Add Redis nodes\n- **Geographic Distribution**: Edge caching\n- **Load Distribution**: Smart client routing\n\n### Monitoring & Observability\n- **Real-time Metrics**: Prometheus integration\n- **Alerting**: Rate limit breach detection\n- **Audit Trail**: Policy change tracking",
    "tags": [
      "api",
      "rest"
    ],
    "difficulty": "advanced",
    "channel": "system-design",
    "subChannel": "api-design",
    "diagram": "graph TD\n    A[Client Request] --> B[API Gateway]\n    B --> C[Rate Limiter Service]\n    C --> D{Local Cache Check}\n    D -->|Hit| E[Return Decision]\n    D -->|Miss| F[Redis Cluster]\n    F --> G[Policy Engine]\n    G --> H[Rate Limit Algorithm]\n    H --> I[Update Local Cache]\n    I --> E\n    E --> J{Allow?}\n    J -->|Yes| K[Forward to Service]\n    J -->|No| L[Return 429]\n    \n    subgraph \"Redis Cluster\"\n        F1[Shard 1]\n        F2[Shard 2]\n        F3[Shard N]\n    end\n    \n    subgraph \"Policy Store\"\n        G1[Global Policies]\n        G2[Service Policies]\n        G3[API Key Policies]\n    end\n    \n    G --> G1\n    G --> G2\n    G --> G3",
    "lastUpdated": "2025-12-12T09:36:34.140Z"
  },
  {
    "id": "sy-139",
    "question": "Design a rate limiting system for a multi-tenant API that supports burst capacity with token bucket algorithm, distributed coordination, and dynamic tenant-specific policies",
    "answer": "Distributed token bucket with Redis-backed state, tenant-specific rate configs, and local caches for performance",
    "explanation": "This system needs to handle multiple tenants with different rate limits, support burst capacity through token buckets, ensure coordination across multiple API instances, and provide dynamic policy updates.\n\nKey components:\n- **Token Bucket Algorithm**: Each tenant has a bucket with capacity (burst) and refill rate. Tokens are added at configured rate, requests consume tokens.\n- **Distributed State**: Redis stores current token counts and last refill timestamps for each tenant, ensuring consistency across instances.\n- **Local Caching**: Each API instance maintains a local cache of token counts with TTL to reduce Redis calls, using optimistic updates.\n- **Policy Management**: Dynamic rate limit configuration stored in database, propagated through Redis pub/sub.\n- **Fallback**: When Redis unavailable, fall back to local bucket with reduced capacity to prevent service interruption.\n\nPerformance considerations:\n- Pipeline Redis operations for batch updates\n- Use Lua scripts for atomic token operations\n- Implement write-back caching for burst scenarios\n- Separate hot path (rate check) from policy updates\n\nChallenges addressed:\n- Race conditions in distributed environment\n- Clock synchronization issues\n- Graceful degradation during failures\n- Tenant isolation and fairness",
    "tags": [
      "api",
      "rest"
    ],
    "difficulty": "advanced",
    "channel": "system-design",
    "subChannel": "api-design",
    "diagram": "graph TD\n    A[Client Request] --> B{Rate Limiter Service}\n    B --> C[Local Cache Check]\n    C -->|Hit| D[Consume Local Tokens]\n    C -->|Miss| E[Redis Token Check]\n    E -->|Available| F[Consume Redis Tokens]\n    E -->|Limited| G[Reject - 429]\n    D --> H[Update Local Cache Async]\n    F --> I[Update Local Cache]\n    H --> J[Forward to API]\n    I --> J\n    G --> K[Return Rate Limit Headers]\n    J --> L[Response]\n    M[Policy Manager] --> N[Redis Pub/Sub]\n    N --> O[Update Local Config]\n    O --> B",
    "lastUpdated": "2025-12-12T09:36:49.367Z"
  },
  {
    "id": "sy-140",
    "question": "Design a rate limiting service that can handle 10 million requests per second with distributed consistency across multiple data centers. The service should support multiple rate limiting strategies (token bucket, sliding window, fixed window) and provide sub-millisecond latency. How would you architect this to handle bursts, prevent thundering herd problems, and ensure accurate global rate limits?",
    "answer": "Use Redis Cluster with Consistent Hashing + Local Caching + Adaptive Rate Limiting with Hierarchical Rate Limiting (user → API → global).",
    "explanation": "## Architecture Overview\n\n**Core Components:**\n1. **Rate Limiting Engine** - Pluggable strategy pattern supporting token bucket, sliding window, and fixed window algorithms\n2. **Distributed Cache Layer** - Redis Cluster with consistent hashing for horizontal scaling\n3. **Local Cache Tier** - L1 cache with write-through to reduce Redis load\n4. **Configuration Service** - Dynamic rule management with hot-reloading\n5. **Metrics & Analytics** - Real-time monitoring and alerting\n\n**Key Design Decisions:**\n\n### 1. Hierarchical Rate Limiting\n- **User Level**: Per-user quotas (e.g., 1000 req/min)\n- **API Level**: Per-endpoint limits (e.g., 100 req/min)\n- **Global Level**: System-wide protection (e.g., 10M req/s)\n\n### 2. Multi-Level Caching Strategy\n- **L1 Cache**: In-memory with 1-second TTL for 90% of requests\n- **L2 Cache**: Redis Cluster with consistent hashing\n- **Write-through**: Updates propagate to both levels\n\n### 3. Burst Handling\n- **Token Bucket**: Allows controlled bursts\n- **Credit System**: Accumulates unused capacity\n- **Priority Queues**: VIP users get preferential treatment\n\n### 4. Thundering Herd Prevention\n- **Request Coalescing**: Batch requests for same key\n- **Exponential Backoff**: Adaptive retry with jitter\n- **Circuit Breakers**: Fail-fast during overload\n\n### 5. Global Consistency\n- **Vector Clocks**: Resolve conflicts across data centers\n- **Gossip Protocol**: Sync rate limit state\n- **Eventual Consistency**: Acceptable for rate limiting\n\n### 6. Performance Optimizations\n- **Connection Pooling**: Reuse Redis connections\n- **Pipelining**: Batch Redis operations\n- **Compression**: Reduce network overhead\n- **Async Processing**: Non-blocking I/O\n\n### 7. Monitoring & Alerting\n- **Real-time Dashboards**: Rate limit utilization\n- **Anomaly Detection**: Unusual traffic patterns\n- **Auto-scaling**: Dynamic cluster sizing\n\n## Implementation Considerations\n\n**Data Model:**\n- Key: `rate_limit:{user_id}:{api_id}:{window}`\n- Value: `{count, last_reset, credits}`\n- TTL: Window duration + safety margin\n\n**Failure Modes:**\n- Redis unavailable: Fall back to local limits\n- Network partition: Permissive mode with logging\n- Cache stampede: Request deduplication\n\n**Scalability:**\n- Horizontal scaling with Redis Cluster\n- Geographic distribution with edge caching\n- Load balancing with consistent hashing",
    "tags": [
      "api",
      "rest"
    ],
    "difficulty": "advanced",
    "channel": "system-design",
    "subChannel": "api-design",
    "diagram": "graph TD\n    A[Client Request] --> B[Load Balancer]\n    B --> C[Rate Limiting Service]\n    \n    C --> D{Local Cache Check}\n    D -->|Hit| E[Allow/Deny]\n    D -->|Miss| F[Distributed Cache]\n    \n    F --> G{Redis Cluster}\n    G --> H[Shard 1]\n    G --> I[Shard 2]\n    G --> J[Shard N]\n    \n    C --> K[Rate Limiting Engine]\n    K --> L[Token Bucket]\n    K --> M[Sliding Window]\n    K --> N[Fixed Window]\n    \n    C --> O[Configuration Service]\n    O --> P[Rate Limit Rules]\n    O --> Q[User Quotas]\n    \n    C --> R[Analytics Engine]\n    R --> S[Metrics Dashboard]\n    R --> T[Alert System]\n    \n    U[Data Center 1] --> G\n    V[Data Center 2] --> G\n    W[Data Center N] --> G\n    \n    G --> X[Gossip Protocol]\n    X --> Y[State Synchronization]\n    \n    style C fill:#e1f5fe\n    style G fill:#f3e5f5\n    style K fill:#e8f5e8",
    "lastUpdated": "2025-12-12T09:37:42.396Z"
  },
  {
    "id": "sy-141",
    "question": "Design a globally distributed serverless platform for real-time collaborative document editing with offline support and conflict resolution. How would you handle data consistency, versioning, and low-latency synchronization across AWS regions while maintaining sub-50ms response times?",
    "answer": "Use CRDTs for conflict resolution, WebSocket for real-time sync, edge locations for caching, DynamoDB Global Tables with multi-region replication.",
    "explanation": "## Architecture Overview\n\n### Data Model & Consistency\n- **CRDT-based Operational Transformation**: Each client tracks operations using Conflict-Free Replicated Data Types\n- **Document State Partitioning**: Split documents into chunks by section/paragraph for parallel processing\n- **Version Vectors**: Lamport timestamps for causality tracking across regions\n\n### Multi-Region Strategy\n- **Active-Active Regions**: Deploy Lambda functions in us-east-1, eu-west-1, ap-southeast-1\n- **DynamoDB Global Tables**: Multi-master replication with conflict-free write patterns\n- **CloudFront Edge Locations**: Cache hot documents with WebSocket support\n\n### Real-time Synchronization\n- **WebSocket Connections**: API Gateway with WebSocket protocol for bidirectional communication\n- **EventBridge**: Cross-region event propagation for coordination\n- **Local Caching**: ElastiCache Redis in each region for hot document state\n\n### Offline Support\n- **Service Worker**: IndexedDB for local storage and operation queuing\n- **Delta Synchronization**: Only transmit changes, not full document state\n- **Conflict Resolution**: CRDT automatically resolves merge conflicts when reconnecting\n\n### Performance Optimizations\n- **Edge Computing**: CloudFront Functions for document diff calculation at edge\n- **Connection Pooling**: WebSocket multiplexing to reduce connection overhead\n- **Smart Routing**: Route 53 latency-based routing to nearest region\n\n### Monitoring & Scaling\n- **Auto Scaling**: Lambda provisioned concurrency for predictable performance\n- **Real-time Metrics**: CloudWatch custom metrics for collaboration metrics\n- **Circuit Breakers**: Regional isolation to prevent cascade failures",
    "tags": [
      "infra",
      "scale"
    ],
    "difficulty": "advanced",
    "channel": "system-design",
    "subChannel": "infrastructure",
    "diagram": "graph TD\n    A[Client Browser] -->|WebSocket| B[CloudFront Edge]\n    B -->|WebSocket| C[API Gateway WebSocket]\n    C --> D[Lambda Auth]\n    C --> E[Lambda Router]\n    E --> F[Lambda Document Handler]\n    E --> G[Lambda Sync Handler]\n    F --> H[DynamoDB Global Table]\n    G --> I[EventBridge Bus]\n    I --> J[Cross-Region EventBridge]\n    J --> K[Other Region Lambda]\n    F --> L[ElastiCache Redis]\n    K --> M[DynamoDB Replica]\n    A --> N[Service Worker]\n    N --> O[IndexedDB Storage]\n    P[Route 53] -->|Latency Routing| B\n    Q[CloudWatch] -->|Metrics| E",
    "lastUpdated": "2025-12-12T09:38:09.154Z"
  },
  {
    "id": "sy-144",
    "question": "Design a distributed rate limiting system that can handle 1M+ requests per second across multiple data centers while maintaining consistency and preventing thundering herd problems during cache misses.",
    "answer": "Use sliding window counters with Redis Cluster, consistent hashing, and circuit breakers with jittered backoff for cache miss protection.",
    "explanation": "## Architecture Components\n\n### 1. Distributed Counter Storage\n- **Redis Cluster** with consistent hashing for horizontal scaling\n- **Sliding window counters** using sorted sets with timestamps\n- **Multi-level caching**: L1 (local), L2 (regional), L3 (global)\n\n### 2. Rate Limiting Algorithm\n```\nkey = user_id:window_start_time\ncount = ZCOUNT key (now-window_size) now\nif count < limit:\n  ZADD key now unique_request_id\n  EXPIRE key window_size\n  return ALLOW\nelse:\n  return DENY\n```\n\n### 3. Consistency Strategy\n- **Eventually consistent** across regions (acceptable for rate limiting)\n- **Strong consistency** within data center using Redis transactions\n- **Conflict resolution**: Last-writer-wins with vector clocks\n\n### 4. Thundering Herd Prevention\n- **Circuit breaker pattern** with exponential backoff\n- **Jittered cache refresh** (random 10-30% of TTL)\n- **Probabilistic early expiration** to spread load\n- **Request coalescing** for identical cache misses\n\n### 5. High Availability\n- **Multi-master Redis setup** with cross-region replication\n- **Graceful degradation**: Allow requests when cache unavailable\n- **Health checks** and automatic failover\n- **Rate limit approximation** during partial failures",
    "tags": [
      "dist-sys",
      "architecture"
    ],
    "difficulty": "advanced",
    "channel": "system-design",
    "subChannel": "distributed-systems",
    "diagram": "graph TD\n    A[Client Request] --> B[Load Balancer]\n    B --> C[Rate Limiter Service]\n    C --> D{Local Cache Hit?}\n    D -->|Yes| E[Check Counter]\n    D -->|No| F[Circuit Breaker]\n    F -->|Open| G[Allow Request]\n    F -->|Closed| H[Redis Cluster]\n    H --> I[Sliding Window Counter]\n    I --> J{Under Limit?}\n    J -->|Yes| K[Increment Counter]\n    J -->|No| L[Reject Request]\n    K --> M[Update Local Cache]\n    M --> N[Forward Request]\n    E --> J\n    \n    subgraph Redis Cluster\n        H1[Redis Master 1]\n        H2[Redis Master 2]\n        H3[Redis Master 3]\n        H1 -.-> H2\n        H2 -.-> H3\n        H3 -.-> H1\n    end\n    \n    subgraph Multi-DC\n        DC1[Data Center 1]\n        DC2[Data Center 2]\n        DC1 -.->|Async Replication| DC2\n    end",
    "lastUpdated": "2025-12-12T10:05:21.929Z"
  },
  {
    "id": "sy-151",
    "question": "Design a rate limiting API for a multi-tenant SaaS platform where different customers have different rate limits (free: 100 req/hour, premium: 1000 req/hour, enterprise: custom). How would you design the API endpoints and data structures to efficiently track and enforce these limits?",
    "answer": "Use token bucket algorithm with Redis, API key middleware, and tiered limit configs stored in DB with in-memory cache for fast lookups.",
    "explanation": "## Rate Limiting API Design\n\n### Core Components\n\n**1. API Structure**\n```\nPOST /api/v1/ratelimit/check\nGET /api/v1/ratelimit/status/:apiKey\nPOST /api/v1/ratelimit/reset/:apiKey (admin)\n```\n\n**2. Data Structures**\n- **Redis**: Store token buckets with TTL\n  - Key: `ratelimit:{tenant_id}:{window}`\n  - Value: `{tokens_remaining, last_refill_time}`\n- **Database**: Tenant configurations\n  - `tenants` table: `{id, tier, custom_limit, window_seconds}`\n- **In-Memory Cache**: Hot tenant limits (LRU cache)\n\n**3. Token Bucket Algorithm**\n- Each request consumes 1 token\n- Tokens refill at configured rate\n- Bucket capacity = tier limit\n- Use Redis INCR/DECR for atomic operations\n\n**4. Implementation Flow**\n1. Extract API key from request header\n2. Check in-memory cache for tenant tier\n3. If miss, query DB and cache result\n4. Check Redis for current token count\n5. If tokens available: decrement and allow\n6. If depleted: return 429 with Retry-After header\n\n**5. Response Headers**\n```\nX-RateLimit-Limit: 1000\nX-RateLimit-Remaining: 847\nX-RateLimit-Reset: 1640000000\nRetry-After: 3600 (if rate limited)\n```\n\n**6. Scalability Considerations**\n- Use Redis Cluster for horizontal scaling\n- Implement sliding window for smoother limits\n- Add circuit breaker for Redis failures (fail open)\n- Use distributed rate limiting for multi-region\n\n**7. Edge Cases**\n- Burst allowance for enterprise customers\n- Grace period for tier upgrades\n- Rate limit exemptions for health checks",
    "tags": [
      "api",
      "rest"
    ],
    "difficulty": "intermediate",
    "channel": "system-design",
    "subChannel": "api-design",
    "diagram": "graph TD\n    A[Client Request] --> B[API Gateway]\n    B --> C{Extract API Key}\n    C --> D[Check Memory Cache]\n    D --> E{Cache Hit?}\n    E -->|No| F[Query DB for Tier]\n    F --> G[Cache Tier Config]\n    G --> H[Check Redis Token Bucket]\n    E -->|Yes| H\n    H --> I{Tokens Available?}\n    I -->|Yes| J[Decrement Token]\n    J --> K[Allow Request]\n    K --> L[Add Rate Limit Headers]\n    I -->|No| M[Return 429 Too Many Requests]\n    M --> N[Add Retry-After Header]\n    L --> O[Forward to Backend]\n    \n    subgraph Redis\n    H\n    J\n    end\n    \n    subgraph Database\n    F\n    end\n    \n    subgraph Memory Cache\n    D\n    G\n    end",
    "lastUpdated": "2025-12-13T01:07:37.875Z"
  },
  {
    "id": "sy-158",
    "question": "Design a distributed rate limiter that can handle 1M requests/second across 100 data centers with <10ms latency. How do you ensure accurate rate limiting while avoiding coordination overhead?",
    "answer": "Use local counters with async gossip protocol, sliding window algorithm, and token bucket with eventual consistency guarantees.",
    "explanation": "## Solution Architecture\n\n### Key Components\n\n1. **Local Rate Limiters**: Each node maintains local counters using sliding window or token bucket algorithms\n2. **Gossip Protocol**: Nodes periodically exchange counter information to achieve eventual consistency\n3. **Hybrid Approach**: Combine local decisions with periodic synchronization\n\n### Design Choices\n\n**Local Token Buckets**\n- Each node gets quota allocation (total_limit / num_nodes)\n- Tokens refill at configured rate\n- Fast local decisions (<1ms)\n- Trade-off: May allow brief bursts above global limit\n\n**Sliding Window Counters**\n- Track requests in time windows (e.g., 1-second buckets)\n- Use Redis sorted sets or in-memory structures\n- Weighted counting for window boundaries\n\n**Gossip Synchronization**\n- Nodes exchange counter deltas every 100-500ms\n- Epidemic broadcast ensures eventual consistency\n- Adjust local quotas based on cluster-wide usage\n\n### Handling Edge Cases\n\n**Hot Partitions**: Use consistent hashing with virtual nodes to distribute load\n\n**Network Partitions**: Implement conservative limits during splits (fail-safe mode)\n\n**Burst Traffic**: Pre-allocate burst capacity (e.g., 120% of steady-state limit)\n\n### Accuracy vs Performance Trade-offs\n\n- **Strict Accuracy**: Use centralized Redis with Lua scripts (higher latency ~50ms)\n- **High Performance**: Local-only decisions (may exceed limit by 5-10%)\n- **Balanced**: Gossip-based with 1-2% overage tolerance\n\n### Implementation Details\n\n```\nAlgorithm: Hybrid Rate Limiter\n1. Check local token bucket\n2. If tokens available, allow immediately\n3. If near limit, check gossip state\n4. Background: sync counters every 200ms\n5. Adjust local quota based on cluster load\n```\n\n### Scalability Considerations\n\n- **Horizontal Scaling**: Add nodes dynamically, redistribute quotas\n- **Geographic Distribution**: Regional rate limiters with cross-region aggregation\n- **Storage**: Use in-memory stores (Redis, Memcached) with TTL-based cleanup",
    "tags": [
      "dist-sys",
      "architecture"
    ],
    "difficulty": "advanced",
    "channel": "system-design",
    "subChannel": "distributed-systems",
    "diagram": "graph TD\n    A[Client Request] --> B[Load Balancer]\n    B --> C[Node 1: Local Rate Limiter]\n    B --> D[Node 2: Local Rate Limiter]\n    B --> E[Node 3: Local Rate Limiter]\n    \n    C --> F[Local Token Bucket]\n    D --> G[Local Token Bucket]\n    E --> H[Local Token Bucket]\n    \n    F -.Gossip Sync.-> G\n    G -.Gossip Sync.-> H\n    H -.Gossip Sync.-> F\n    \n    C --> I[Redis Cache]\n    D --> I\n    E --> I\n    \n    I --> J[Sliding Window Counters]\n    \n    F --> K{Tokens Available?}\n    K -->|Yes| L[Allow Request]\n    K -->|No| M[Reject: 429]\n    \n    N[Gossip Manager] --> F\n    N --> G\n    N --> H\n    \n    O[Quota Adjuster] --> N\n    I -.Periodic Sync.-> O",
    "lastUpdated": "2025-12-13T01:09:32.003Z"
  },
  {
    "id": "sy-169",
    "question": "Design a simple URL shortening service like bit.ly. What components would you need and how would they work together?",
    "answer": "Database for mappings, API server for redirects, web interface for users, cache for performance.",
    "explanation": "# URL Shortening Service Design\n\n## Core Components\n\n1. **API Server**: Handles HTTP requests for creating short URLs and redirects\n2. **Database**: Stores mapping between short codes and original URLs\n3. **Cache Layer**: Redis for frequently accessed URLs to reduce database load\n4. **Web Interface**: Frontend for users to create and manage short URLs\n\n## Data Flow\n\n1. User submits long URL via web interface\n2. API generates unique short code (base62 encoding)\n3. Store mapping in database\n4. Return short URL to user\n5. When short URL accessed, check cache first, then database\n6. Redirect to original URL\n\n## Key Considerations\n\n- Use base62 encoding for compact, readable short codes\n- Implement rate limiting to prevent abuse\n- Add analytics tracking for click counts\n- Consider CDN for global performance",
    "tags": [
      "infra",
      "scale"
    ],
    "difficulty": "beginner",
    "channel": "system-design",
    "subChannel": "infrastructure",
    "diagram": "graph TD\n    A[User] --> B[Web Interface]\n    B --> C[API Server]\n    C --> D[Database]\n    C --> E[Cache Layer]\n    F[Short URL] --> C\n    C --> G[Redirect to Original URL]\n    E --> C",
    "lastUpdated": "2025-12-14T01:17:10.517Z"
  },
  {
    "id": "sy-171",
    "question": "Design a globally distributed, multi-region database system that provides strong consistency with 99.999% availability while handling 10M QPS and supporting automatic failover within 50ms.",
    "answer": "Use consensus-based replication with quorum writes, geo-distributed nodes, and intelligent routing with local read caches.",
    "explanation": "## Architecture Overview\n\nThis system requires a sophisticated approach to achieve both strong consistency and high availability across multiple geographic regions.\n\n### Core Components\n\n**1. Consensus Layer (Raft/PBFT)**\n- Implement a consensus protocol for leader election and log replication\n- Use majority quorum (n/2 + 1) for strong consistency guarantees\n- Deploy consensus nodes across all regions for fault tolerance\n\n**2. Storage Layer**\n- Partition data using consistent hashing for even distribution\n- Replicate each partition across multiple regions (typically 3-5)\n- Use write-ahead logging (WAL) for durability\n- Implement compaction and garbage collection for log management\n\n**3. Routing Layer**\n- Global load balancer with health checks\n- Intelligent request routing based on proximity and data locality\n- Connection pooling and multiplexing for performance\n\n**4. Caching Layer**\n- Multi-level caching: edge, regional, and global\n- Cache invalidation through pub/sub mechanisms\n- Read-through/write-through patterns for consistency\n\n### Performance Optimizations\n\n**Write Path Optimization**\n- Batch writes to reduce network round trips\n- Parallel replication to multiple regions\n- Optimistic concurrency control with conflict resolution\n\n**Read Path Optimization**\n- Local read replicas for low latency\n- Read-after-write consistency through version vectors\n- Adaptive caching based on access patterns\n\n### Failure Handling\n\n**Automatic Failover Mechanisms**\n- Continuous health monitoring with heartbeat detection\n- Leader election within 50ms using consensus protocol\n- Seamless client redirection during failover\n- Data reconciliation after partition healing\n\n**Disaster Recovery**\n- Point-in-time recovery through periodic snapshots\n- Cross-region backup replication\n- Automated restoration procedures\n\n### Scalability Considerations\n\n**Horizontal Scaling**\n- Dynamic node addition/removal without downtime\n- Automatic data rebalancing using consistent hashing\n- Elastic resource allocation based on load\n\n**Vertical Scaling**\n- Memory optimization for hot data\n- SSD/NVVM storage for performance-critical operations\n- CPU-intensive operations offloaded to specialized nodes\n\n### Monitoring and Observability\n\n**Metrics Collection**\n- Real-time performance monitoring (latency, throughput, error rates)\n- Distributed tracing for request flow analysis\n- Resource utilization tracking across regions\n\n**Alerting and Automation**\n- Proactive alerting for performance degradation\n- Automated scaling based on predefined thresholds\n- Self-healing capabilities for common failure modes",
    "tags": [
      "infra",
      "scale"
    ],
    "difficulty": "advanced",
    "channel": "system-design",
    "subChannel": "infrastructure",
    "diagram": "graph TD\n    A[Client Request] --> B[Global Load Balancer]\n    B --> C[Regional Router]\n    C --> D[Consensus Leader]\n    D --> E[Write-Ahead Log]\n    E --> F[Replication Manager]\n    F --> G[Region 1 Primary]\n    F --> H[Region 2 Primary]\n    F --> I[Region 3 Primary]\n    G --> J[Region 1 Replicas]\n    H --> K[Region 2 Replicas]\n    I --> L[Region 3 Replicas]\n    C --> M[Read Cache Layer]\n    M --> N[Edge Cache]\n    M --> O[Regional Cache]\n    D --> P[Health Monitor]\n    P --> Q[Failover Controller]\n    Q --> R[Automatic Leader Election]\n    G --> S[Storage Engine]\n    H --> T[Storage Engine]\n    I --> U[Storage Engine]\n    S --> V[Compaction Service]\n    T --> W[Compaction Service]\n    U --> X[Compaction Service]",
    "lastUpdated": "2025-12-14T01:17:45.908Z"
  },
  {
    "id": "sy-171",
    "question": "Design a simple monitoring dashboard for a web application that shows real-time metrics like CPU usage, memory consumption, and request response times. What components would you need and how would they work together?",
    "answer": "Metrics collectors, time-series database, dashboard UI, and real-time data streaming.",
    "explanation": "# Simple Monitoring Dashboard Design\n\n## Core Components\n\n1. **Metrics Collectors**: Agents on servers that collect system metrics (CPU, memory, disk) and application metrics (response times, error rates)\n2. **Time-Series Database**: Storage optimized for timestamped data like InfluxDB or Prometheus\n3. **API Gateway**: Exposes metrics data via REST endpoints for the frontend\n4. **Dashboard UI**: Web interface that displays charts and graphs in real-time\n5. **WebSocket/Server-Sent Events**: For real-time data streaming to the dashboard\n\n## Data Flow\n\n1. Metrics collectors gather data every few seconds\n2. Data sent to time-series database with timestamps\n3. Dashboard queries latest data via API\n4. Real-time updates pushed through WebSockets\n5. Charts refresh automatically showing current system state\n\n## Key Considerations\n\n- Use efficient data compression for storage\n- Implement data retention policies (keep detailed data for 24h, aggregated for longer)\n- Add alerting thresholds for critical metrics\n- Consider caching for frequently accessed data",
    "tags": [
      "infra",
      "scale"
    ],
    "difficulty": "beginner",
    "channel": "system-design",
    "subChannel": "infrastructure",
    "diagram": "graph TD\n    A[Web Servers] --> B[Metrics Collectors]\n    B --> C[Time-Series DB]\n    C --> D[API Gateway]\n    D --> E[Dashboard UI]\n    D --> F[WebSocket Server]\n    F --> E\n    G[Alert Manager] --> C\n    H[Data Retention] --> C",
    "lastUpdated": "2025-12-14T01:18:54.878Z"
  }
]